---
title: "BuildSdmTMB"
author: "Stephanie Hopkins"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    
---

```{r setup, include=FALSE}

rm(list=ls())
gc()

load.lib <- c("tidyverse", "data.table", "cowplot", "sf", "rcompanion",
              "pROC", "here", "sdmTMB", "sdmTMBextra", "visreg",
              "ggforce", # enables plot_anisotropy
              "spatstat",# Get summary table for nearest neighbor
              "rsample", "timetk") 

install.lib <- load.lib[!load.lib %in% installed.packages()]

for(lib in install.lib) install.packages(lib,dependencies=TRUE)
sapply(load.lib,require,character=TRUE)

sf_use_s2(FALSE)

# remotes::install_github("pbs-assess/sdmTMBextra", dependencies = TRUE)

dir <- here(getwd())

```

# Load previous sdm data

Build new adult sardine SDM using environmental predictors from the 3km WC15 ROMS.
Vars are SST, surface chl-a, upper 50m zooplankton biomass, SSB. 
Chl-a didn't improve AUC, dropped for now.

```{r, load.previous.dat}

# Where previous data is stored
sdmDir <- here(dir, "Data", "Previous.sdmTMB.Data")

# Load data
dat <- readRDS(paste(sdmDir, "/sardineTrainingData.rds", sep = ""))

# Project data
dat2 <- dat %>%
  filter(yr > 2002 & survey == 'cps') %>% 
  distinct() %>%
  group_by(yr) %>%
  mutate(hauls = n_distinct(haul)) %>%
  ungroup() %>% 
  select(yr, cruise, haul, lat, hauls) %>% 
  distinct()
# View(dat2)

test2003 <- dat %>% filter(yr == 2003 & survey == 'cps')

length(unique(test2003$haul))
# [1] 41
# View(test2003)

```

# Load new sdm data

There will likely be some hauls not previously accounted, but the number of hauls should be similar. Use 2003 as a test run.

```{r, load.new.dat}

areaTranBuffsum.polygons <- 
   read_sf(here(dir, "Data", "Shapefiles", "cluster.with.area.defs.shp"))
#View(areaTranBuff.polygons)

unique(areaTranBuffsum.polygons$Area_id)
# [1] 2 3 4 5 1

sort(unique(areaTranBuffsum.polygons$Area_id))
# [1] 1 2 3 4 5

areaTranBuff.polygons.2003 <- areaTranBuffsum.polygons %>%
  filter(Year == 2003)

colnames(areaTranBuffsum.polygons)

```

# Create density estimates to be used for biomass abundance indices

Note that here we want to include the absence data and are not concerned by lengths

```{r, biomass.density.area2}

area2poly.dens <- areaTranBuffsum.polygons %>%
  group_by(Year, Month, Day, tim_stp, clstGRP, Hauls, NbClust, Area_id, Area_km2,
           geometry) %>%
  # Sum over length bins for each cluster
  summarise(WGHT_bin = sum(WGHT_bin), .groups = "keep") %>%
  # Crate a density variable
  mutate(Density = WGHT_bin/Area_km2,
         # Create a grouping variable
         YearMon = paste0(Year, "-", sprintf("%s", Month))) %>%
  mutate(YearMon.nm = as_datetime(paste0(YearMon, "-01"), 
                              format = "%Y - %m - %d", 
                              tz = "America/Los_Angeles")) %>%
  ungroup() %>%
  # Assign unique row
  mutate(FID = row_number(.)) %>%
  select(-tim_stp)
# View(area2poly.dens)

plotNormalHistogram(area2poly.dens$Density)
range(area2poly.dens$Density)
# [1] 0.000 1621.613

```

# Extract polygon centers

```{r, area2.mesh.prestep}

# Extract polygon centers
area2poly.dens2 <- area2poly.dens %>%
  mutate(X = st_coordinates(st_centroid(.))[,1]/1000,
         Y = st_coordinates(st_centroid(.))[,2]/1000) %>%
  st_drop_geometry()

```

# Test/train split: 16 years for training, 6 years for out-of-model testing

```{r, split.data}

tims <- data.frame("YearMon.nm" = sort(unique(area2poly.dens2$YearMon.nm)))

# Single split: 75% train, last 3 time points as test
split <- timetk::time_series_split(
  tims,
  date_var = YearMon.nm,
  initial = floor(0.75 * nrow(tims)),
  assess = ceiling(0.25 * nrow(tims))
)

# Create training set
train_dates <- rsample::training(split)
train <- subset(area2poly.dens2, YearMon.nm %in% train_dates$YearMon.nm) %>%
  mutate(YearMon.nm2 = as.numeric(YearMon.nm))
table(train$YearMon.nm) 

# Create test set
test_dates  <- rsample::testing(split)
test <- subset(area2poly.dens2, YearMon.nm %in% test_dates$YearMon.nm) %>%
  mutate(YearMon.nm2 = as.numeric(YearMon.nm))
table(test$YearMon.nm)

```

# Next construct the mesh: 

see https://pbs-assess.github.io/sdmTMB/articles/basic-intro.html: cutoff is in the units of X and Y (km here), represents minimum distance between knots before a new mesh vertex is added). Muhling et al., 2025 tried a bunch of cutoff values, and chose one that 1) had good out-of-sample predictability, and 2) wasn't too "blocky". They found Values between ~ 50 and 200 were fairly reasonable. 

Here we look at the distance between clusters to assign knots using the summary function.

Important: cannot use anisotropy with barrier mesh (see warning when run sdmTMB).

```{r, construct.mesh}

# Read full resolution - Continental land masses and ocean islands, except Antarctica.
land.barrier <- read_sf(here(dir, "Data", "Shapefiles", "gshhg-shp-2.3.7",
                             "GSHHS_shp", "f", "GSHHS_f_L1.shp")) %>%
  # Project to same coordinate system
  st_transform(crs = st_crs(area2poly.dens)) %>%
  # Trim to area extent
  st_crop(st_bbox(area2poly.dens))

plot(land.barrier)

# Filter geometry type
land.barrier <- land.barrier[st_geometry_type(land.barrier) %in%
                               c("POLYGON", "MULTIPOLYGON"), ] %>%
  st_union()

# Wrap back into sf object
land.barrier <- st_sf(geometry = land.barrier)

# Check that union worked
st_is_valid(land.barrier)
# [1] TRUE

# Drop levels
land.barrier <- land.barrier %>% mutate(across(where(is.factor), droplevels))

# Get summary table for nearest neighbor
coords <- cbind(train$X, train$Y)
nn_dist <- nndist(coords)
summary(nn_dist)
#   Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# 0.4631   4.8171   8.6841  11.2030  13.9212 163.6689 

# Set cut-toff to 1 + 2/3 distance (mean or median - choose median if strong outliers)
## This is informed by the SPDE theory about mesh resolution relative to
## spatial range (check Lindgren et al. 2011 and related papers)
cutoff <- round(median(nn_dist) + median(nn_dist)*(2/3))
cutoff

# Cutoff values control the minimum distance between knots.
## It is generally better to start with a coarser mesh (larger cutoff)
## However there is a tradeoff on spatial predictability (more knots) and
## over fitting the time to process. If the day is irregularly distributed
## you can also try residual-based knot placement
meshTrain <- make_mesh(train, xy_cols = c("X", "Y"), cutoff = cutoff) 

# Check number of mesh nodes 
## If greater than >1,000–2,000 nodes, 
## you’re likely in trouble unless you have a lot of RAM.
length(mesh$mesh$loc[,1])

# proj_scaling should match units of (since we are working in m, but density
# is in km, divide by 1000)
barrier <- add_barrier_mesh(meshTrain, land.barrier, proj_scaling = 1000)

# Example plot code from ?add_barrier_mesh
mesh_df_water <- barrier$mesh_sf[barrier$normal_triangles, ]
mesh_df_land <- barrier$mesh_sf[barrier$barrier_triangles, ]

Mesh.with.barrier <-
ggplot() +
  geom_sf(data = land.barrier) +
  geom_sf(data = mesh_df_water, size = 1, colour = "blue") +
  geom_sf(data = mesh_df_land, size = 1, colour = "green")

Mesh.with.barrier

ggsave2(filename=paste("Mesh.with.barrier.jpeg",sep=''),
        plot=Mesh.with.barrier, device="jpeg", 
        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
        unit="cm", limitsize = FALSE)

plot(barrier)
points(train$X, train$Y, col = "red", pch = 19, 
       cex = 0.2)

```

# Now build the models:

## For a "Non-spatial" model without covars

This is a test run to see if we can explain the distribution by discrete time and OM spatial area 

```{r, make.base.nonsptl.model}

set.seed(1)

fit0 <- sdmTMB(Density ~ 1 + Area_id, 
               data = train,
               mesh = barrier,
               # Fit model by year and month
               time = "YearMon.nm2",
               # Use "ar1" if you expect temporal autocorrelation
               # Otherwise, use default "rw"
               spatiotemporal = "ar1",
               spatial = "off", 
               extra_time = test$YearMon.nm2,
               family = tweedie(link = "log"), 
               anisotropy = FALSE)

# Check model skill, fit, and convergence
sanity(fit0)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
# ✖ `sigma_E` is larger than 100
# ℹ Consider simplifying the model or adding priors
# ✔ Range parameter doesn't look unreasonably large

summary(fit0)

# Predict (gives a whole new df, unlike GAM)
pTrain0 <- predict(fit0, newdata = train, type = "response")
pTest0 <- predict(fit0, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain0))$r.squared # 0.8749693
summary(lm(Density ~ est, data = pTest0))$r.squared # 0.001235628

# MAE
mean(abs(pTrain0$Density - pTrain0$est)) # 6.788811
mean(abs(pTest0$Density - pTest0$est)) # 9.126628

mean(train$Density)
# [1] 19.32284
sd(train$Density)
# [1] 87.73338

mean(test$Density)
# [1] 9.020028                                                                  
sd(test$Density)
# [1] 58.06881

# Partial plots. Could make nicer later
## Area_id
Area_idPartial0 <- visreg(fit0, xvar = "Area_id", gg = TRUE)
Area_idPartial0 

```

## For a "Spatial" model without covars

This is a test run to see if we can explain the distribution by spatialtemporal distribution.

```{r, make.base.sptl.model}

set.seed(1)

fit1 <- sdmTMB(Density ~ 1 + Area_id, 
               data = train, 
               mesh = barrier,
               spatial = "on",
               time = "YearMon.nm2",
               extra_time = test$YearMon.nm2,
               spatiotemporal = "ar1",
               family = delta_gamma(), 
               anisotropy = FALSE)

# Check model skill, fit, and convergence
sanity(fit1)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
# ✖ No sigma parameters are > 100

summary(fit1)

# Predict (gives a whole new df, unlike GAM)
pTrain1 <- predict(fit1, newdata = train, type = "response")
pTest1 <- predict(fit1, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain1))$r.squared # 0.7182932
summary(lm(Density ~ est, data = pTest1))$r.squared # 0.00090351

# MAE
mean(abs(pTrain1$Density - pTrain1$est)) # 15.36969
mean(abs(pTest1$Density - pTest1$est)) # 14.27339

mean(train$Density)
# [1] 18.10529                                                                     
sd(train$Density)
# [1] 91.00154

mean(test$Density)
# [1] 11.67489                                                                     
sd(test$Density)
# [1] 32.81762

# Partial plots. Could make nicer later
## Area_id
Area_idPartial1 <- visreg(fit1, xvar = "Area_id", gg = TRUE)
Area_idPartial1 

```
