---
title: "BuildSdmTMB"
author: "Stephanie Hopkins"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    
---

```{r setup, include=FALSE}

# Run first if not already installed
# remotes::install_github("pbs-assess/sdmTMBextra", dependencies = TRUE)

rm(list=ls())
gc()

load.lib <- c("tidyverse", "data.table", "cowplot", "sf", "rcompanion",
              "pROC", "here", "sdmTMB", "sdmTMBextra", "visreg",
              "ggforce", # enables plot_anisotropy
              "spatstat",# Get summary table for nearest neighbor
              "rsample", "timetk", "zoo") 

install.lib <- load.lib[!load.lib %in% installed.packages()]

for(lib in install.lib) install.packages(lib,dependencies=TRUE)
sapply(load.lib,require,character=TRUE)

sf_use_s2(FALSE)

dir <- here(getwd())

```

# Load previous sdm data

Build new adult sardine SDM using environmental predictors from the 3km WC15 ROMS.
Vars are SST, surface chl-a, upper 50m zooplankton biomass, SSB. 
Chl-a didn't improve AUC, dropped for now.

```{r, load.previous.dat}

# Where previous data is stored
sdmDir <- here(dir, "Data", "Previous.sdmTMB.Data")

# Load data
dat <- readRDS(paste(sdmDir, "/sardineTrainingData.rds", sep = ""))

# Project data
dat2 <- dat %>%
  filter(yr > 2002 & survey == 'cps') %>% 
  distinct() %>%
  group_by(yr) %>%
  mutate(hauls = n_distinct(haul)) %>%
  ungroup() %>% 
  select(yr, cruise, haul, lat, hauls) %>% 
  distinct()
# View(dat2)

test2003 <- dat %>% filter(yr == 2003 & survey == 'cps')

length(unique(test2003$haul))
# [1] 41
# View(test2003)

```

# Load new sdm data

There will likely be some hauls not previously accounted, but the number of hauls should be similar. Use 2003 as a test run.

```{r, load.new.dat}

areaTranBuffsum.polygons <- 
   read_sf(here(dir, "Data", "Shapefiles", "cluster.with.area.defs.shp"))
#View(areaTranBuff.polygons)

unique(areaTranBuffsum.polygons$Area_id)
# [1] 2 3 4 5 1

sort(unique(areaTranBuffsum.polygons$Area_id))
# [1] 1 2 3 4 5

areaTranBuff.polygons.2003 <- areaTranBuffsum.polygons %>%
  filter(Year == 2003)

colnames(areaTranBuffsum.polygons)

```

# Create density estimates to be used for biomass abundance indices

Note that here we want to include the absence data and are not concerned by lengths

```{r, biomass.density.area2}

area2poly.dens <- areaTranBuffsum.polygons %>%
  group_by(Year, Month, Day, tim_stp, clstGRP, Hauls, NbClust, Area_id, Area_km2,
           geometry, age, len_bin) %>%
  summarise(WghtLenBin = sum(WghtLenBin), .groups = "drop") %>%
  mutate(
    # Calculate density
    Density = WghtLenBin / Area_km2,

    # Create proper date object
    Date = as.Date(paste(Year, Month, Day, sep = "-"), format = "%Y-%m-%d"),
    
    # Convert to numeric
    Year = as.numeric(Year),
    Month = as.numeric(Month),
    Day = as.numeric(Day),

    # Create YearQuarter
    YearQuarter = as.yearqtr(Date),

    # Extract numeric quarter
    Quarter = case_when(
      str_detect(YearQuarter, "Q1") ~ 1,
      str_detect(YearQuarter, "Q2") ~ 2,
      str_detect(YearQuarter, "Q3") ~ 3,
      str_detect(YearQuarter, "Q4") ~ 4
    ),

    # Create numeric time step index from baseline year (e.g., 2003)
    NumericQuarter = (Year - min(Year)) * 4 + Quarter,

  ) %>%
  select(-tim_stp)

# View(area2poly.dens)

plotNormalHistogram(area2poly.dens$Density)
range(area2poly.dens$Density)
# [1] 0.000 912149.7

```

# Extract polygon centers

```{r, area2.mesh.prestep}

# Extract polygon centers
area2poly.dens2 <- area2poly.dens %>%
  mutate(X = st_coordinates(st_centroid(.))[,1]/1000,
         Y = st_coordinates(st_centroid(.))[,2]/1000) %>%
  st_drop_geometry()

```

# Restructure data to include P/A

```{r, presence.absence.col}


Absent.dat <- area2poly.dens2 %>%
  filter(is.na(age)) %>%
  select(-c(age,len_bin)) %>%
  mutate(PA = 0)

Present.dat <- area2poly.dens2 %>%
  filter(!is.na(age))

Full.dat <- left_join(Present.dat, Absent.dat) %>%
  mutate(FID = row_number())

Full.dat$PA[is.na(Full.dat$PA)] <- 1
# View(Full.dat)

```

# Find missing time steps

```{r, missing.quarters}

# Generate complete sequence
full_seq <- data.frame(
  YearQuarter = seq(min(Full.dat$YearQuarter), 
                    max(Full.dat$YearQuarter), 
                    by = 0.25)
  )

missing_qtrs <- full_seq %>%
  anti_join(Full.dat, by = "YearQuarter") %>%
  mutate(Year = as.numeric(substr(YearQuarter, 1, 4)),
         Quarter = 
           case_when(
             str_detect(YearQuarter, "Q1") ~ 1,
             str_detect(YearQuarter, "Q2") ~ 2,
             str_detect(YearQuarter, "Q3") ~ 3,
             str_detect(YearQuarter, "Q4") ~ 4
             )
    ) %>%
  mutate(NumericQuarter = (Year - min(Year)) * 4 + Quarter) %>%
  select(NumericQuarter)

```

# Test/train split: 16 years for training, 6 years for out-of-model testing

Here you need to have a datetime object. But this will not be used in the model

```{r, split.data}

tims <- data.frame("YearQuarter" = sort(unique(Full.dat$YearQuarter))) %>%
  mutate(
    YearQuarter = as.yearqtr(YearQuarter, format = "%Y Q%q")
  ) %>%
  arrange(YearQuarter)  # ensures sorted order

# Single split: 75% train, last 3 time points as test
split <- timetk::time_series_split(
  tims,
  date_var = YearQuarter,
  initial = floor(0.75 * nrow(tims)),
  assess = ceiling(0.25 * nrow(tims))
)

# Create training set
train_dates <- rsample::training(split)
train <- subset(Full.dat, YearQuarter %in% train_dates$YearQuarter)
tail(train$YearQuarter)

# Create test set
test_dates  <- rsample::testing(split)
test <- subset(Full.dat, YearQuarter %in% test_dates$YearQuarter) 
head(test$YearQuarter)

```

# Next construct the mesh: 

see https://pbs-assess.github.io/sdmTMB/articles/basic-intro.html: cutoff is in the units of X and Y (km here), represents minimum distance between knots before a new mesh vertex is added). Muhling et al., 2025 tried a bunch of cutoff values, and chose one that 1) had good out-of-sample predictability, and 2) wasn't too "blocky". They found Values between ~ 50 and 200 were fairly reasonable. 

Here we look at the distance between clusters to assign knots using the summary function.

Important: cannot use anisotropy with barrier mesh (see warning when run sdmTMB).

```{r, construct.mesh}

# Read full resolution - Continental land masses and ocean islands, except Antarctica.
land.barrier <- read_sf(here(dir, "Data", "Shapefiles", "gshhg-shp-2.3.7",
                             "GSHHS_shp", "f", "GSHHS_f_L1.shp")) %>%
  # Project to same coordinate system
  st_transform(crs = st_crs(area2poly.dens)) %>%
  # Trim to area extent
  st_crop(st_bbox(area2poly.dens))

plot(land.barrier)

# Filter geometry type
land.barrier <- land.barrier[st_geometry_type(land.barrier) %in%
                               c("POLYGON", "MULTIPOLYGON"), ] %>%
  st_union()

# Wrap back into sf object
land.barrier <- st_sf(geometry = land.barrier)

# Check that union worked
st_is_valid(land.barrier)
# [1] TRUE

# Drop levels
land.barrier <- land.barrier %>% mutate(across(where(is.factor), droplevels))

# Get summary table for nearest neighbor
coords <- cbind(train$X, train$Y)
nn_dist <- nndist(coords)
summary(nn_dist)
#   Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# 0.4631   4.8171   8.6841  11.2030  13.9212 163.6689 

# Set cut-toff to 1 + 2/3 distance (mean or median - choose median if strong outliers)
## This is informed by the SPDE theory about mesh resolution relative to
## spatial range (check Lindgren et al. 2011 and related papers)
cutoff <- round(median(nn_dist) + median(nn_dist)*(2/3))
cutoff

# Cutoff values control the minimum distance between knots.
## It is generally better to start with a coarser mesh (larger cutoff)
## However there is a tradeoff on spatial predictability (more knots) and
## over fitting the time to process. If the day is irregularly distributed
## you can also try residual-based knot placement
meshTrain <- make_mesh(train, xy_cols = c("X", "Y"), cutoff = 50) 

# Check number of mesh nodes 
## If greater than >1,000–2,000 nodes, 
## you’re likely in trouble unless you have a lot of RAM.
length(meshTrain$mesh$loc[,1])
# [1] 229

# proj_scaling should match units of (since we are working in m, but density
# is in km, divide by 1000)
barrier <- add_barrier_mesh(meshTrain, land.barrier, proj_scaling = 1000)

## If greater than >1,000–2,000 nodes, 
## you’re likely in trouble unless you have a lot of RAM.
length(barrier$mesh$loc[,1])
# [1] 229

# Example plot code from ?add_barrier_mesh
mesh_df_water <- barrier$mesh_sf[barrier$normal_triangles, ]
mesh_df_land <- barrier$mesh_sf[barrier$barrier_triangles, ]

Mesh.with.barrier <-
ggplot() +
  geom_sf(data = land.barrier) +
  geom_sf(data = mesh_df_water, size = 1, colour = "blue") +
  geom_sf(data = mesh_df_land, size = 1, colour = "green")

Mesh.with.barrier
# [1] 231

ggsave2(filename=paste("Mesh.with.barrier.jpeg",sep=''),
        plot=Mesh.with.barrier, device="jpeg", 
        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
        unit="cm", limitsize = FALSE)

plot(barrier)
points(train$X, train$Y, col = "red", pch = 19, 
       cex = 0.2)

```

# Check for redundancy:


```{r}


X <- model.matrix(Density ~ 0 + as.factor(age) + as.factor(len_bin), data = train)
library(caret)
findLinearCombos(X)

table(train$age)
table(train$len_bin)

#-------------------------------------------------------------------------------
# Debug size constraint
#-------------------------------------------------------------------------------

#Identify hidden levels that may bog down the model
sapply(train, function(x) if (is.factor(x)) nlevels(x))

summary(dplyr::select_if(train, is.factor))

#-------------------------------------------------------------------------------

```

# Now build the models:

## For a "Non-spatial" model without covars

This is a test run to see if we can explain the distribution by discrete time and OM spatial area 

```{r, make.base.nonsptl.model}

set.seed(1) 

# Define extra time steps to estimate
est.missing.time <- unique(c(missing_qtrs$NumericQuarter, test$NumericQuarter)) %>%
  sort

fit0 <- sdmTMB(Density ~ 1,
               data = train, 
               mesh = barrier,
               # Fit model by year and month
               time = "NumericQuarter",
               # Use "ar1" or "rw" if you expect temporal autocorrelation
               # Otherwise, use default "iid" (independent per time step)
               spatiotemporal = "off",
               spatial = "off", 
               extra_time = est.missing.time,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE,
               silent = FALSE)


# Check model skill, fit, and convergence

sanity(fit0)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
# ✔ No sigma parameters are > 100

summary(fit0)

# Predict (gives a whole new df, unlike GAM)
pTrain0 <- predict(fit0, newdata = train, type = "response")

pTest0 <- predict(fit0, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain0))$r.squared # 0
summary(lm(Density ~ est, data = pTest0))$r.squared # 0

# MAE
mean(abs(pTrain0$Density - pTrain0$est)) # 1304.142
mean(abs(pTest0$Density - pTest0$est)) # 665.5521

mean(train$Density)
# [1] 666.9524
sd(train$Density)
# [1] 19792.58

mean(test$Density)
# [1] 2.688917                                                                  
sd(test$Density)
# [1] 42.18626

```

## Try with temporal autocorrelation

```{r, make.nonsptl.model1}

set.seed(1) 

fit1 <- sdmTMB(Density ~ 1,
               data = train, 
               mesh = barrier,
               # Fit model by year and month
               ## Note if time is specified, spatiotemporal correlation is tested
               ## by default
               time = "NumericQuarter",
               # Use "ar1" or "rw" if you expect temporal autocorrelation
               # Otherwise, use default "iid" (independent by time step)
               spatiotemporal = "ar1",
               spatial = "off", 
               extra_time = est.missing.time,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE,
               # Show progress
               silent = FALSE)


# Check model skill, fit, and convergence

sanity(fit1)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✖ `ln_tau_E` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
# ✖ `ln_phi` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
# ✖ `sigma_E` is larger than 100
# ℹ Consider simplifying the model or adding priors

summary(fit1)

# Predict (gives a whole new df, unlike GAM)
pTrain1 <- predict(fit1, newdata = train, type = "response")

pTest1 <- predict(fit1, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain1))$r.squared # 0.07398241
summary(lm(Density ~ est, data = pTest1))$r.squared # 5.998492e-05

# MAE
mean(abs(pTrain1$Density - pTrain1$est)) # 1304.142
mean(abs(pTest1$Density - pTest1$est)) # 665.5521

mean(train$Density)
# [1] 666.9524
sd(train$Density)
# [1] 19792.58

mean(test$Density)
# [1] 2.688917                                                                  
sd(test$Density)
# [1] 42.18626

```

## Try with length and age

```{r, make.nonsptl.model2}

set.seed(1) 

# Add ran age and length random effects with temporal autocorrelation
fit2 <- sdmTMB(Density ~ 0 + as.factor(age) + as.factor(len_bin),
               data = train, 
               mesh = barrier,
               # Fit model by year and month
               ## Note if time is specified, spatiotemporal correlation is tested
               ## by default
               time = "NumericQuarter",
               # Use "ar1" or "rw" if you expect temporal autocorrelation
               # Otherwise, use default "iid" (independent by time step)
               spatiotemporal = "ar1",
               spatial = "off", 
               extra_time = est.missing.time,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE,
               # Show progress
               silent = FALSE)

# Check model skill, fit, and convergence

sanity(fit2)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✖ `ln_tau_E` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
# ✖ `ln_phi` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
# ✖ `sigma_E` is larger than 100
# ℹ Consider simplifying the model or adding priors

summary(fit2)

# Predict (gives a whole new df, unlike GAM)
pTrain2 <- predict(fit2, newdata = train, type = "response")

pTest2 <- predict(fit2, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain2))$r.squared # 0.07398241
summary(lm(Density ~ est, data = pTest2))$r.squared # 5.998492e-05

# MAE
mean(abs(pTrain2$Density - pTrain2$est)) # 1304.142
mean(abs(pTest2Density - pTest2$est)) # 665.5521

mean(train$Density)
# [1] 666.9524
sd(train$Density)
# [1] 19792.58

mean(test$Density)
# [1] 2.688917                                                                  
sd(test$Density)
# [1] 42.18626

```


## For a "Spatial" model without covars

This is a test run to see if we can explain the distribution by spatialtemporal distribution.

```{r, make.base.sptl.model}

set.seed(1)

fit1 <- sdmTMB(Density ~ 0 + as.factor(age) + as.factor(len_bin), 
               data = train, 
               mesh = barrier,
               time_varying = ~ 1,
               # Fit model by year and quarter
               time = "NumericQuarter",
               # Use "ar1" if you expect temporal autocorrelation
               # Otherwise, use default "rw"
               spatiotemporal = "ar1",
               spatial = "on", 
               extra_time = est.missing.time,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE)

```

```{r}

```

# Check model skill, fit, and convergence
sanity(fit1)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
# ✖ No sigma parameters are > 100

summary(fit1)

# Predict (gives a whole new df, unlike GAM)
pTrain1 <- predict(fit1, newdata = train, type = "response")
pTest1 <- predict(fit1, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain1))$r.squared # 0.7182932
summary(lm(Density ~ est, data = pTest1))$r.squared # 0.00090351

# MAE
mean(abs(pTrain1$Density - pTrain1$est)) # 15.36969
mean(abs(pTest1$Density - pTest1$est)) # 14.27339

mean(train$Density)
# [1] 18.10529                                                                     
sd(train$Density)
# [1] 91.00154

mean(test$Density)
# [1] 11.67489                                                                     
sd(test$Density)
# [1] 32.81762

# Partial plots. Could make nicer later
## Area_id
Area_idPartial1 <- visreg(fit1, xvar = "Area_id", gg = TRUE)
Area_idPartial1 

```
