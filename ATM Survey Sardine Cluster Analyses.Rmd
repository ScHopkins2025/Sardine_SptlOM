---
title: "ATM Survey Sardine Cluster Analyses"
author: "Stephanie Hopkins"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    
---

```{r setup, include=FALSE}

rm(list=ls())
gc()

load.lib <- c("tidyverse", "data.table", "cowplot", "sf", "ggspatial", "ggnewscale", "rcompanion", "rnaturalearth", "rnaturalearthhires", "r4ss", "patchwork", "zoo", "suncalc", "here", "furrr")

install.lib <- load.lib[!load.lib %in% installed.packages()]

for(lib in install.lib) install.packages(lib,dependencies=TRUE)
sapply(load.lib,require,character=TRUE)

sf_use_s2(FALSE)

dir <- here(getwd())

survey.path <- "/Survey.Data/"

Figure.path <- "Figures/"

source("~/Final.OM.spatial.structure/functions.R")

```

# Read data

Remember to convert length to cm to be able to use VBGF!

```{r, read.data}

full.series <- 
  fread(paste(here(dir), 
              "/Data", survey.path,
              "2003.2024.ATM.Survey.with.integrated.specimen.data.csv", 
              sep = ""), sep = ",") %>%
  # convert mm to cm to be able to use VBGF
  mutate(standard_length = standard_length/10) 

length(unique(full.series$collection))
# [1] 2675

colnames(full.series)

```

# Mean of the haul geometry

Note that we are taking the mean of the starting point and ending point to denote the centroid.

```{r, mean.haul.geom}

#-------------------------------------------------------------------------------
# For Shooting location
#-------------------------------------------------------------------------------

range(full.series$start_latitude)
# [1] 28.6513 54.3997
Lat1 <- full.series$start_latitude

mean(full.series$start_latitude)
# [1] 39.06714

range(full.series$start_longitude)
# [1] -134.0793 -114.7928
Lon1 <- full.series$start_longitude

mean(full.series$start_longitude)
# [1] -122.4603

#-------------------------------------------------------------------------------
# For hauling location
#-------------------------------------------------------------------------------

range(full.series$stop_latitude)
# [1] 28.6548 54.4157
Lat2 <- full.series$stop_latitude

range(full.series$stop_longitude)
# [1] -134.0325 -114.8483
Lon2 <- full.series$stop_longitude

#-------------------------------------------------------------------------------
# Find the mean of Shooting and Hauling location
#-------------------------------------------------------------------------------

LAT <- data.table(Lat1, Lat2)
LON <- data.table(Lon1, Lon2)

LAT <- rowMeans(LAT)
LON <- rowMeans(LON)
 
plot(LON, LAT)

```

# Verify georefrence values

This will tell us whether or not to clip the data when we make plots.

```{r lat.lon.extent}

range(LAT)
# [1] 28.65305 54.40770

range(LON)
# [1] -134.0559 -114.8205

```

# Spatialize the transect data 

For each haul transect, group together the starting and stopping latitudinal longitudinal coordinates and create a line string spatial object.

```{r, spatial.transects.defs}

# Create a linestring for each transect
trip_rect <- make_transect_sf(full.series)
  
```

# Determine fishing duration

Note that for initial plots, CPUE was defined as kg/hr, but since we are estimating biomass abundance, we area is needed instead. For each haul transect, the same CPUE values are present for both computational methods.

```{r, convert.POSIXct.object}

full.series.sf <- trip_rect %>%
  group_by(cruise, ship, haul, equilibrium_time.utc, collection, geometry) %>%
  mutate(haulback_time.pacific = 
           strptime(haulback_time.utc, "%Y-%m-%d %H:%M:%S", 
                    tz = "America/Los_Angeles"),
         equilibrium_time.pacific = 
           strptime(equilibrium_time.utc, "%Y-%m-%d %H:%M:%S", 
                    tz = "America/Los_Angeles")) %>%
  ungroup() 

head(unique(full.series.sf$haulback_time.pacific))
head(unique(full.series.sf$equilibrium_time.pacific))
  
```

There are many different values used to assess length at age in the region and there are high levels of variability between individuals, years, seasons, and cohorts. For example K was historically nearly double that of the most recent publication (Wildermuth et al., 2024), where previous values were L(inf) = 31 cm, and K = 0.460 (Female) off the coast of California >> Phillips, J.B., 1948. Growth of the sardine *Sardinops caerulea* 1941-42 through 1945-47. Calif. Dept. Fish and Game. Calif. Fish. Bull. 71:33 p. Below, we investigate the potential of using the Wildermuth et al., 2024 values for L(inf), K, and t0 in the extended data set with all lengths considered.

```{r, extract.ss.growth.params}

# Where ss output files are
model_dir <- here(getwd(), "Data", "SSMSE.Base.Model.Data", "start2001",
                  "constGrowthMidSteepNewSelex_OM")

# Read ssouput file
OM_age.length <- SSgetoutput(dir = file.path(model_dir))

# Check structure
str(OM_age.length)

# How many age classes were fed into the model (model forecasts the rest)
OM_age.length$replist1$agebins
# [1] 0 1 2 3 4 5 6 7 8

#-------------------------------------------------------------------------------
# Below are the values needed for VBGF
#-------------------------------------------------------------------------------

# Estimated age
Linf <- OM_age.length$replist1$Growth_Parameters$Linf
Linf
# [1] 25.153

K <- OM_age.length$replist1$Growth_Parameters$K
K
# [1] 0.273111

# A1 is the starting length (true age 0 values)
A1 <- OM_age.length$replist1$Growth_Parameters$A1
A1 
# [1] 0.5

# A2 is either fixed at a theoretical maximum or inf (999)
A2 <- OM_age.length$replist1$Growth_Parameters$A2
A2 
# [1] 999

# Smallest observed length in the data set
L_a_A1 <- OM_age.length$replist1$Growth_Parameters$L_a_A1
L_a_A1
# [1] 13.1744

# You can also index them all at once
OM_age.length$replist1$Growth_Parameters

#-------------------------------------------------------------------------------
# To find t0, which is estimated in the ss model rather than explicit
#-------------------------------------------------------------------------------

# Equation used
t0 <- A1 + (log(1-L_a_A1/Linf))/K
t0
# [1] -2.216315

# Undefined parameter in the model handbook thought to be t0
A_a_L0 <- OM_age.length$replist1$Growth_Parameters$A_a_L0
A_a_L0
# [1] -2.2163

# Things to look up later for types of errors (!= fleets, != # of params)
OM_age.length$replist1$age_error_mean
OM_age.length$replist1$age_error_sd

# Returns the Maximum Likelihood Estimates (MLE)
OM_age.length$replist1$growthseries[1,5:15]

# Create a vector of mle values
mle.vals <- c(t(OM_age.length$replist1$growthseries[1,5:15]))

# Create a vector of ages
age.vals <- seq(0,10,1)

# Combine in a data frame ages and mle values
ss.age.len <- data.frame(mle.vals, age.vals)

# To view parameterizations and associated sd values
OM_age.length$replist1$parameters 

```

# Estimate age

Use the values extracted from the ss model for pacific sardine

```{r, apply.inverse.vbgg}

# Apply VBGF
Sardine <- full.series.sf %>% 
  filter(scientific_name == "Sardinops sagax") %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry, 
           LAT, LON) %>%
  mutate(age = round((-1/K)*log(1-(standard_length/Linf)) + A_a_L0)) %>% 
  ungroup() %>%
  group_by(age) %>%
  mutate(mean.length.age = mean(standard_length)) %>%
  ungroup() %>%  
  distinct()

# Explore NaN values
sardine.nans.pt1 <- Sardine %>%
  filter(is.na(age))

# How many observations are lost
dim(sardine.nans.pt1)
# [1] 1118  41

# What lengths are not represented
sardine.nans.pt2 <- sardine.nans.pt1 %>%
  distinct(standard_length)

# Vector of lengths not represented
c(t(sardine.nans.pt2))
#  [1] 25.2 25.8 27.1 25.3 26.7 25.5 25.6 26.2 25.4 25.7 26.0 25.9 26.8 28.9 26.1
# [16] 26.4 26.3 28.4 27.2 27.3 27.4 28.1 26.5 26.9 27.8 26.6 27.5 27.0 27.6 28.0
# [31] 29.2 27.9 28.2 27.7 29.1

# Plot age length per haul, average age per haul, and ss mle values 
# (when excluding larger lengths from the data)...not a sensitivity analysis is 
# still needed
ggplot() + 
  geom_point(data=na.omit(Sardine), aes(x=age, y=standard_length)) +
  geom_line(data=na.omit(Sardine), aes(x=age, y=mean.length.age), col = "red") +
  geom_line(data=ss.age.len, aes(x=age.vals, y=mle.vals), col = "blue") + 
  scale_y_continuous(breaks = seq(0, 31, by = 2)) +
  scale_x_continuous(breaks = seq(0, 20, by = 1)) +
  ylab("Standard Length (cm)") +
  xlab("Estimate Age (yr)") +
  theme_classic()

```

# Define length bin definitions for Sardine only

Because aging is uncertain, propose length bins where the first 100mm are grouped (considered age 0), and where every 2.25 cm is added.

```{r, length.bin.defs}

len.bins <- Sardine

head(len.bins)

range(len.bins$standard_length)
# [1]  2.5 29.2

#-------------------------------------------------------------------------------
# Check lowest intervals
#-------------------------------------------------------------------------------

len100 <- len.bins %>%
  filter(standard_length <= 10) %>%
  mutate(len.bin = "100")

sort(unique(len100$age))
# [1] -2 -1 0

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len125 <- len.bins %>%
  filter(standard_length > 10 & standard_length < 12.6) %>%
  mutate(len.bin = "125")

sort(unique(len125$age))
# [1] 0

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len150 <- len.bins %>%
  filter(standard_length > 12.5 & standard_length < 15.1) %>%
  mutate(len.bin = "150")

sort(unique(len150$age))
# [1] 0 1

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len175 <- len.bins %>%
  filter(standard_length > 15 & standard_length < 17.6) %>%
  mutate(len.bin = "175")

sort(unique(len175$age))
# [1] 1 2

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len200 <- len.bins %>%
  filter(standard_length > 17.5 & standard_length < 20.1) %>%
  mutate(len.bin = "200")

sort(unique(len200$age))
# [1] 2 3 4

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len225 <- len.bins %>%
  filter(standard_length > 20 & standard_length < 22.6) %>%
  mutate(len.bin = "225")

sort(unique(len225$age))
# [1] 4 5 6

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len250 <- len.bins %>%
  filter(standard_length > 22.5 & standard_length < 25.1) %>%
  mutate(len.bin = "250")

sort(unique(len250$age))
# [1] 6 7 8 9 10 11 12 13 15 16

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm and group values
#-------------------------------------------------------------------------------

len250plus <- len.bins %>%
  filter(standard_length > 25.5) %>%
  mutate(len.bin = "250+")

#-------------------------------------------------------------------------------
# Re-code negative age to 0
#-------------------------------------------------------------------------------

len100.negs <- len100 %>%
  filter(age < 0) %>%
  mutate(age = 0)

len100.0s <- len100 %>%
  filter(age == 0)

len100.new <- rbind(len100.negs,  len100.0s)

#-------------------------------------------------------------------------------
# Join data sets 
#-------------------------------------------------------------------------------

len.bins2 <- rbind(len100.new, len125, len150, len175, len200, len225, len250, len250plus)

#-------------------------------------------------------------------------------
# Re-code Sardines NaNs and to 13+ ages to 10+
#-------------------------------------------------------------------------------

len.bins2.ok <- len.bins2 %>%
  filter(!is.na(age) & age < 10)

len.bins2.no <- len.bins2 %>%
  filter(age > 9) %>%
  mutate(age = 10)

len250plus <- len250plus %>%
  filter(is.na(age)) %>%
  mutate(age = 10)

len.bins2 <- rbind(len.bins2.ok, len.bins2.no, len250plus)

```

# Calculate the total catch weight for each length

Note that here we are standardizing and correcting the data to remove noise and better match the outputs in the commercial data sets.

```{r, calculate.mean.weight.per.lenbin}

# Note that the sum of weight (individual weight obs) should equal the subsample_weight (checked in data prep code)
Sardine2 <- Sardine %>% 
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry, 
           duration, LAT, LON) %>%
  # Sum over the number of sampled weights and find the total weight of the catch 
  mutate(Total_N = sum(subsample_count), 
         Total_W = sum(subsample_weight+remaining_weight)) %>%
  ungroup() %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry,
           standard_length, Total_N, Total_W, duration, LAT, LON) %>% 
  # Remove entries where there are no length observations and sum sampled number 
  # and weight
  filter(!is.na(standard_length)) %>%
  summarise(NBRE_IN = sum(subsample_count), 
            WGHT_IN = sum(as.numeric(weight)),
            .groups = "keep") %>%
  ungroup() %>% 
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry, 
           duration, LAT, LON) %>% 
  # Determine portion of lengths represented in the catch and find the mean 
  # weight at length
  mutate(PropLen = NBRE_IN/Total_N, 
         TheoreticalWgt_i = (7.52e-06*((standard_length)^3.23))) %>%
  ungroup() %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry, 
           duration, LAT, LON) %>%
  # Take the proportion of the mean weight at length in the catch 
  mutate(EstimatedWeight = sum(PropLen*TheoreticalWgt_i)*Total_N) %>%
  ungroup() %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry,
           Total_N, duration, LAT, LON) %>%
  #  Adjust the total weight (sampled + unsampled weight) based on the proportion
  # of mean weight at length
  mutate(Correction.factor = sum(Total_W)/EstimatedWeight) %>%
  ungroup() %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry,
           Total_N, duration, LAT, LON) %>%
  # Multiple the proportion of lengths represented in the catch by the adjusted
  # proportion of weight represented by length
  mutate(CatchWeightLen = Total_N*PropLen*TheoreticalWgt_i*Correction.factor) %>%
  ungroup()
# View(Sardine)

range(Sardine$Total_W)
# [1]0.0005 1559162.8353
# View(Sardine)

```

## Check outputs

```{r, check.length.distributions}

dim(Sardine)
# [1] 10342   19

dim(distinct(Sardine))
# [1] 10342   19

summary(distinct(Sardine))

sum(Sardine$NBRE_IN)
# [1] 800004

Sardine$equilibrium_time.pacific <- as.factor(Sardine$equilibrium_time.pacific)

plotNormalHistogram(Sardine$standard_length)

ggplot(data = na.omit(Sardine), aes(x=standard_length, y=PropLen, 
                                    col=equilibrium_time.pacific)) +
  geom_bar(stat = "identity", position = "stack") +
  theme(legend.position="none") 

range(Sardine$standard_length)
# [1] 2.5 29.2

```

# Calculate CPUE by length bin for Sardine only

This is the same procedure as above, only in place of age we now have length bins.

```{r, length.bin.cpues}

len.bins3 <- len.bins2 %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, len.bin, duration, 
           geometry) %>%
  mutate(Catch.wt = sum(CatchWeightLen)) %>%
  mutate(log1p.CPUE.trp = log1p(Catch.wt / as.numeric(duration))) %>%
  ungroup() %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, len.bin, geometry) %>%
  mutate(Year = as.numeric(substr(equilibrium_time.pacific,1,4)),
         Month = as.numeric(substr(equilibrium_time.pacific,6,7))) %>%
  ungroup() %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, len.bin, geometry) %>%
  mutate(time.step = paste(Year," - ", Month, sep="")) %>%
  ungroup() %>%
  st_transform(.,crs = 4326) %>%
  select(cruise, ship, haul, collection, age, len.bin, Catch.wt, log1p.CPUE.trp,
         Year, Month, time.step, geometry, equilibrium_time.pacific, 
         standard_length, NBRE_IN, Total_N, LON, LAT, CatchWeightLen) %>%
  mutate(keys = paste(cruise, ship, haul, collection, equilibrium_time.pacific,
                      geometry, sep = "-")) %>%
  distinct()

# Find min and max values for plots
lim.max <- max(len.bins3$log1p.CPUE.trp)
lim.min <- min(len.bins3$log1p.CPUE.trp)

# View(len.bins3)

```

#  Calculate CPUE by length bin for Other species

These values will be used for determining absent values.

```{r, Absent.bin}

Others <- full.series.sf %>% 
  filter(scientific_name != "Sardinops sagax") %>%
  mutate(scientific_name = "Other")  %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, duration, geometry) %>%
  mutate(Catch.wt = 0, 
         log1p.CPUE.trp = 0, 
         len.bin = NA, 
         standard_length = NA, 
         NBRE_IN = 0, 
         Total_N = 0, 
         CatchWeightLen = 0,
         age = NA) %>%
  ungroup() %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, geometry) %>%
  mutate(Year = as.numeric(substr(equilibrium_time.pacific,1,4)),
         Month = as.numeric(substr(equilibrium_time.pacific,6,7))) %>%
  ungroup() %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, geometry) %>%
  mutate(Season =
           case_when(Month %in% c(3:5) ~ "Spring",  
                     Month %in% c(6:8) ~ "Summer",
                     Month %in% c(9:11) ~ "Fall")) %>%
  ungroup() %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, len.bin, geometry) %>%
  mutate(time.step = paste(Year," - ",  sep="")) %>%
  ungroup() %>%
  st_transform(.,crs = 4326) %>%
  select(cruise, ship, haul, collection, age, len.bin, Catch.wt, log1p.CPUE.trp,
         Year, Month,  time.step, geometry, equilibrium_time.pacific, 
         standard_length, NBRE_IN, Total_N, LON, LAT, CatchWeightLen) %>%
  mutate(keys = paste(cruise, ship, haul, collection, equilibrium_time.pacific,
                      geometry, sep = "-")) %>%
  distinct()

```

# Sum tables and simplify

Check for unique keys where no sardines were caught. These will be treated as true absent values.

```{r, view.absent.present.data.structure}

# Check that no weird stuff is happening (there shouldn't be)
!colnames(len.bins3) %in% colnames(Others)
!colnames(Others) %in% colnames(len.bins3)

Absent <- Others %>%
  filter(!keys %in% len.bins3$keys)
# View(Absent)

#-------------------------------------------------------------------------------
# Check structure before join
#-------------------------------------------------------------------------------

str(len.bins3)
str(Absent)

```

# Complete table joins

From the structure it is necessary to change the len.bin3 equilibrium_time.pacifc to POSIXct.

```{r, join.tables}

len.bins3 <- len.bins3 %>%
  mutate(equilibrium_time.pacific = as.POSIXct(equilibrium_time.pacific, 
                                               tz = "America/Los_Angeles"))

PreAbs <- rbind(len.bins3, Absent) %>%
  mutate(date = as.Date(equilibrium_time.pacific))
# View(PreAbs)

unique(is.na(PreAbs))

fwrite(PreAbs, paste(here(dir), "/Data", survey.path,
                           "CPS.ATM.Survey.final.dat.csv", sep=""), sep = ";")

# plot locations
ggplot() +
  geom_sf(data = PreAbs, col = "black")

# Project data for intersection analyses
PreAbs.5070 <- PreAbs %>%
  st_transform(., crs = 5070)

# plot projected data locations
ggplot() +
  geom_sf(data = PreAbs.5070, col = "black")

```

# Find min and max geographical extents

Note this is only for graph labels and is chosen based on the mean lat and lon values of the haul transect locations.

```{r, Create.plot.label.breaks}

range(PreAbs$LAT)  
# [1] 28.65305 54.40770

lat_breaks <- seq(27, 55, 4)  # Latitude breaks

lat_labels <- c(paste(27, "\u00b0", "N", sep=""),
                paste(31, "\u00b0", "N", sep=""),
                paste(35, "\u00b0", "N", sep=""),
                paste(39, "\u00b0", "N", sep=""),
                paste(43, "\u00b0", "N", sep=""),
                paste(47, "\u00b0", "N", sep=""),
                paste(51, "\u00b0", "N", sep=""),
                paste(55, "\u00b0", "N", sep=""))

```

# Create base map

Note that cropping should be done before projecting the data and projecting the data should be done before any intersection analysis.

```{r, basemap}

#-------------------------------------------------------------------------------
# Make coastline backdrop
#-------------------------------------------------------------------------------

PacificCoast <- ne_countries(scale = 10, returnclass = "sf") 
PacificCoast2 <- st_crop(PacificCoast, xmin = -133, xmax = -110, ymin = 25, 
                         ymax = 55)
class(PacificCoast2)

#-------------------------------------------------------------------------------
# Plot output
#-------------------------------------------------------------------------------

ggplot() +
    geom_sf(data = PacificCoast2, fill = "#D2B48C", col = "#927143") +
    xlab("Longitude") + ylab("Latitude") +
    ggtitle("West Coast map") +
    theme_classic()

```

## Apply min and max geographical extents

```{r, adjust.map.dimensions}

#-------------------------------------------------------------------------------
# Convert to sf object
#-------------------------------------------------------------------------------

PacificCoast3.wgs84 <- PacificCoast2 %>%
  st_transform(., crs = 4326)

plot(PacificCoast3.wgs84)

#-------------------------------------------------------------------------------
# Find spatial extents
#-------------------------------------------------------------------------------

xmin <- st_bbox(PacificCoast3.wgs84)["xmin"]
xmin
# [1] -133

xmax <- st_bbox(PacificCoast3.wgs84)["xmax"]
xmax
# [1] -110

#-------------------------------------------------------------------------------
# Crop land mass
#-------------------------------------------------------------------------------

PacificCoast4.wgs84 <- PacificCoast3.wgs84 %>% 
  # Clip the shapefile to match the latitude range
  st_crop(PacificCoast3.wgs84, 
          xmin = -133, 
          xmax = -111,
          ymin = 27, 
          ymax = 55)
  
#-------------------------------------------------------------------------------
# Plot
#-------------------------------------------------------------------------------

PacificCoast4.wgs84.map <-
  ggplot() +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  theme_void() +
  scale_y_continuous(limits = c(27,55), 
                     breaks = lat_breaks, 
                     labels = lat_labels, 
                     expand = c(0.05, 0.05)) +
  labs(y = "Latitude")

PacificCoast4.wgs84.map

st_bbox(PacificCoast4.wgs84)
#     xmin ymin xmax ymax
# [1] -133   27 -111   55

```

## Creat hline boundaries

Because the current model looks at only the Northern sub population in US waters, one of the boundary extents should include the national boarders. The second should look at the coast of California where the most mixing is thought to occur for Pacific Sardine sub populations (under the current survey hypothesis). From the data we are looking at data from UTM zone 10 and UTM zone 11 use the USA Contiguous Albers Equal Area Conic project (ESRI:102003). The corresponding ESPG code for ESRI:102003 is 5070.

```{r, find.original.model.break.dimensions}

# First transform the data to be able to do intersection analysis
PacificCoast4.5070 <- st_transform(PacificCoast4.wgs84, crs = 5070)

#-------------------------------------------------------------------------------
# In order to find the state boundary extents, use ne_states function in 
# rnaturalearth
#-------------------------------------------------------------------------------

# Find state boundaries
PCoastStates.wgs84 <- ne_states(returnclass = "sf")

PCoastStates.5070 <-
  # transform to NAD83 / Conus Albers
  st_transform(PCoastStates.wgs84, crs = 5070) 

PCoastStates <- st_intersection(PCoastStates.5070, PacificCoast4.5070) %>%
  st_transform(., crs = 4326)

#-------------------------------------------------------------------------------
# For EEZ definitions 
#-------------------------------------------------------------------------------

# https://nauticalcharts.noaa.gov/data/us-maritime-limits-and-boundaries.html

eez.dir <- here(getwd(), "Data", "EEZ.Boundaries")
eez <- read_sf(paste(eez.dir, "/USMaritimeLimitsNBoundaries.shp", sep = ""))

#-------------------------------------------------------------------------------
# Putting it all together
#-------------------------------------------------------------------------------

hlines <- eez %>%
  filter(REGION == "Pacific Coast") %>%
  mutate(
    # Create southern US EEZ boundary
    ymin = st_bbox(.)["ymin"],
    # Create northern US EEZ boundary
    ymax = st_bbox(.)["ymax"],
    # Create northern stock boundary (Westernmost Point, CA)
    nstock = 40.4385,
    # Create southern stock boundary (Point Conception, CA)
    point.concept = 34.4486)

```

## Create city markers

To make the map easier to interpret, add the names and locations of most populated cites

```{r, add.map.details}

#-------------------------------------------------------------------------------
# populated areas (cities and city names)
#-------------------------------------------------------------------------------

Populated10.wgs84 <- ne_download(scale = 110, type = "populated_places_simple", 
                           category = "cultural", 
                           returnclass = "sf") 

Populated10.5070 <- Populated10.wgs84 %>%
  st_transform(., crs = 5070)

Populated10 <- st_intersection(Populated10.5070, PacificCoast4.5070) %>%
  st_transform(., crs = 4326)

#-------------------------------------------------------------------------------
# Plot definitions
#-------------------------------------------------------------------------------

Defs.with.cps.atm.survey.dat <- 
ggplot() +
  geom_hline(data = hlines, aes(yintercept = nstock), col = "blue", 
             linewidth = 0.7, linetype = "dashed") +
  geom_hline(data = hlines, aes(yintercept = point.concept), col = "red", 
             linewidth = 0.7, linetype = "longdash") +
  geom_hline(data = hlines, aes(yintercept = ymin), col = "black", 
             linewidth = 0.7) +
  geom_hline(data = hlines, aes(yintercept = ymax), col = "black", 
             linewidth = 0.7) +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = PCoastStates,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = Populated10, col = "black") +
  geom_text(data = Populated10,
            aes(x = longitude, y = latitude, label = name),
            col = "black", 
            position = position_dodge(width = 1),
            vjust = 1) +
  scale_y_continuous(limits = c(27,55), 
                     breaks = lat_breaks, 
                     labels = lat_labels, 
                     expand = c(0.05, 0.05)) +
  labs(y = "Latitude") +
  geom_sf(data = PreAbs, col = "black")

Defs.with.cps.atm.survey.dat

ggsave2(filename=paste("Defs.with.cps.atm.survey.dat.jpeg",sep=''),
        plot=Defs.with.cps.atm.survey.dat, device="jpeg", 
        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
        unit="cm", limitsize = FALSE)

```

# Cluster hauls by date and sun event

## First find the corresponding sunrise and suntime times

This should be done by data, lat, and lon.

```{r, sunrise.table}

# Use unique combinations from your data
suntimes <- PreAbs %>%
  transmute(
    lat = LAT,
    lon = LON,
    date = as.Date(equilibrium_time.pacific)
  ) %>%
  distinct() %>%
  st_drop_geometry()

# Plan parallel execution (adjust workers if needed)
plan(multisession)

# Apply get_sun_times function in parallel
sun_times <- future_pmap_dfr(suntimes, get_sun_times)

# Check that tz is correct
head(sun_times$sunrise_pacific)
head(sun_times$sunset_pacific)

# ------------------------------------------------------------------------------
# Save (optional) : Note that the tz value is not stored and needs specified 
# manually each time you load data with a POSIXct structure.
# ------------------------------------------------------------------------------

# Where to save output:
## sun_times.dir <- here(getwd(), "Data", "Sun_times")

# Save as CSV:
## fwrite(sun_times, file = paste(sun_times.dir, "/sunrise_sunset_times.csv",
##                               sep=""), sep = ",")

```

## Join set set table with data for clusters

Note that you the code above returns an IDate column for date and needs converted to standard date.

```{r, join.sunrise.set.and.dat}

# Check output
sun_times <- sun_times %>%
  mutate(LAT = lat,
         LON = lon,
         date = as.Date(date)) %>%
  select(-c(lat,lon))
# View(sun_times)

# Add sun_times to original data set
area.transects <- merge(PreAbs, sun_times, by = c("date", "LAT", "LON")) %>%
  st_transform(., crs = 5070)
# View(area.transects)  

# Double check values
unique(is.na(area.transects))

dim(PreAbs)
# [1] 11961    22

dim(area.transects)
# [1] 11961    24

# Filter hauls that are taken before 
# sunset time that are after sunrise time
area.transects.d <- area.transects %>%
  group_by(date) %>%
  filter(equilibrium_time.pacific >= sunrise_pacific) %>%
  mutate(cluster.grp = "day Trawl") %>%
  ungroup()
  
# Filter hauls that are taken before 
# sunrise time that are after sunset time
area.transects.n <- area.transects %>%
  group_by(date) %>%
  filter(equilibrium_time.pacific < sunrise_pacific |
           equilibrium_time.pacific >= sunset_pacific) %>%
  mutate(cluster.grp = "night Trawl") %>%
  ungroup()
  
# Join spatial objects and convert DateTime values to character 
# string (ESIRI does not work with DateTime)
area.transects.final <- rbind(area.transects.d, area.transects.n) %>%
  mutate(equilibrium_time.pacific = as.character(equilibrium_time.pacific),
         sunrise_pacific = as.character(sunrise_pacific),
         sunset_pacific = as.character(sunset_pacific))

dim(area.transects.final)
# [1] 11961    25

```

## Create 100 m Buffer distance per haul

```{r, 100m.buffer}

# Make a 100 m spatial buffer around each transect
area.transects.buff <- st_buffer(area.transects.final, 100)
# View(area.transects.buff)

# Where to save output
bufferTrans.dir <- here(getwd(), "Data", "Shapefiles")

# Save shapefile
write_sf(area.transects.buff, 
         file.path(paste(bufferTrans.dir, "/buffer.transects.shp", sep = "")))

```

## Sum clusters

### Join geometries

This gives us area and the square km of the clusters.

```{r, group.hauls.by.cluster.pt1}

# Load data
area.transects.buff <- read_sf(paste(bufferTrans.dir, "/buffer.transects.shp", 
                                     sep = ""))

colnames(area.transects.buff)
#  [1] "date"     "LAT"      "LON"      "cruise"   "ship"     "haul"     "collctn" 
#  [8] "age"      "len_bin"  "Ctch_wt"  "l1_CPUE"  "Year"     "Month"    "Season"  
# [15] "tim_stp"  "eqlbr__"  "stndrd_"  "NBRE_IN"  "Total_N"  "CtchWgL"  "keys"    
# [22] "snrs_pc"  "snst_pc"  "clstr_g"  "geometry"

#-------------------------------------------------------------------------------
# Sum over like values
#-------------------------------------------------------------------------------

areaTranBuffsum.prep <- area.transects.buff %>%
  # Select only general group id names plus haul identifiers
  select(keys,  date, tim_stp, clstr_g, geometry) %>%
  distinct() %>%
  group_by(tim_stp, clstr_g) %>%
  # How many hauls per unique key and night/day are there
  mutate(Hauls = n_distinct(keys)) %>%
  ungroup() %>%
  # Combine unique group identifier sequences per cluster
  group_by( date, tim_stp, clstr_g, Hauls) %>%
  # Spatially join and aggregate cluster geometries
  summarise(geometry = st_union(geometry), .groups = "keep") %>%
  # calculate the area of the cluster
  mutate(ClstArea = st_area(geometry)) %>%
  ungroup() %>%
  # Combine all unique identifier sequences per cluster
  group_by(tim_stp, clstr_g, Hauls) %>%
  # How many clusters per season and night/day are there
  mutate(NbClust = n()) %>%
  ungroup()

#-------------------------------------------------------------------------------  
# Note that the area is a special class and to be able to convert to km, you need 
# to remove the units attribute.
#-------------------------------------------------------------------------------  

attributes(areaTranBuffsum.prep$ClstArea) = NULL

# Convert to meaningful units
areaTranBuffsum.prep2 <- areaTranBuffsum.prep %>%
  mutate(Area_km2 = round(ClstArea/1e6, digits = 3)) %>% 
  mutate(Area_nmi2 = round(0.2915533496*Area_km2, digits = 3)) %>%
  select(-ClstArea)
# View(areaTranBuffsum.prep2)

head(areaTranBuffsum.prep2)

```

### Check max area of single haul obs.

Because a single haul transects vary between 0.03 km2 and 1.78 km2, check for accuracy. 

```{r, check.single.haul.area}

# Find minimum number of clusters
min(areaTranBuffsum.prep2$NbClust)
# [1] 2

# Filter on the returned data where only 2 haul transect was used in area
# computation, and that had the maximum area
test.accuracy <- areaTranBuffsum.prep2 %>%
  filter(Hauls == 2) %>%
  filter(Area_km2 == max(Area_km2)) 
View(test.accuracy)

# Create a logical vector where the filtered values are equal to the geometry
# of the original data
matches <- st_equals(area.transects.buff, test.accuracy, sparse = FALSE)

# Filter rows where there is at least one match (Note this will only return
# a single observation)
distance.test <- area.transects.buff[which(rowSums(matches) > 0), ]       

# Select the start and stop lat / lon from the unique keys         
distance.test2 <- distance.test %>%
  mutate(coords.str = str_extract(.$keys, "c\\([^\\)]+\\)"))
# 1) c\\( matches the literal "c(".
# 2) [^\\)]+ matches any characters except ")". So it grabs everything inside the 
# parentheses.
# 3) \\) matches the closing ")".

# Remove the "c()" part
distance.test2$coords.str <- str_remove_all(distance.test2$coords.str, "c\\(|\\)")

# Split by comma and convert to numeric
distance.test3 <- distance.test2 %>%
  st_drop_geometry() %>%
  select(coords.str)

# Create a data frame with two points  
vals <-  
  data.frame(
    lon_dists = c(as.numeric(strsplit(distance.test3$coords.str, ",\\s*")[[1]][1]),
                  as.numeric(strsplit(distance.test3$coords.str, ",\\s*")[[1]][2])),
    lat_dists = c(as.numeric(strsplit(distance.test3$coords.str, ",\\s*")[[1]][3]),
                  as.numeric(strsplit(distance.test3$coords.str, ",\\s*")[[1]][4])))

# Calculate the distance between the two points and manually calculate the distance
# and area using a 100 m buffer. Note that the values will be slightly different.
vals_sf <- vals %>%
  st_as_sf(., coords = c("lon_dists", "lat_dists"), crs = 4326) %>%
  st_transform(., crs = 5070) %>% 
  st_buffer(., 100) %>% 
  mutate(start_to_end_dist_m = as.numeric(st_distance(.[1,], .[2,]))) %>%
  mutate(backcalculatedArea = (start_to_end_dist_m * (2 * 100) + (pi * 100^2))/1e6)
View(vals_sf)

```

### Sum observations by cluster

This gives us a summary of the total catch without regard to length and individual observations. Note that the sum of CtchWgL (all weights represented by individuals of length *i*) should be equal to the sum of total catches in the cluster (Ctch_wt, *c*).

```{r, group.hauls.by.cluster.pt2}

area.transects.buff2 <- area.transects.buff %>%
  st_drop_geometry() %>%
  distinct() %>%
  # Combine all unique identifier sequences per cluster
  group_by( date, tim_stp, clstr_g, stndrd_, age, len_bin, LON, LAT) %>%
  mutate(Ctch_wt = round(sum(Ctch_wt),2),
         NBRE_IN = round(sum(NBRE_IN)),
         Total_N = round(sum(Total_N)),
         CtchWgL = round(sum(CtchWgL),2)) %>%
  ungroup() %>%
  select( date, tim_stp, clstr_g, len_bin, stndrd_, age, Total_N, NBRE_IN,
         Ctch_wt, NBRE_IN, Total_N, CtchWgL, LON, LAT, keys)
# View(area.transects.buff2)

```

### Join spatial and observational summaries

This allows us to maintain the geographic elements of the data.

```{r, group.hauls.by.cluster.pt3}

areaTranBuffsum.new <- inner_join(area.transects.buff2, areaTranBuffsum.prep2, 
                                 by = join_by( date, tim_stp, clstr_g)) %>%
  st_as_sf(., crs = 5070)
# View(areaTranBuffsum.new)

dim(areaTranBuffsum.new)
# [1] 11961  19

#-------------------------------------------------------------------------------  
# Graph output
#-------------------------------------------------------------------------------  

plotNormalHistogram(areaTranBuffsum.new$Area_km2)

plotNormalHistogram(areaTranBuffsum.new$Area_nmi2)

ggplot() +
  geom_sf(data = areaTranBuffsum.new, fill = "red", alpha = 0.5) +
  geom_sf(data = PreAbs.5070, col = "black")

#-------------------------------------------------------------------------------  
# Save output
#-------------------------------------------------------------------------------  

write_sf(areaTranBuffsum.new, 
         file.path(paste(bufferTrans.dir, "/CPS.ATM.Survey.Clusters.shp", sep = "")),
         overwrite = TRUE)

```

# Create summary table over full area extent

This has to be done in multiple steps.

## Simplify temporal aspects

```{r, summary.table.time.step}

areaTranBuff.polygons.summary <- areaTranBuffsum.new %>% 
  # separate out date elements (note the other way would be to rename a 
  # vectored list) 
  separate(date, into = c("Year", "Month", "Day")) %>%
  # Create a numeric index of Month Day combination to sort on
  mutate(MonthDay.num = as.numeric(paste(Month, Day, sep = ""))) %>%
  # Create abbreviated month value
  mutate(Month = month.abb[as.numeric(Month)],
         Year = as.numeric(Year)) %>%
  # Create Month day label
  mutate(MonthDay = paste(Month, ". ", Day, sep = "")) %>%
  distinct()

dim(areaTranBuff.polygons.summary) 
# [1] 11961  23
# View(areaTranBuff.polygons.summary)

#-------------------------------------------------------------------------------
# Summarize the the corresponding months and days sampled
# per season and night or day sampling
#-------------------------------------------------------------------------------

Hauls.N <- areaTranBuff.polygons.summary %>%
  # Group Year, Season and night or day trawls
  group_by(tim_stp, clstr_g) %>% 
  arrange(MonthDay.num, .by_group = TRUE) %>%
  mutate(MonthDay = paste(first(MonthDay), "-", last(MonthDay))) %>%
  ungroup() %>%
  # Remove unique identifiers
  select(-c(MonthDay.num, Day, Month)) %>%
  distinct()
# View(Hauls.N)

```

## Simplify spatial aspects

Summarize the corresponding latitudinal range per season and night or day sampling.

```{r, summary.table.sptl.dims}

Hauls.N2 <- Hauls.N %>%
  # Group Year, Season and night or day trawls
  group_by(tim_stp, clstr_g) %>% 
  mutate(NbClust = NbClust,
         Hauls = Hauls,
         Absent_hauls = sum(is.na(stndrd_)),
         Latitudes = paste(format(round(min(LAT), 2), nsmall = 2), # 2 digits after
                           " - ", 
                           format(round(max(LAT), 2), nsmall = 2), sep = ""),
         
         Area_range_km2 = paste(round(min(Area_km2), digits = 3), " - ", 
                                round(max(Area_km2), digits = 3), sep = ""),
         Area_range_nmi2 = paste(round(min(Area_nmi2), digits = 3), " - ", 
                                 round(max(Area_nmi2), digits = 3), sep = "")) %>%
  ungroup() %>%
  st_drop_geometry() %>%
  # Remove individual measure observations
  select(-c(LAT, LON, Area_km2, Area_nmi2, keys)) %>%
  distinct()

dim(Hauls.N2)  
# [1] 10124  18
# View(Hauls.N2)

```

## summarize observations

If there are presence observations, find length range. Note that if more than one time.step value exists, it means more than one observation is present!

### Sardine presence data

```{r, summary.table.sum.obs.pt1}

#-------------------------------------------------------------------------------
# If Sardine are present and only one observation:
#-------------------------------------------------------------------------------

Hauls.N.p1 <- Hauls.N2 %>% 
  group_by(tim_stp, clstr_g) %>%
  filter(n() == 1 & !is.na(stndrd_)) 

if (nrow(Hauls.N.p1) != 0){
  Hauls.N.p1 <- Hauls.N.p1 %>% 
    mutate(Lengths = paste(stndrd_, " cm", sep = ""),
           N = sum(Total_N), 
           n = sum(NBRE_IN),
           Ages = paste(min(age), " - ",
                        max(age), " yrs")) %>%
    ungroup() %>%
    distinct()
}else{
  rm(Hauls.N.p1)
}

#-------------------------------------------------------------------------------
# If Sardine are present and more than one observation:  
#-------------------------------------------------------------------------------

Hauls.N.p2 <- Hauls.N2 %>% 
  group_by(tim_stp, clstr_g) %>%
  filter(n() > 1 & !is.na(stndrd_)) %>%
      mutate(Lengths = paste(min(na.omit(stndrd_)), " - ",
                         max(na.omit(stndrd_)), 
                         " cm", sep = ""),
         Ages = if (n_distinct(age) > 1) {
           paste(min(na.omit(age)), " - ",
                 max(na.omit(age)), " yrs")
           }else{
             paste(age, " yrs")
             },
         N = sum(Total_N), 
         n = sum(NBRE_IN)) %>%
  ungroup() %>%
  distinct()

# Join presence data sets:  
Hauls.N.p <- 
  if (exists("Hauls.N.p1")){
    Hauls.N.p = rbind(Hauls.N.p1, Hauls.N.p2) %>% 
        # Remove individual measures
        select(-c(age, len_bin, Ctch_wt, stndrd_, CtchWgL, Total_N, NBRE_IN)) %>%
        distinct()
    }else{
      Hauls.N.p = Hauls.N.p2 %>%
      # Remove individual measures
      select(-c(age, len_bin, Ctch_wt, stndrd_, CtchWgL, Total_N, NBRE_IN)) %>%
      distinct()
      }
# View(Hauls.N.p)

```

### Sardine absence data

If there are no presence observations, write absent and put NA or 0.

```{r, summary.table.sum.obs.pt2}

Hauls.N.a <- Hauls.N2 %>%
  filter(is.na(stndrd_) & !tim_stp %in% Hauls.N.p$tim_stp) %>%
  mutate(Lengths = NA,
         Ages = NA,
         N = 0, 
         n = 0) %>%
  # Remove individual measures
  select(-c(age, len_bin, Ctch_wt, stndrd_, CtchWgL, Total_N, NBRE_IN)) %>%
  distinct()

```

### Join presence absence summary data

```{r, summary.table.sum.obs.pt3}

#-------------------------------------------------------------------------------  
# Merge presence absence data tables
#-------------------------------------------------------------------------------  

Hauls.N3 <- rbind(Hauls.N.p, Hauls.N.a) %>%
  distinct()

# Explore output  
head(Hauls.N3) 
sort(unique(Hauls.N3$Year))

#-------------------------------------------------------------------------------  
# Create an index
#-------------------------------------------------------------------------------  

Hauls.N3 <- Hauls.N3 %>%
  distinct()

EntryID <- seq_len(length(unique(Hauls.N3$Year)))
Year <- as.numeric(substr(unique(Hauls.N3$Year),1,4))
          
EntryID.df <- data.frame(EntryID, Year)
EntryID.df$Year <- EntryID.df$Year
          
#-------------------------------------------------------------------------------  
# Put everything together
#-------------------------------------------------------------------------------  

CPS.summary.table <- 
  inner_join(Hauls.N3, EntryID.df, relationship = "many-to-many",
             by = join_by(Year))
# View(CPS.summary.table)
          
# Group file output with entry id
dup.survey <- distinct(CPS.summary.table) 
# View(dup.survey)

```

# Select distinct values

This is our final summary table.

```{r, summary.table.sum.obs.pt4}

#-------------------------------------------------------------------------------  
# Simplify table
#-------------------------------------------------------------------------------  

dup.survey <- distinct(dup.survey[,c("EntryID", "Year", "Season", "MonthDay",
                                     "clstr_g", "Hauls", "NbClust", "Area_range_km2",
                                     "Area_range_nmi2","Absent_hauls",  "N", 
                                     "Lengths", "Ages", "n","Latitudes")])
  
names(dup.survey) <- c("ID", "Year", "Season", "MonthDay", "Sampling", "Hauls",
                       "Nb. Clusters", "Area Sampled (km2)", "Area Sampled (nmi2)",
                       "Nb. Absent Hauls", "N Est.", "Lengths", "Est. Ages", "n",
                       "Latitudes") 
# View(dup.survey)

dup.survey <- dup.survey %>%
  mutate(Season.code =
           case_when(
             Season %in% "Spring" ~ 1,
             Season %in% "Summer" ~ 2,
             Season %in% "Fall" ~ 3)) %>%
  mutate(sort.key = as.numeric(paste(ID, Year, Season.code, sep = ""))) %>%
  arrange(sort.key) %>%
  select(-c(sort.key, Season.code))
#View(dup.survey)

#-------------------------------------------------------------------------------  
# Save output
#-------------------------------------------------------------------------------  

fwrite(dup.survey, paste(dir,"/Results/summaryTable.csv", sep=""), sep=";")

```

# Create area definitions

Note that the min and max of the plot should have great spatial bound across all data sets and that this will likely change in the next iteration.

```{r, define.OM.box.extents}

#-------------------------------------------------------------------------------
# Find extent of CPS ATM Survey
#-------------------------------------------------------------------------------

# Find the center of the transects observed
st_transform(areaTranBuffsum.new, crs = 4326) %>% st_bbox()
#       xmin       ymin       xmax       ymax 
# -134.08076   28.65040 -114.79178   54.41664  

# Note the the observation extent should first be used to define the outer min 
# and max spatial boundaries. 

#-------------------------------------------------------------------------------
# Define data boundaries
#-------------------------------------------------------------------------------

xmin = -134.11630 # CUFES boundary
ymin =  28.6504   # CPS boundary
xmax = -114.0     # Artificial boundary
ymax = 54.74310   # CUFES boundary

#-------------------------------------------------------------------------------
# For Northern US EEZ to Inf
#-------------------------------------------------------------------------------

z1.bbox_coords <- c(xmin, unique(hlines$ymax), xmax, ymax)
names(z1.bbox_coords) = c("xmin","ymin","xmax","ymax")

z1bbp <- st_as_sfc(st_bbox(z1.bbox_coords)) 
st_crs(z1bbp) = 4326

st_bbox(z1bbp)

#-------------------------------------------------------------------------------
# For Northern stock to Northern US EEZ
#-------------------------------------------------------------------------------

z2.bbox_coords <- c(xmin, unique(hlines$nstock), xmax, unique(hlines$ymax))
names(z2.bbox_coords) = c("xmin","ymin","xmax","ymax")

z2bbp <- st_as_sfc(st_bbox(z2.bbox_coords)) 
st_crs(z2bbp) = 4326

st_bbox(z2bbp)

#-------------------------------------------------------------------------------
# For Southern stock to Northern stock
#-------------------------------------------------------------------------------

z3.bbox_coords <- c(xmin, unique(hlines$point.concept), xmax,
                    unique(hlines$nstock))
names(z3.bbox_coords) = c("xmin","ymin","xmax","ymax")

z3bbp <- st_as_sfc(st_bbox(z3.bbox_coords))
st_crs(z3bbp) = 4326

st_bbox(z3bbp)

#-------------------------------------------------------------------------------
# For Southern US EEZ to Southern stock
#-------------------------------------------------------------------------------

z4.bbox_coords <- c(xmin, unique(hlines$ymin), xmax,
                    unique(hlines$point.concept))
names(z4.bbox_coords) = c("xmin","ymin","xmax","ymax")

z4bbp <- st_as_sfc(st_bbox(z4.bbox_coords))
st_crs(z4bbp) = 4326

st_bbox(z4bbp)

#-------------------------------------------------------------------------------
# For Inf to Southern US EEZ
#-------------------------------------------------------------------------------

z5.bbox_coords <- c(xmin, ymin, xmax, unique(hlines$ymin))
names(z5.bbox_coords) = c("xmin","ymin","xmax","ymax")

z5bbp <- st_as_sfc(st_bbox(z5.bbox_coords))
st_crs(z5bbp) = 4326

st_bbox(z5bbp)

```

## Check OM spatial layout

This will give us an idea if the previous definitions are appropriate.

```{r, plot.OM.box.extents}

area.diams <-
  ggplot() +
  geom_sf(data = z1bbp, fill = "lightgrey", col = "black", alpha = 0.5) +
  geom_sf(data = z2bbp, fill = "blue", col = "black", alpha = 0.5) +
  geom_sf(data = z3bbp, fill = "lightgreen", col = "black", alpha = 0.5) +
  geom_sf(data = z4bbp, fill = "green", col = "black", alpha = 0.5) +
  geom_sf(data = z5bbp, fill = "steelblue", col = "black", alpha = 0.5) +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = PCoastStates,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = Populated10, col = "black") +
  geom_text(data = Populated10,
            aes(x = longitude, y = latitude, label = name),
            col = "black", 
            position = position_dodge(width = 1),
            vjust = 1) +
  scale_y_continuous(limits = c(27,55), 
                     breaks = lat_breaks, 
                     labels = lat_labels, 
                     expand = c(0.05, 0.05)) +
  labs(y = "Latitude")

area.diams

ggsave2(filename=paste("Area.Definitions.jpeg",sep=''), plot=area.diams,
        device="jpeg", path=paste(dir, "/Figures",sep=""),
        dpi=1200, width = 29, height=21, unit="cm", limitsize = FALSE)

```

# Tranform and merge area and transect data

Project the data and create and area,  and data index.

```{r, proj.and.common.scales}

# Create Multipolygon layer of OM box extents
bbox.frame <- st_sf(Area_id = 1:5, 
                    geometry = c(z1bbp, z2bbp, z3bbp, z4bbp, z5bbp)) %>%
  st_transform(.,crs = 5070)

```

# Spatially overlay data and area definitions

Note that ESIRI does not allow for DateTime and that this will need adjusted in the next step.

```{r, intersection.spatial.area.defs}

# 1. Perform polygon–polygon intersection
intersections <- st_intersection(
  areaTranBuffsum.new,  
  bbox.frame
)

# 2. Calculate area of overlap
intersections <- intersections %>%
  mutate(overlap_area = st_area(geometry))

# 3. Pick the Area_id with the greatest overlap for each original polygon
best_match <- intersections %>%
  group_by(date, clstr_g) %>%
  slice_max(overlap_area, with_ties = FALSE) %>%
  ungroup() %>%
  st_drop_geometry() %>%
  select(date, clstr_g, Area_id)

# 4. Join back to area.transects.buff and assign Area_id
areaTranBuffsum.polygons <- areaTranBuffsum.new %>%
  left_join(best_match, by = c("date", "clstr_g"))

sort(unique(areaTranBuffsum.polygons$Area_id))
# [1] 1 2 3 4 5
# View(areaTranBuffsum.new)

# Add back month and year data
areaTranBuffsum.polygons <- areaTranBuffsum.polygons %>%
  separate(date, into = c("Year", "Month", "Day"))

write_sf(areaTranBuffsum.polygons, 
       file.path(paste(bufferTrans.dir, "/cluster.with.area.defs.shp", sep="")))

```

# Plot clusters and areas

```{r, intersection.spatial.area.plot}

bbox.frame.wgs84 <- bbox.frame %>%
  mutate(Area_id = as.factor(Area_id)) %>%
  st_transform(., crs = 4326)

areaTranBuff.polygons.wgs84 <- areaTranBuffsum.polygons %>%
  mutate(Area_id = as.factor(Area_id)) %>%
  st_transform(., crs = 4326)

custom.pal <- c("1" = "lightgrey", "2" = "blue", "3" = "lightgreen", 
                "4" = "green", "5" = "steelblue")

# Plot to visually verify spatial overlap
Clusters.sdmTMB <-
  ggplot() +
  geom_hline(data = hlines, aes(yintercept = nstock), col = "blue", 
             linewidth = 0.7, linetype = "dashed") +
  geom_hline(data = hlines, aes(yintercept = point.concept), col = "red", 
             linewidth = 0.7, linetype = "longdash") +
  geom_hline(data = hlines, aes(yintercept = ymin), col = "black", 
             linewidth = 0.7) +
  geom_hline(data = hlines, aes(yintercept = ymax), col = "black", 
             linewidth = 0.7) +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = PCoastStates,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = Populated10, col = "black") +
  geom_text(data = Populated10,
            aes(x = longitude, y = latitude, label = name),
            col = "black", 
            position = position_dodge(width = 1),
            vjust = 1) +
  geom_sf(data = areaTranBuff.polygons.wgs84, fill = "black", col = "black") +
  labs(y = "Latitude")

# plot(st_geometry(bbox.frame), border = 'blue')
# plot(st_geometry(areaTranBuff.polygons), border = 'red', add = TRUE)

Clusters.sdmTMB 

ggsave2(filename=paste("CPS.ATM.Survey.Clusters.w.Area.Defs.jpeg",sep=''),
        plot=Clusters.sdmTMB, device="jpeg", 
        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
        unit="cm", limitsize = FALSE)

```
