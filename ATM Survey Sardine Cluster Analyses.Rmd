---
title: "ATM Survey Sardine Cluster Analyses"
author: "Stephanie Hopkins"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    
---

```{r setup, include=FALSE}

rm(list=ls())
gc()

load.lib <- c("tidyverse", "data.table", "cowplot", "sf", "rcompanion", "rnaturalearth", "rnaturalearthhires", "r4ss", "zoo", "suncalc", "here", "furrr")

install.lib <- load.lib[!load.lib %in% installed.packages()]

for(lib in install.lib) install.packages(lib,dependencies=TRUE)
sapply(load.lib,require,character=TRUE)

# Turn off s2 and use
# geos since you are computing
# distance and area
sf_use_s2(FALSE)

dir <- here(getwd())

survey.path <- "/Survey.Data/"

Figure.path <- "Figures/"

source("~/Final.OM.spatial.structure/functions.R")

```

# Read data

Remember to convert length to cm to be able to use VBGF!

```{r, read.data}

full.series <- 
  fread(paste(here(dir), 
              "/Data", survey.path,
              "2003.2024.ATM.Survey.with.integrated.specimen.data.csv", 
              sep = ""), sep = ",") %>%
  # convert mm to cm to be able to use VBGF
  mutate(standard_length = standard_length/10) %>%
  select(-FID) %>%
  distinct()

length(unique(full.series$collection))
# [1] 2673

colnames(full.series)

```

# Mean of the haul geometry

Note that we are taking the mean of the starting point and ending point to denote the centroid.

```{r, mean.haul.geom}

#-------------------------------------------------------------------------------
# For Shooting location
#-------------------------------------------------------------------------------

range(full.series$start_latitude)
# [1] 28.6513 54.3997
Lat1 <- full.series$start_latitude

mean(full.series$start_latitude)
# [1] 39.06714

range(full.series$start_longitude)
# [1] -134.0793 -114.7928
Lon1 <- full.series$start_longitude

mean(full.series$start_longitude)
# [1] -122.4603

#-------------------------------------------------------------------------------
# For hauling location
#-------------------------------------------------------------------------------

range(full.series$stop_latitude)
# [1] 28.6548 54.4157
Lat2 <- full.series$stop_latitude

range(full.series$stop_longitude)
# [1] -134.0325 -114.8483
Lon2 <- full.series$stop_longitude

#-------------------------------------------------------------------------------
# Find the mean of Shooting and Hauling location
#-------------------------------------------------------------------------------

LAT <- data.table(Lat1, Lat2)
LON <- data.table(Lon1, Lon2)

LAT <- rowMeans(LAT)
LON <- rowMeans(LON)
 
plot(LON, LAT)

# Since the data falls within two UTM zones and there is not an ideal projection to use, 
# apply a custom equal area albers projection
aea_custom <- "+proj=aea +lat_1=30 +lat_2=50 +lat_0=40 +lon_0=-124.5 +datum=WGS84 +units=m +no_defs"

```

# Verify georefrence values

This will tell us whether or not to clip the data when we make plots.

```{r lat.lon.extent}

range(LAT)
# [1] 28.65305 54.40770

range(LON)
# [1] -134.0559 -114.8205

```

# Spatialize the transect data 

For each haul transect, group together the starting and stopping latitudinal longitudinal coordinates and create a line string spatial object.

```{r, spatial.transects.defs}

# Create a linestring for each transect
trip_rect <- make_transect_sf(full.series)

summary(trip_rect)
  
```

# Determine fishing duration

Note that for initial plots, CPUE was defined as kg/hr, but since we are estimating biomass abundance, we area is needed instead. For each haul transect, the same CPUE values are present for both computational methods.

```{r, convert.POSIXct.object}

full.series.sf <- trip_rect %>%
  group_by(cruise, ship, haul, equilibrium_time.utc, collection, geometry) %>%
  mutate(haulback_time.pacific = 
           strptime(haulback_time.utc, "%Y-%m-%d %H:%M:%S", 
                    tz = "America/Los_Angeles"),
         equilibrium_time.pacific = 
           strptime(equilibrium_time.utc, "%Y-%m-%d %H:%M:%S", 
                    tz = "America/Los_Angeles")) %>%
  ungroup() 

head(unique(full.series.sf$haulback_time.pacific))
head(unique(full.series.sf$equilibrium_time.pacific))
  
```

There are many different values used to assess length at age in the region and there are high levels of variability between individuals, years, seasons, and cohorts. For example K was historically nearly double that of the most recent publication (Wildermuth et al., 2024), where previous values were L(inf) = 31 cm, and K = 0.460 (Female) off the coast of California >> Phillips, J.B., 1948. Growth of the sardine *Sardinops caerulea* 1941-42 through 1945-47. Calif. Dept. Fish and Game. Calif. Fish. Bull. 71:33 p. Below, we investigate the potential of using the Wildermuth et al., 2024 values for L(inf), K, and t0 in the extended data set with all lengths considered.

```{r, extract.ss.growth.params}

# Where ss output files are
model_dir <- here(getwd(), "Data", "SSMSE.Base.Model.Data", "start2001",
                  "constGrowthMidSteepNewSelex_OM")

# Read ssouput file
OM_age.length <- SSgetoutput(dir = file.path(model_dir))

# Check structure
str(OM_age.length)

# How many age classes were fed into the model (model forecasts the rest)
OM_age.length$replist1$agebins
# [1] 0 1 2 3 4 5 6 7 8

#-------------------------------------------------------------------------------
# Below are the values needed for VBGF
#-------------------------------------------------------------------------------

# Estimated age
Linf <- OM_age.length$replist1$Growth_Parameters$Linf
Linf
# [1] 25.153

K <- OM_age.length$replist1$Growth_Parameters$K
K
# [1] 0.273111

# A1 is the starting length (true age 0 values)
A1 <- OM_age.length$replist1$Growth_Parameters$A1
A1 
# [1] 0.5

# A2 is either fixed at a theoretical maximum or inf (999)
A2 <- OM_age.length$replist1$Growth_Parameters$A2
A2 
# [1] 999

# Smallest observed length in the data set
L_a_A1 <- OM_age.length$replist1$Growth_Parameters$L_a_A1
L_a_A1
# [1] 13.1744

# You can also index them all at once
OM_age.length$replist1$Growth_Parameters

#-------------------------------------------------------------------------------
# To find t0, which is estimated in the ss model rather than explicit
#-------------------------------------------------------------------------------

# Equation used
t0 <- A1 + (log(1-L_a_A1/Linf))/K
t0
# [1] -2.216315

# Undefined parameter in the model handbook thought to be t0
A_a_L0 <- OM_age.length$replist1$Growth_Parameters$A_a_L0
A_a_L0
# [1] -2.2163

# Things to look up later for types of errors (!= fleets, != # of params)
OM_age.length$replist1$age_error_mean
OM_age.length$replist1$age_error_sd

# Returns the Maximum Likelihood Estimates (MLE)
OM_age.length$replist1$growthseries[1,5:15]

# Create a vector of mle values
mle.vals <- c(t(OM_age.length$replist1$growthseries[1,5:15]))

# Create a vector of ages
age.vals <- seq(0,10,1)

# Combine in a data frame ages and mle values
ss.age.len <- data.frame(mle.vals, age.vals)

# To view parameterizations and associated sd values
OM_age.length$replist1$parameters 

```

# Estimate age

Use the values extracted from the ss model for pacific sardine

```{r, apply.inverse.vbgg}

# Apply VBGF
Sardine <- full.series.sf %>% 
  filter(scientific_name == "Sardinops sagax") %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry) %>%
  mutate(age = round((-1/K)*log(1-(standard_length/Linf)) + A_a_L0)) %>% 
  ungroup() %>%
  group_by(age) %>%
  mutate(mean.length.age = mean(standard_length)) %>%
  ungroup() %>%  
  distinct()

# Explore NaN values
sardine.nans.pt1 <- Sardine %>%
  filter(is.na(age))

# How many observations are lost
dim(sardine.nans.pt1)
# [1] 1118  41

# What lengths are not represented
sardine.nans.pt2 <- sardine.nans.pt1 %>%
  distinct(standard_length)

# Vector of lengths not represented
c(t(sardine.nans.pt2))
#  [1] 25.2 25.8 27.1 25.3 26.7 25.5 25.6 26.2 25.4 25.7 26.0 25.9 26.8 28.9 26.1
# [16] 26.4 26.3 28.4 27.2 27.3 27.4 28.1 26.5 26.9 27.8 26.6 27.5 27.0 27.6 28.0
# [31] 29.2 27.9 28.2 27.7 29.1

# Plot age length per haul, average age per haul, and ss mle values 
# (when excluding larger lengths from the data)...not a sensitivity analysis is 
# still needed
age.length.plot <-
  ggplot() + 
  geom_point(data = Sardine, aes(x=age, y=standard_length), col = "black") +
  geom_line(data = Sardine, aes(x=age, y=mean.length.age), col = "red") +
  geom_line(data=ss.age.len, aes(x=age.vals, y=mle.vals), col = "blue") + 
  scale_y_continuous(breaks = seq(0, 31, by = 2)) +
  scale_x_continuous(breaks = seq(0, 20, by = 1)) +
  ylab("Standard Length (cm)") +
  xlab("Estimate Age (yr)") +
  theme_classic()

age.length.plot

ggsave2(filename=paste("age.length.keys.jpeg",sep=''),
        plot=age.length.plot, device="jpeg", 
        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
        unit="cm", limitsize = FALSE)

```

# Define length bin definitions for Sardine only

Because aging is uncertain, propose length bins where the first 100mm are grouped (considered age 0), and where every 2.25 cm is added.

```{r, length.bin.defs}

len.bins <- Sardine

head(len.bins)

range(len.bins$standard_length)
# [1]  2.5 29.2

#-------------------------------------------------------------------------------
# Check lowest intervals
#-------------------------------------------------------------------------------

len100 <- len.bins %>%
  filter(standard_length <= 10) %>%
  mutate(len.bin = "100")

sort(unique(len100$age))
# [1] -2 -1 0

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len125 <- len.bins %>%
  filter(standard_length > 10 & standard_length < 12.6) %>%
  mutate(len.bin = "125")

sort(unique(len125$age))
# [1] 0

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len150 <- len.bins %>%
  filter(standard_length > 12.5 & standard_length < 15.1) %>%
  mutate(len.bin = "150")

sort(unique(len150$age))
# [1] 0 1

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len175 <- len.bins %>%
  filter(standard_length > 15 & standard_length < 17.6) %>%
  mutate(len.bin = "175")

sort(unique(len175$age))
# [1] 1 2

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len200 <- len.bins %>%
  filter(standard_length > 17.5 & standard_length < 20.1) %>%
  mutate(len.bin = "200")

sort(unique(len200$age))
# [1] 2 3 4

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len225 <- len.bins %>%
  filter(standard_length > 20 & standard_length < 22.6) %>%
  mutate(len.bin = "225")

sort(unique(len225$age))
# [1] 4 5 6

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len250 <- len.bins %>%
  filter(standard_length > 22.5 & standard_length < 25.1) %>%
  mutate(len.bin = "250")

sort(unique(len250$age))
# [1] 6 7 8 9 10 11 12 13 15 16

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm and group values
#-------------------------------------------------------------------------------

len250plus <- len.bins %>%
  filter(standard_length > 25.5) %>%
  mutate(len.bin = "250+")

#-------------------------------------------------------------------------------
# Re-code negative age to 0
#-------------------------------------------------------------------------------

len100.negs <- len100 %>%
  filter(age < 0) %>%
  mutate(age = 0)

len100.0s <- len100 %>%
  filter(age == 0)

len100.new <- rbind(len100.negs,  len100.0s)

#-------------------------------------------------------------------------------
# Join data sets 
#-------------------------------------------------------------------------------

len.bins2 <- rbind(len100.new, len125, len150, len175, len200, len225, len250, len250plus)

#-------------------------------------------------------------------------------
# Re-code Sardines NaNs and to 13+ ages to 10+
#-------------------------------------------------------------------------------

len.bins2.ok <- len.bins2 %>%
  filter(!is.na(age) & age < 10)

len.bins2.no <- len.bins2 %>%
  filter(age > 9) %>%
  mutate(age = 10)

len250plus <- len250plus %>%
  filter(is.na(age)) %>%
  mutate(age = 10)

len.bins2 <- rbind(len.bins2.ok, len.bins2.no, len250plus)

```

# Calculate the total catch weight for each length

Note that here we are standardizing and correcting the data to remove noise and better match the outputs in the commercial data sets.

```{r, calculate.median.weight.per.lenbin}

len.bins3 <- len.bins2 %>%
  # Compute theoretical weight per individual
  mutate(TheoreticalWgt_i = 7.52e-06 * standard_length^3.23,
         ) %>%
  
  # Step 1: Aggregate per bin, haul, age
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry, 
           age, len.bin) %>%
  mutate(
    WGHT_bin = sum(as.numeric(weight)),
    NBRE_bin = n(),
    TheoreticalWgt_median = median(TheoreticalWgt_i),
    # haul-level total count (assumed constant within haul)
    NBRE_samp = first(subsample_count),
    # haul-level total weight (assumed constant within haul)
    Total_W = first(Species_weight)
  ) %>%
  ungroup() %>%
  
  # Step 2: Calculate proportion per bin
  mutate(
    PropLen_bin = NBRE_bin / NBRE_samp
  ) %>%
  
  # Step 3: Calculate EstimatedWeight and Correction.factor per haul group
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry) %>%
  mutate(
    EstimatedWeight = sum(PropLen_bin * TheoreticalWgt_median) * NBRE_samp,
    Correction.factor = Total_W / EstimatedWeight
  ) %>%
  ungroup() %>%
  
  # Step 4: Calculate corrected catch weight per bin
  mutate(
    CatchWeightLen_bin = NBRE_bin * TheoreticalWgt_median * Correction.factor
  )

summary(len.bins3)

```

## Check outputs

```{r, check.length.distributions}

dim(len.bins3)
# [1] 17163  49

dim(distinct(len.bins3))
# [1] 17163  49

len.bins3$geometry

```

# Expand spatial temporal elements for hauls with Sardine

```{r, sardine.presence}

Sardine.present <- len.bins3 %>%
  group_by(cruise, ship, haul, collection, equilibrium_time.pacific, geometry) %>%
  mutate(Year = as.numeric(substr(equilibrium_time.pacific,1,4)),
         Month = as.numeric(substr(equilibrium_time.pacific,6,7))) %>%
  mutate(time.step = paste(Year," - ", Month, sep=""),
         keys = paste(cruise, ship, haul, collection, equilibrium_time.pacific,
                      geometry, sep = "-")) %>%
  ungroup() %>%
  select(-c(longitude, latitude, quality.indic, sex, is_random_sample, fork_length, 
            total_length, mantle_length, presence_only, ship_spd_through_water, 
            surface_temp_method, surface_temp, TheoreticalWgt_i, TheoreticalWgt_median,
            PropLen_bin, EstimatedWeight, Correction.factor, mean.length.age)) %>%
  distinct()
# View(Sardine.present)

```

# Expand spatial temporal elements for hauls with Sardine

```{r, Absent.bin}

Others <- full.series.sf %>% 
  filter(scientific_name != "Sardinops sagax") %>%
  group_by(cruise, ship, haul, collection, equilibrium_time.pacific, geometry) %>%
  mutate(Year = as.numeric(substr(equilibrium_time.pacific,1,4)),
         Month = as.numeric(substr(equilibrium_time.pacific,6,7))) %>%
  mutate(time.step = paste(Year," - ", Month, sep=""),
         keys = paste(cruise, ship, haul, collection, equilibrium_time.pacific,
                      geometry, sep = "-")) %>%
  ungroup() %>%
  select(-c(longitude, latitude, quality.indic, sex, is_random_sample, fork_length, 
            total_length, mantle_length, presence_only, ship_spd_through_water, 
            surface_temp_method, surface_temp)) %>%
  distinct() %>%
  mutate(remaining_weight = 0, 
         len.bin = NA, 
         age = NA, 
         NBRE_bin = 0, 
         NBRE_samp = 0, 
         CatchWeightLen_bin = 0,
         ) %>%
  distinct()

```

# Sum tables and simplify

Check for unique keys where no sardines were caught. These will be treated as true absent values.

```{r, view.absent.present.data.structure}

# Check that no weird stuff is happening (there shouldn't be)
!colnames(Sardine.present) %in% colnames(Others)
names(Sardine.present)[27]
names(Sardine.present)[30]

# Add unmatched columns to Others
Others <- Others %>%
  mutate(WGHT_bin = 0,
         Total_W = 0)

# Double check column names from other direction
!colnames(Others) %in% colnames(Sardine.present)

# Filter out hauls that had Sardine catch. What is left
# will be considered "true" absence.
Absent <- Others %>%
  filter(!keys %in% Sardine.present$keys)
# View(Absent)

#-------------------------------------------------------------------------------
# Check structure before join
#-------------------------------------------------------------------------------

str(len.bins3)
str(Absent)

# Check time zones
head(Sardine.present$equilibrium_time.pacific)
head(Absent$equilibrium_time.pacific)

```

# Complete table joins

From the structure it is necessary to change the len.bin3 equilibrium_time.pacifc to POSIXct.

```{r, join.tables}

# Formats match, join the two data frames.
PreAbs <- rbind(Sardine.present, Absent) %>%
  # Create a data object
  mutate(date = as.Date(equilibrium_time.pacific))
# View(PreAbs)

unique(is.na(PreAbs))

fwrite(PreAbs, paste(here(dir), "/Data", survey.path,
                           "CPS.ATM.Survey.final.dat.csv", sep=""), sep = ";")

# plot locations
ggplot() +
  geom_sf(data = PreAbs, col = "black")

# Project data for intersection analyses
PreAbs.aea_custom <- PreAbs %>%
  st_transform(., crs = aea_custom)

# plot projected data locations
ggplot() +
  geom_sf(data = PreAbs.aea_custom, col = "black")

```

# Find min and max geographical extents

Note this is only for graph labels and is chosen based on the mean lat and lon values of the haul transect locations.

```{r, Create.plot.label.breaks}

st_bbox(PreAbs) 
#      xmin      ymin      xmax      ymax 
# -134.0793   28.6513 -114.7928   54.4157 

lat_breaks <- seq(27, 55, 4)  # Latitude breaks

lat_labels <- c(paste(27, "\u00b0", "N", sep=""),
                paste(31, "\u00b0", "N", sep=""),
                paste(35, "\u00b0", "N", sep=""),
                paste(39, "\u00b0", "N", sep=""),
                paste(43, "\u00b0", "N", sep=""),
                paste(47, "\u00b0", "N", sep=""),
                paste(51, "\u00b0", "N", sep=""),
                paste(55, "\u00b0", "N", sep=""))

```

# Create base map

Note that cropping should be done before projecting the data and projecting the data should be done before any intersection analysis.

```{r, basemap}

#-------------------------------------------------------------------------------
# Make coastline backdrop
#-------------------------------------------------------------------------------

PacificCoast <- ne_countries(scale = 10, returnclass = "sf") 
PacificCoast2 <- st_crop(PacificCoast, xmin = -133, xmax = -110, ymin = 25, 
                         ymax = 55)
class(PacificCoast2)

#-------------------------------------------------------------------------------
# Plot output
#-------------------------------------------------------------------------------

ggplot() +
    geom_sf(data = PacificCoast2, fill = "#D2B48C", col = "#927143") +
    xlab("Longitude") + ylab("Latitude") +
    ggtitle("West Coast map") +
    theme_classic()

```

## Apply min and max geographical extents

```{r, adjust.map.dimensions}

#-------------------------------------------------------------------------------
# Convert to sf object
#-------------------------------------------------------------------------------

PacificCoast3.wgs84 <- PacificCoast2 %>%
  st_transform(., crs = 4326)

plot(PacificCoast3.wgs84)

#-------------------------------------------------------------------------------
# Find spatial extents
#-------------------------------------------------------------------------------

xmin <- st_bbox(PacificCoast3.wgs84)["xmin"]
xmin
# [1] -133

xmax <- st_bbox(PacificCoast3.wgs84)["xmax"]
xmax
# [1] -110

#-------------------------------------------------------------------------------
# Crop land mass
#-------------------------------------------------------------------------------

PacificCoast4.wgs84 <- PacificCoast3.wgs84 %>% 
  # Clip the shapefile to match the latitude range
  st_crop(PacificCoast3.wgs84, 
          xmin = -133, 
          xmax = -111,
          ymin = 27, 
          ymax = 55)
  
#-------------------------------------------------------------------------------
# Plot
#-------------------------------------------------------------------------------

PacificCoast4.wgs84.map <-
  ggplot() +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  theme_void() +
  scale_y_continuous(limits = c(27,55), 
                     breaks = lat_breaks, 
                     labels = lat_labels, 
                     expand = c(0.05, 0.05)) +
  labs(y = "Latitude")

PacificCoast4.wgs84.map

st_bbox(PacificCoast4.wgs84)
#     xmin ymin xmax ymax
# [1] -133   27 -111   55

```

## Creat hline boundaries

Because the current model looks at only the Northern sub population in US waters, one of the boundary extents should include the national boarders. The second should look at the coast of California where the most mixing is thought to occur for Pacific Sardine sub populations (under the current survey hypothesis). From the data we are looking at data from UTM zone 10 and UTM zone 11 use the USA Contiguous Albers Equal Area Conic project (ESRI:102003). The corresponding ESPG code for ESRI:102003 is aea_custom.

```{r, find.original.model.break.dimensions}

# First transform the data to be able to do intersection analysis
PacificCoast4.aea_custom <- st_transform(PacificCoast4.wgs84, crs = aea_custom)

#-------------------------------------------------------------------------------
# In order to find the state boundary extents, use ne_states function in 
# rnaturalearth
#-------------------------------------------------------------------------------

# Find state boundaries
PCoastStates.wgs84 <- ne_states(returnclass = "sf")

PCoastStates.aea_custom <-
  # transform to NAD83 / Conus Albers
  st_transform(PCoastStates.wgs84, crs = aea_custom) 

PCoastStates <- st_intersection(PCoastStates.aea_custom, PacificCoast4.aea_custom) %>%
  st_transform(., crs = 4326)

#-------------------------------------------------------------------------------
# For EEZ definitions 
#-------------------------------------------------------------------------------

# https://nauticalcharts.noaa.gov/data/us-maritime-limits-and-boundaries.html

eez.dir <- here(getwd(), "Data", "EEZ.Boundaries")
eez <- read_sf(paste(eez.dir, "/USMaritimeLimitsNBoundaries.shp", sep = ""))

#-------------------------------------------------------------------------------
# Putting it all together
#-------------------------------------------------------------------------------

hlines <- eez %>%
  filter(REGION == "Pacific Coast") %>%
  mutate(
    # Create southern US EEZ boundary
    ymin = st_bbox(.)["ymin"],
    # Create northern US EEZ boundary
    ymax = st_bbox(.)["ymax"],
    # Create northern stock boundary (Westernmost Point, CA)
    nstock = 40.4385,
    # Create southern stock boundary (Point Conception, CA)
    point.concept = 34.4486)

```

## Create city markers

To make the map easier to interpret, add the names and locations of most populated cites

```{r, add.map.details}

#-------------------------------------------------------------------------------
# populated areas (cities and city names)
#-------------------------------------------------------------------------------

Populated10.wgs84 <- ne_download(scale = 110, type = "populated_places_simple", 
                           category = "cultural", 
                           returnclass = "sf") 

Populated10.aea_custom <- Populated10.wgs84 %>%
  st_transform(., crs = aea_custom)

Populated10 <- st_intersection(Populated10.aea_custom, PacificCoast4.aea_custom) %>%
  st_transform(., crs = 4326)

#-------------------------------------------------------------------------------
# Plot definitions
#-------------------------------------------------------------------------------

Defs.with.cps.atm.survey.dat <- 
ggplot() +
  geom_hline(data = hlines, aes(yintercept = nstock), col = "blue", 
             linewidth = 0.7, linetype = "dashed") +
  geom_hline(data = hlines, aes(yintercept = point.concept), col = "red", 
             linewidth = 0.7, linetype = "longdash") +
  geom_hline(data = hlines, aes(yintercept = ymin), col = "black", 
             linewidth = 0.7) +
  geom_hline(data = hlines, aes(yintercept = ymax), col = "black", 
             linewidth = 0.7) +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = PCoastStates,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = Populated10, col = "black") +
  geom_text(data = Populated10,
            aes(x = longitude, y = latitude, label = name),
            col = "black", 
            position = position_dodge(width = 1),
            vjust = 1) +
  scale_y_continuous(limits = c(27,55), 
                     breaks = lat_breaks, 
                     labels = lat_labels, 
                     expand = c(0.05, 0.05)) +
  labs(y = "Latitude") +
  geom_sf(data = PreAbs, col = "black")

Defs.with.cps.atm.survey.dat

ggsave2(filename=paste("Defs.with.cps.atm.survey.dat.jpeg",sep=''),
        plot=Defs.with.cps.atm.survey.dat, device="jpeg", 
        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
        unit="cm", limitsize = FALSE)

```

# Cluster hauls by date and sun event

## First find the corresponding sunrise and suntime times

This should be done by data, lat, and lon.

```{r, sunrise.table}

# Find the center of the polygon layers to use for joins
PreAbs2 <- PreAbs %>%
  mutate(
    lat = st_coordinates(st_centroid(.))[,2],
    lon = st_coordinates(st_centroid(.))[,1]
  )

# Use unique combinations from your data
suntimes <- PreAbs2 %>%
  transmute(
    lat = lat,
    lon = lon,
    date = date
  ) %>%
  distinct() %>%
  st_drop_geometry()

dim(suntimes)
# [1] 2672  3

# Plan parallel execution (adjust workers if needed)
plan(multisession)

# Apply get_sun_times function in parallel.
## Note that get_sun_times must in wgs84 
sun_times <- future_pmap_dfr(suntimes, get_sun_times)

# Check that tz is correct
head(sun_times$sunrise_pacific)
head(sun_times$sunset_pacific)

# ------------------------------------------------------------------------------
# Save (optional) : Note that the tz value is not stored and needs specified 
# manually each time you load data with a POSIXct structure.
# ------------------------------------------------------------------------------

# Where to save output:
## sun_times.dir <- here(getwd(), "Data", "Sun_times")

# Save as CSV:
## fwrite(sun_times, file = paste(sun_times.dir, "/sunrise_sunset_times.csv",
##                               sep=""), sep = ",")

```

## Join set set table with data for clusters

Note that you the code above returns an IDate column for date and needs converted to standard date.

```{r, join.sunrise.set.and.dat}

# Check reference system
st_crs(PreAbs2)

dim(PreAbs2)
# [1] 25488  38

# Check output
head(sun_times$sunrise_pacific)
head(sun_times$sunset_pacific)

dim(sun_times)
# [1] 2672    5

# Add sun_times to original data set
area.transects <- inner_join(PreAbs2, sun_times, 
                             join_by("date", "lat", "lon")) %>%
  st_transform(., crs = aea_custom)
# View(area.transects)  

dim(area.transects)
# [1] 25488  40

# Double check values
unique(is.na(area.transects))

```

## Cluster of date and whether the haul was day or night

```{r, join.sunrise.set.and.dat}

# Filter hauls that are taken before 
# sunset time that are after sunrise time
area.transects.d <- area.transects %>%
  group_by(date) %>%
  filter(equilibrium_time.pacific >= sunrise_pacific) %>%
  mutate(cluster.grp = "day Trawl") %>%
  ungroup()
  
# Filter hauls that are taken before 
# sunrise time that are after sunset time
area.transects.n <- area.transects %>%
  group_by(date) %>%
  filter(equilibrium_time.pacific < sunrise_pacific |
           equilibrium_time.pacific >= sunset_pacific) %>%
  mutate(cluster.grp = "night Trawl") %>%
  ungroup()
  
# Join spatial objects and convert DateTime values to character 
# string (ESIRI does not work with DateTime)
area.transects.final <- rbind(area.transects.d, area.transects.n) %>%
  mutate(equilibrium_time.pacific = as.character(equilibrium_time.pacific),
         sunrise_pacific = as.character(sunrise_pacific),
         sunset_pacific = as.character(sunset_pacific))

dim(area.transects.final)
# [1] 25488  41

```

## Create 100 m Buffer distance per haul

```{r, 100m.buffer}

#-------------------------------------------------------------------------------
# Make a 100 m spatial buffer around each transect
#-------------------------------------------------------------------------------

area.transects.buff <- st_buffer(area.transects.final, units::as_units(100,"m"),
                                 max_cells=1e5)
# View(area.transects.buff)

#-------------------------------------------------------------------------------
# Adjust names for clarity
#-------------------------------------------------------------------------------

colnames(area.transects.buff)

# Remove unnecessary columns
area.transects.buff <- area.transects.buff %>%
  select(-c(haulback_time.utc, haulback_time.pacific, equilibrium_time.utc,
            beginTransect, endTransect, subsample_count, subsample_weight,
            itis_tsn, remaining_weight, Species_weight))

# Shorten the names
names(area.transects.buff) <- c("cruise", "ship", "haul", "collection",
                                "init_lat", "init_lon", "end_lat", "end_lon",
                                "sciname", "weight", "specimen_i,", "length_cm",
                                "geometry", "datetimePC", "age", "len_bin",
                                "WGHT_bin", "NBRE_bin", "NBRE_samp", "Total_W",
                                "WghtLenBin","Year","Month", "tim_stp", "keys",
                                "date", "latCntr", "lonCntr", "sunrisePC",
                                "sunsetPC", "clstGRP")

write_sf(area.transects.buff, 
         file.path(paste(dir, "/Data/Shapefiles/area.transects.pre-step.shp", 
                         sep = "")),
         overwrite = TRUE)

```

## Sum clusters

### Join geometries

This gives us area and the square km of the clusters.

```{r, group.hauls.by.cluster.pt1}

test.area.transects.prestep <- 
  read_sf(file.path(paste(dir, "/Data/Shapefiles/area.transects.pre-step.shp", 
                          sep = "")
                    ))

colnames(test.area.transects.prestep)
#  [1] "cruise"   "ship"     "haul"     "collctn"  "init_lt"  "init_ln" 
#  [7] "end_lat"  "end_lon"  "sciname"  "weight"   "spcmn_,"  "lngth_c" 
# [13] "dattmPC"  "age"      "len_bin"  "WGHT_bn"  "NBRE_bn"  "NBRE_sm" 
# [19] "Total_W"  "WghtLnB"  "Year"     "Month"    "tim_stp"  "keys"    
# [25] "date"     "latCntr"  "lonCntr"  "sunrsPC"  "sunstPC"  "clstGRP" 
# [31] "geometry"

#-------------------------------------------------------------------------------
# Sum over like values
#-------------------------------------------------------------------------------

areaTranBuffsum.prep <- test.area.transects.prestep %>%
  # Select only general group id names plus haul identifiers
  select(keys,  date, tim_stp, clstGRP, geometry) %>%
  distinct() %>%
  group_by(tim_stp, clstGRP) %>%
  # How many hauls per unique year and month by night and day
  mutate(Hauls = n_distinct(keys)) %>%
  ungroup() %>%
  # Combine unique group identifier sequences per cluster
  group_by(tim_stp, date, clstGRP, Hauls) %>%
  # Spatially join and aggregate cluster geometries
  summarise(geometry = st_union(geometry), .groups = "keep") %>%
  # calculate the area of the cluster
  mutate(ClstArea = st_area(geometry),
         ) %>%
  ungroup() %>%
  # Combine all unique identifier sequences per cluster
  group_by(tim_stp, clstGRP, Hauls) %>%
  # How many clusters per season and night/day are there
  mutate(NbClust = n()) %>%
  ungroup()
# View(areaTranBuffsum.prep)

#-------------------------------------------------------------------------------  
# Note that the area is a special class and to be able to convert to km, you need 
# to remove the units attribute.
#-------------------------------------------------------------------------------  

attributes(areaTranBuffsum.prep$ClstArea) = NULL

# Convert to meaningful units
areaTranBuffsum.prep2 <- areaTranBuffsum.prep %>%
  mutate(Area_km2 = round(ClstArea/1e6, digits = 3)) %>% 
  mutate(Area_nmi2 = round(0.2915533496*Area_km2, digits = 3)) %>%
  select(-c(ClstArea))
# View(areaTranBuffsum.prep2)

head(areaTranBuffsum.prep2)

```

### Check spatial joins

```{r, sptl.join.quality.ctrl}

# Returns TRUE/FALSE — basic validity
all(st_is_valid(areaTranBuffsum.prep2))
# [1] TRUE

# Detects self-intersections and overlapping edges
all(st_is_simple(areaTranBuffsum.prep2))
# [1] TRUE

plot(st_geometry(areaTranBuffsum.prep2), border = 'black')

```

### Check max area of single haul obs.

Because a single haul transects vary between 0.03 km2 and 1.78 km2, check for accuracy. 

```{r, check.single.haul.area}

# Find minimum number of clusters
min(areaTranBuffsum.prep2$NbClust)
# [1] 1

# Filter on the returned data where only 1 haul transect was used in area
# computation, and that had the maximum area
test.accuracy <- areaTranBuffsum.prep2 %>%
  filter(Hauls == 1) %>%
  filter(Area_km2 == max(Area_km2)) 
# View(test.accuracy)

# Create a logical vector where the filtered values are equal to the geometry
# of the original data
matches <- st_equals(area.transects.buff, test.accuracy, sparse = FALSE)

# Filter rows where there is at least one match (Note this will only return
# a single observation)
distance.test <- area.transects.buff[which(rowSums(matches) > 0), ]       

# Select the start and stop lat / lon from the unique keys         
distance.test2 <- distance.test %>%
  mutate(coords.str = str_extract(.$keys, "c\\([^\\)]+\\)"))
# 1) c\\( matches the literal "c(".
# 2) [^\\)]+ matches any characters except ")". So it grabs everything inside the 
# parentheses.
# 3) \\) matches the closing ")".

# Remove the "c()" part
distance.test2$coords.str <- str_remove_all(distance.test2$coords.str, "c\\(|\\)")

# Split by comma and convert to numeric
distance.test3 <- distance.test2 %>%
  st_drop_geometry() %>%
  select(coords.str)

# Create a data frame with two points  
vals <-  
  data.frame(
    lon_dists = c(as.numeric(strsplit(distance.test3$coords.str, ",\\s*")[[1]][1]),
                  as.numeric(strsplit(distance.test3$coords.str, ",\\s*")[[1]][2])),
    lat_dists = c(as.numeric(strsplit(distance.test3$coords.str, ",\\s*")[[1]][3]),
                  as.numeric(strsplit(distance.test3$coords.str, ",\\s*")[[1]][4])))

# Calculate the distance between the two points and manually calculate the distance
# and area using a 100 m buffer. Note that the values will be slightly different.
vals_sf <- vals %>%
  st_as_sf(., coords = c("lon_dists", "lat_dists"), crs = 4326) %>%
  st_transform(., crs = aea_custom) %>% 
  st_buffer(., units::as_units(100,"m"), max_cells=1e5) %>% 
  mutate(start_to_end_dist_m = as.numeric(st_distance(.[1,], .[2,]))) %>%
  mutate(backcalculatedArea = (start_to_end_dist_m * (2 * 100) + (pi * 100^2))/1e6)
# View(vals_sf)

```

### Sum observations by cluster

This gives us a summary of the total catch without regard to length and individual observations. Note that the sum of CtchWgL (all weights represented by individuals of length *i*) should be equal to the sum of total catches in the cluster (Ctch_wt, *c*).

```{r, group.hauls.by.cluster.pt2}

area.transects.buff2 <- area.transects.buff %>%
  st_drop_geometry() %>%
  distinct() %>%
  # Combine all unique identifier sequences per cluster excluding 
  # geometry (Note that we will later join geometries)
  group_by(date, tim_stp, clstGRP, age, len_bin)  %>%
  mutate(Total_W = round(sum(Total_W),2),
         WGHT_bin = round(sum(WGHT_bin),2),
         NBRE_samp = round(sum(NBRE_samp)),
         NBRE_bin = round(sum(NBRE_bin)),
         WghtLenBin = round(sum(WghtLenBin),2)) %>%
  ungroup() %>%
  select(date, tim_stp, clstGRP, len_bin, age, Year, Month, WGHT_bin, NBRE_bin, 
         NBRE_samp, Total_W, WghtLenBin, keys) %>%
  distinct() 
# View(area.transects.buff2)

```

### Join spatial and observational summaries

This allows us to maintain the geographic elements of the data.

```{r, group.hauls.by.cluster.pt3}

areaTranBuffsum.new <- inner_join(area.transects.buff2, areaTranBuffsum.prep2, 
                                 by = join_by(tim_stp, date, clstGRP)) %>%
  st_as_sf(., crs = aea_custom)
# View(areaTranBuffsum.new)

dim(areaTranBuffsum.new)
# [1] 5100   18

#-------------------------------------------------------------------------------  
# Graph output
#-------------------------------------------------------------------------------  

plotNormalHistogram(areaTranBuffsum.new$Area_km2)

plotNormalHistogram(areaTranBuffsum.new$Area_nmi2)

ggplot() +
  geom_sf(data = areaTranBuffsum.new, fill = "red", alpha = 0.5) +
  geom_sf(data = PreAbs.aea_custom, col = "black")

#-------------------------------------------------------------------------------  
# Save output
#-------------------------------------------------------------------------------  

write_sf(areaTranBuffsum.new, 
         file.path(paste(dir, "/Data/Shapefiles/CPS.ATM.Survey.Clusters.shp", 
                         sep = "")),
         overwrite = TRUE)

```

# Create summary table over full area extent

This has to be done in multiple steps.

## Simplify temporal aspects

```{r, summary.table.time.step}

areaTranBuff.polygons.summary <- areaTranBuffsum.new  %>%
  # For the summary table, remove Year and Month initially
  select(-c(Year, Month)) %>%
  # separate out date elements (note the other way would be to rename a 
  # vectored list) 
  separate(date, into = c("Year", "Month", "Day")) %>%
  # Create a numeric index of Month Day combination to sort on
  mutate(MonthDay.num = as.numeric(paste(Month, Day, sep = ""))) %>%
  # Create abbreviated month value
  mutate(Month = month.abb[as.numeric(Month)],
         Year = as.numeric(Year)) %>%
  # Create Month day label
  mutate(MonthDay = paste(Month, ". ", Day, sep = "")) %>%
  distinct()

dim(areaTranBuff.polygons.summary) 
# [1] 5100  20
# View(areaTranBuff.polygons.summary)

#-------------------------------------------------------------------------------
# Summarize the the corresponding months and days sampled
# per season and night or day sampling
#-------------------------------------------------------------------------------

Hauls.N <- areaTranBuff.polygons.summary %>%
  # Group Year, Season and night or day trawls
  group_by(tim_stp, clstGRP) %>% 
  arrange(MonthDay.num, .by_group = TRUE) %>%
  mutate(MonthDay = paste(first(MonthDay), "-", last(MonthDay))) %>%
  ungroup() %>%
  # Remove unique identifiers
  select(-c(MonthDay.num, Month, Day)) %>%
  distinct()
# View(Hauls.N)

```

## Simplify spatial aspects

Summarize the corresponding latitudinal range per season and night or day sampling.

```{r, summary.table.sptl.dims}

Hauls.N2 <- Hauls.N %>%
  # Group Year, Season and night or day trawls
  group_by(tim_stp, clstGRP) %>% 
    # For summary only, transform back to wgs84
  st_transform(., 4326) %>%
  mutate(
    geometry = st_union(geometry),  # union to combine geometries per group
    # Extract the lowest latitude from the transect key
    minLAT = min(st_coordinates(geometry)[, "Y"]),
    # Extract the greatest latitude from the transect key
    maxLAT = max(st_coordinates(geometry)[, "Y"]),
    #minLAT = min(as.numeric(tail(str_extract_all(keys, "\\d+\\.\\d+")[[1]], 2))),
    # Extract the greatest latitude from the transect key
    #maxLAT = max(as.numeric(tail(str_extract_all(keys, "\\d+\\.\\d+")[[1]], 2))),
    NbClust = NbClust,
    Hauls = Hauls,
    Absent_hauls = sum(is.na(len_bin))) %>%
    mutate(
      Latitudes = if (first(minLAT) == first(maxLAT)) {
      minLAT
      }else{
        paste(format(round(minLAT, 2), nsmall = 2), # 2 digits after
                      " - ", 
                      format(round(maxLAT, 2), nsmall = 2), sep = "")
        },
    Area_range_km2 = if (n_distinct(Area_km2) > 1){
      paste0(round(min(Area_km2), digits = 3), " - ", 
             round(max(Area_km2), digits = 3), sep = "")
      }else{
        as.character(first(Area_km2))
      },
    Area_range_nmi2 = if (n_distinct(Area_nmi2) > 1) {
      paste0(round(min(Area_nmi2), digits = 3), " - ", 
             round(max(Area_nmi2), digits = 3), sep = "")
      }else{
        as.character(first(Area_nmi2))
      }) %>%
  ungroup() %>%
  st_drop_geometry() %>%
  # Remove individual measure observations
  select(-c(minLAT, maxLAT, Area_km2, Area_nmi2, keys)) %>%
  distinct()

dim(Hauls.N2)  
# [1] 2747  17
# View(Hauls.N2)

```

## summarize observations

If there are presence observations, find length range. Note that if more than one time.step value exists, it means more than one observation is present!

### Sardine presence data

```{r, summary.table.sum.obs.pt1}

#-------------------------------------------------------------------------------
# If Sardine are present and only one observation:
#-------------------------------------------------------------------------------

Hauls.N.p1 <- Hauls.N2 %>% 
  group_by(tim_stp, clstGRP) %>%
  filter(n() == 1 & !is.na(len_bin)) 

if (nrow(Hauls.N.p1) != 0){
  Hauls.N.p1 <- Hauls.N.p1 %>% 
    mutate(Lengths = paste(len_bin, " cm", sep = ""),
           n_bin = sum(NBRE_bin), 
           n_samp = sum(NBRE_samp),
           Total_W = sum(Total_W),
           Ages = paste(age, " yrs", sep = "")) %>%
    ungroup() %>%
    distinct()
}else{
  rm(Hauls.N.p1)
}

#-------------------------------------------------------------------------------
# If Sardine are present and more than one observation:  
#-------------------------------------------------------------------------------

Hauls.N.p2 <- Hauls.N2 %>% 
  group_by(tim_stp, clstGRP) %>%
  filter(n() > 1 & !is.na(len_bin)) %>%
      mutate(Lengths = paste(min(na.omit(len_bin)), " - ",
                         max(na.omit(len_bin)), 
                         " cm", sep = ""),
         Ages = if (n_distinct(age) > 1) {
           paste(min(na.omit(age)), " - ",
                 max(na.omit(age)), " yrs")
           }else{
             paste(age, " yrs")
             },
         n_bin = sum(NBRE_bin), 
         n_samp = sum(NBRE_samp),
         Total_W = sum(Total_W)) %>%
  ungroup() %>%
  distinct()

# Join presence data sets:  
Hauls.N.p <- 
  if (exists("Hauls.N.p1")){
    Hauls.N.p = rbind(Hauls.N.p1, Hauls.N.p2) %>% 
        # Remove individual measures
        select(-c(age, len_bin, WGHT_bin, WghtLenBin, NBRE_bin, NBRE_samp)) %>%
        distinct()
    }else{
      Hauls.N.p = Hauls.N.p2 %>%
      # Remove individual measures
      select(-c(age, len_bin, WGHT_bin, WghtLenBin, NBRE_bin, NBRE_samp)) %>%
      distinct()
      }
# View(Hauls.N.p)

```

### Sardine absence data

If there are no presence observations, write absent and put NA or 0.

```{r, summary.table.sum.obs.pt2}

Hauls.N.a <- Hauls.N2 %>%
  filter(is.na(len_bin) & !tim_stp %in% Hauls.N.p$tim_stp) %>%
  mutate(Lengths = NA,
         Ages = NA,
         n_bin = 0, 
         n_samp = 0,
         Total_W = 0) %>%
  # Remove individual measures
  select(-c(age, len_bin, WGHT_bin, WghtLenBin, NBRE_bin, NBRE_samp)) %>%
  distinct()

```

### Join presence absence summary data

```{r, summary.table.sum.obs.pt3}

#-------------------------------------------------------------------------------  
# Merge presence absence data tables
#-------------------------------------------------------------------------------  

Hauls.N3 <- rbind(Hauls.N.p, Hauls.N.a) %>%
  distinct()

# Explore output  
head(Hauls.N3) 
sort(unique(Hauls.N3$Year))

#-------------------------------------------------------------------------------  
# Create an index
#-------------------------------------------------------------------------------  

Hauls.N3 <- Hauls.N3 %>%
  distinct()

EntryID <- seq_len(length(unique(Hauls.N3$Year)))
Year <- as.numeric(substr(unique(Hauls.N3$Year),1,4))
          
EntryID.df <- data.frame(EntryID, Year)
EntryID.df$Year <- EntryID.df$Year
          
#-------------------------------------------------------------------------------  
# Put everything together
#-------------------------------------------------------------------------------  

CPS.summary.table <- 
  inner_join(Hauls.N3, EntryID.df, relationship = "many-to-many",
             by = join_by(Year))
# View(CPS.summary.table)
          
# Group file output with entry id
dup.survey <- distinct(Hauls.N3) %>%
  mutate(Month2 =  str_extract(tim_stp, "\\d+$"))
# View(dup.survey)

```

# Select distinct values

This is our final summary table.

```{r, summary.table.sum.obs.pt4}

#-------------------------------------------------------------------------------  
# Simplify table
#-------------------------------------------------------------------------------  

dup.survey2 <- distinct(dup.survey[,c("Year", "Month2", "MonthDay", "clstGRP",
                                      "Hauls", "NbClust", "Area_range_km2",
                                      "Area_range_nmi2", "Absent_hauls", "Total_W",
                                      "Lengths", "Ages", "n_samp","Latitudes")])

dup.survey3 <- dup.survey2 %>%
  mutate(day.code =
           case_when(
             clstGRP %in% "day Trawl" ~ 1,
             clstGRP %in% "night Trawl" ~ 2),
         Month2 = as.numeric(Month2)) %>%
  mutate(sort.key = as.numeric(paste(Year, Month2, day.code, sep = ""))) %>%
  arrange(sort.key) %>%
  select(-c(sort.key, day.code, Month2))
# View(dup.survey3)

names(dup.survey3) <- c("Year", "MonthDay", "Sample Grp.", "Hauls", "Nb. Clusters",
                       "Area Sampled (km2)", "Area Sampled (nmi2)", "Nb. Absent Hauls",
                       "Total Catch (kg)", "Length Bins", "Est. Ages", 
                       "n sampled", "Latitudes") 
# View(dup.survey3)


#-------------------------------------------------------------------------------  
# Save output
#-------------------------------------------------------------------------------  

fwrite(dup.survey3, paste(dir,"/Results/summaryTable.csv", sep=""), sep=";")

```

# Create area definitions

Note that the min and max of the plot should have great spatial bound across all data sets and that this will likely change in the next iteration.

```{r, define.OM.box.extents}

#-------------------------------------------------------------------------------
# Find extent of CPS ATM Survey
#-------------------------------------------------------------------------------

# Find the center of the transects observed
st_transform(areaTranBuffsum.new, crs = 4326) %>% st_bbox()
#       xmin       ymin       xmax       ymax 
# -134.08076   28.65040 -114.79178   54.41662  

# Note the the observation extent should first be used to define the outer min 
# and max spatial boundaries. 

#-------------------------------------------------------------------------------
# Define data boundaries
#-------------------------------------------------------------------------------

xmin = -134.11630 # CUFES boundary
ymin =  28.5      # Artificial boundary
xmax = -114.0     # Artificial boundary
ymax = 54.74310   # CUFES boundary

#-------------------------------------------------------------------------------
# For Northern US EEZ to Inf
#-------------------------------------------------------------------------------

z1.bbox_coords <- c(xmin, unique(hlines$ymax), xmax, ymax)
names(z1.bbox_coords) = c("xmin","ymin","xmax","ymax")

z1bbp <- st_as_sfc(st_bbox(z1.bbox_coords)) 
st_crs(z1bbp) = 4326

st_bbox(z1bbp)

#-------------------------------------------------------------------------------
# For Northern stock to Northern US EEZ
#-------------------------------------------------------------------------------

z2.bbox_coords <- c(xmin, unique(hlines$nstock), xmax, unique(hlines$ymax))
names(z2.bbox_coords) = c("xmin","ymin","xmax","ymax")

z2bbp <- st_as_sfc(st_bbox(z2.bbox_coords)) 
st_crs(z2bbp) = 4326

st_bbox(z2bbp)

#-------------------------------------------------------------------------------
# For Southern stock to Northern stock
#-------------------------------------------------------------------------------

z3.bbox_coords <- c(xmin, unique(hlines$point.concept), xmax,
                    unique(hlines$nstock))
names(z3.bbox_coords) = c("xmin","ymin","xmax","ymax")

z3bbp <- st_as_sfc(st_bbox(z3.bbox_coords))
st_crs(z3bbp) = 4326

st_bbox(z3bbp)

#-------------------------------------------------------------------------------
# For Southern US EEZ to Southern stock
#-------------------------------------------------------------------------------

z4.bbox_coords <- c(xmin, unique(hlines$ymin), xmax,
                    unique(hlines$point.concept))
names(z4.bbox_coords) = c("xmin","ymin","xmax","ymax")

z4bbp <- st_as_sfc(st_bbox(z4.bbox_coords))
st_crs(z4bbp) = 4326

st_bbox(z4bbp)

#-------------------------------------------------------------------------------
# For Inf to Southern US EEZ
#-------------------------------------------------------------------------------

z5.bbox_coords <- c(xmin, ymin, xmax, unique(hlines$ymin))
names(z5.bbox_coords) = c("xmin","ymin","xmax","ymax")

z5bbp <- st_as_sfc(st_bbox(z5.bbox_coords))
st_crs(z5bbp) = 4326

st_bbox(z5bbp)

```

## Check OM spatial layout

This will give us an idea if the previous definitions are appropriate.

```{r, plot.OM.box.extents}

area.diams <-
  ggplot() +
  geom_sf(data = z1bbp, fill = "lightgrey", col = "black", alpha = 0.5) +
  geom_sf(data = z2bbp, fill = "blue", col = "black", alpha = 0.5) +
  geom_sf(data = z3bbp, fill = "lightgreen", col = "black", alpha = 0.5) +
  geom_sf(data = z4bbp, fill = "green", col = "black", alpha = 0.5) +
  geom_sf(data = z5bbp, fill = "steelblue", col = "black", alpha = 0.5) +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = PCoastStates,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = Populated10, col = "black") +
  geom_text(data = Populated10,
            aes(x = longitude, y = latitude, label = name),
            col = "black", 
            position = position_dodge(width = 1),
            vjust = 1) +
  scale_y_continuous(limits = c(27,55), 
                     breaks = lat_breaks, 
                     labels = lat_labels, 
                     expand = c(0.05, 0.05)) +
  labs(y = "Latitude")

area.diams

ggsave2(filename=paste("Area.Definitions.jpeg",sep=''), plot=area.diams,
        device="jpeg", path=paste(dir, "/Figures",sep=""),
        dpi=1200, width = 29, height=21, unit="cm", limitsize = FALSE)

```

# Tranform and merge area and transect data

Project the data and create and area,  and data index.

```{r, proj.and.common.scales}

# Create Multipolygon layer of OM box extents
bbox.frame <- st_sf(Area_id = 1:5, 
                    geometry = c(z1bbp, z2bbp, z3bbp, z4bbp, z5bbp)) %>%
  st_transform(.,crs = aea_custom)

```

# Spatially overlay data and area definitions

Note that ESIRI does not allow for DateTime and that this will need adjusted in the next step.

```{r, intersection.spatial.area.defs}

# 1. Perform polygon–polygon intersection
intersections <- st_intersection(
  areaTranBuffsum.new,  
  bbox.frame
)

# 2. Calculate area of overlap
intersections <- intersections %>%
  mutate(overlap_area = st_area(geometry))

# 3. Pick the Area_id with the greatest overlap for each original polygon
best_match <- intersections %>%
  group_by(date, clstGRP) %>%
  slice_max(overlap_area, with_ties = FALSE) %>%
  ungroup() %>%
  st_drop_geometry() %>%
  select(date, clstGRP, Area_id)

# 4. Join back to area.transects.buff and assign Area_id
areaTranBuffsum.polygons <- areaTranBuffsum.new %>%
  left_join(best_match, by = c("date", "clstGRP"))

unique(areaTranBuffsum.polygons$Area_id)
# [1] 1 2 3 4 5

sort(unique(areaTranBuffsum.polygons$Area_id))
# [1] 1 2 3 4 5
# View(areaTranBuffsum.new)

# Add back month and year data
areaTranBuffsum.polygons <- areaTranBuffsum.polygons %>%
  separate(date, into = c("Year", "Month", "Day"))

write_sf(areaTranBuffsum.polygons, 
        file.path(paste(dir, "/Data/Shapefiles/cluster.with.area.defs.shp",
                        sep="")),
        overwrite = TRUE)

```

# Plot clusters and areas

```{r, intersection.spatial.area.plot}

bbox.frame.wgs84 <- bbox.frame %>%
  mutate(Area_id = as.factor(Area_id)) %>%
  st_transform(., crs = 4326)

areaTranBuff.polygons.wgs84 <- areaTranBuffsum.polygons %>%
  mutate(Area_id = as.factor(Area_id)) %>%
  st_transform(., crs = 4326)

custom.pal <- c("1" = "lightgrey", "2" = "blue", "3" = "lightgreen", 
                "4" = "green", "5" = "steelblue")

# Plot to visually verify spatial overlap
Clusters.sdmTMB <-
  ggplot() +
  geom_sf(data = bbox.frame.wgs84, aes(fill = Area_id), alpha = 0.5) +
  scale_fill_manual(values = custom.pal, 
                    name = "OM Area") +
  geom_hline(data = hlines, aes(yintercept = nstock), col = "blue", 
             linewidth = 0.7, linetype = "dashed") +
  geom_hline(data = hlines, aes(yintercept = point.concept), col = "darkgreen", 
             linewidth = 0.7, linetype = "longdash") +
  geom_hline(data = hlines, aes(yintercept = ymin), col = "black", 
             linewidth = 0.7) +
  geom_hline(data = hlines, aes(yintercept = ymax), col = "black", 
             linewidth = 0.7) +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = PCoastStates,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = Populated10, col = "black") +
  geom_text(data = Populated10,
            aes(x = longitude, y = latitude, label = name),
            col = "black", 
            position = position_dodge(width = 1),
            vjust = 1) +
  geom_sf(data = areaTranBuff.polygons.wgs84, fill = "black", col = "black") +
  labs(y = "Latitude")

Clusters.sdmTMB 

ggsave2(filename=paste("CPS.ATM.Survey.Clusters.w.Area.Defs.jpeg",sep=''),
        plot=Clusters.sdmTMB, device="jpeg", 
        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
        unit="cm", limitsize = FALSE)

```

