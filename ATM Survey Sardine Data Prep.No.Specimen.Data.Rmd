---
title: "ATM Survey Sardine Data Prep"
author: "Stephanie Hopkins"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    
---

```{r setup, include=FALSE}

rm(list=ls())
gc()

load.lib <- c("tidyverse", "data.table", "sf", "here", "units")

install.lib <- load.lib[!load.lib %in% installed.packages()]

for(lib in install.lib) install.packages(lib,dependencies=TRUE)
sapply(load.lib,require,character=TRUE)

dir <- here(getwd())

survey.path <- "/Survey.Data/"

source("~/Final.OM.spatial.structure/functions.R")

```

# Load unmatched original and cleaned data

Recall the originally was 2676 collections. But when running the aggregations, only 1896 were present. Explore what to do with these 779 entries without a corresponding specimen table entry.

```{r, load.data.sets}

unmatched.data <- fread(paste(here(dir), "/Data", survey.path,
                              "ATM.Survey.no.corresponding.specimen.table.entry.csv", 
                              sep = ""),
                        sep = ",")

length(unique(unmatched.data$collection))
# [1] 779

Cleaned.data2 <- fread(paste(here(dir), "/Data", survey.path,
                             "2003.2024.ATM.Survey.with.specimen.data.csv", 
                             sep = ""), 
                       sep = ",")

length(unique(Cleaned.data2$collection))
# [1] 1896

```

# Check for collections accounted for

```{r, accounted.for.collections}

# Find the missing collection ids
original.collections.prestep <- unique(unmatched.data$collection) 
ending.collections <- unique(Cleaned.data2$collection)

# Isolate the position id
Accvals <- original.collections.prestep %in% ending.collections
Accvals <- which(Accvals)
Accvals

rm(Accvals)

```

Hauls without specimen data are unique and not accounted for in the original data filtering method. The next step is to try and reconcile these hauls and systematic way while minimizing assumptions around them.

# Explore which collections were not accounted for

```{r, isolate.unmatched.collections}

# Isolate the position id
vals <- !original.collections.prestep %in% ending.collections
vals <- which(vals)

# Create data frame to filter on
id <- seq(1,length(original.collections.prestep),1)

# Store collection names
index.tab <- tibble(id, original.collections.prestep) %>%
  filter(id %in% vals)

# Filter original data
missing.coll <- unmatched.data %>% 
  filter(collection %in% index.tab$original.collections.prestep)
# View(missing.coll)

```

The majority of unaccounted for hauls correspond to absence data so we can just add corresponding column names and values.

## For absent data

We are only interesting in this point as a location where Sardine are not.

### Add place holders

There is no need to redistribute weight, set everything to 0 or NA. 

```{r, absent.placeholders}

missing.coll.abs <- missing.coll %>% 
  # All but sardine
  filter(scientific_name != "Sardinops sagax") %>%
  # Group by transect
  group_by(collection, ship, haul, equilibrium_time.utc) %>% 
  mutate(
    # Set to zero because we only want this as a place holder
    Species_weight = 0, 
    # Set to zero because we only want this as a place holder
    weight = 0,
    # Set to zero because we only want this as a place holder
    subsample_weight = 0,
    # Set to zero because we only want this as a place holder
    specimen_number = NA,
    # Set to zero because we only want this as a place holder
    subsample_count = 0
    ) %>% 
  ungroup() %>%
  distinct()

```

### Secondly, look for spatial equivalence columns

Find corresponding spatial columns in the initial pre-processed data.

```{r, absent.spatial.columns}

# Check for columns to add
!colnames(Cleaned.data2) %in% colnames(missing.coll.abs)

# Pull the names of the columns not in the missing.coll2.abs table
new.cols <- Cleaned.data2 %>% select(c(2:3,14:15,17:22)) %>% names()
new.cols

# Check what latitude and longitude correspond to

identical(Cleaned.data2$latitude, Cleaned.data2$start_latitude)
# [1] TRUE

identical(Cleaned.data2$longitude, Cleaned.data2$start_longitude)
# [1] TRUE

```

### Check column names and structure

To be able to join the modified data back to the original pre-processed data, check that all columns are present and in the same structural format.

```{r, absent.remaining.columns}

# Since latitude and longitude correspond to starting position, 
# copy those values in the new missing.coll.abs data
missing.coll.abs <- missing.coll.abs %>%
  mutate(longitude = start_longitude,
         latitude = start_latitude,
         # for all other columns except
         # FID, put 0 or NA
         quality.indic = 0,
         specimen_number = NA,
         sex = NA,
         is_random_sample = NA,
         standard_length = NA,
         fork_length = NA,
         total_length = NA,
         mantle_length = NA
         ) %>%
  distinct() %>%
  mutate(# For FID, apply sequence from 
         # max FID value in Cleaned.data2
    FID = seq((max(Cleaned.data2$FID)+1), 
              (max(Cleaned.data2$FID)+ nrow(missing.coll.abs)), 1)
  )
# View(missing.coll.abs)

```

## For present data

For these observations, the goal is to find hauls with closest temporal and spatial distance to approximate the haul length frequency distributions based on length composition and proportional weight. We assume here that temporal distributions are more important than spatial distributions. 

### First create nescessary remaining weight columns

To calculate species weight, we need to obtain remaining weight. Because some values are recorded, while others are not, we need to treat them separately before joining them back together.

```{r, present.processing}

# For obs without a remaining_weight
missing.coll.pres.nas <- missing.coll %>%
  filter(is.na(remaining_weight) & scientific_name == "Sardinops sagax" &
           !is.na(subsample_count)) %>%
  # Group by transect
  group_by(collection, ship, haul, equilibrium_time.utc) %>% 
  mutate(# Create a place holder for specimen weight
    remaining_weight = 0) %>%
  mutate(
    # Calculate species weight
    Species_weight = (subsample_weight + remaining_weight)
    ) %>%
  ungroup()

# For obs with a remaining_weight
missing.coll.pres <- missing.coll %>%
  filter(!is.na(remaining_weight) & 
           scientific_name == "Sardinops sagax" & 
           !is.na(subsample_count)) %>%
  # Group by transect
  group_by(collection, ship, haul, equilibrium_time.utc) %>% 
  mutate(# Create a place holder for specimen weight
    Species_weight = (subsample_weight + remaining_weight)
    ) %>%
  ungroup()

# Join back data sets
missing.coll.pres <- rbind(missing.coll.pres, missing.coll.pres.nas)

```

### Secondly, look for spatial equivalence columns

Here columns are added similar to the absence data, and spatial columns in the initial pre-processed data are compared to find corresponding spatial columns in the missing.coll.pres data.

```{r, present.processing}

# Check for columns to add
!colnames(Cleaned.data2) %in% colnames(missing.coll.pres)

# Pull the names of the columns not in the missing.coll2.abs table
new.cols <- Cleaned.data2 %>% select(c(2:3,5,9,14:15,17:22)) %>% names()
new.cols

# Since latitude and longitude correspond to starting position, 
# copy those values in the new missing.coll.pres data
missing.coll.pres <- missing.coll.pres %>%
  mutate(longitude = start_longitude,
         latitude = start_latitude,
         quality.indic = 0,
         
         ) %>%
  distinct() %>%
  mutate(# For FID, apply sequence from 
         # max FID value in Cleaned.data2
    FID = seq((max(missing.coll.abs$FID)+1), 
              (max(missing.coll.abs$FID)+ nrow(missing.coll.pres)), 1)
  )
# View(missing.coll.pres)

```

### Check if there are other observations to compare with

Match corresponding length frequency data by time and space

```{r, Check.spatial.temporal.distance}

# Create transect data
targetDat_sf <- make_transect_sf(missing.coll.pres) %>%
  mutate(DatSource = "target") %>%
  make_transect_sf(.) %>%
  distinct() %>%
  group_by(collection, ship, haul, equilibrium_time.utc, DatSource, geometry) %>%
  # Create a single group id and assign it to all rows in that group
  mutate(key = cur_group_id()) %>%
  ungroup() %>%
  st_transform(., crs = 5070)

# Create unique grouping key
Cleaned.data2.temp <- Cleaned.data2 %>%
  filter(scientific_name == "Sardinops sagax") %>%
  make_transect_sf(.) %>%
  distinct() %>%
  group_by(collection, ship, haul, equilibrium_time.utc, geometry) %>%
  # Create a single group id and assign it to all rows in that group
  mutate(key = cur_group_id()) %>%
  ungroup()
  
# For known data (pre-processed data where specimen samples are present)
knownDat_sf <- Cleaned.data2.temp %>%
  select(key, cruise, collection, ship, haul, equilibrium_time.utc, geometry) %>%
  distinct() %>% 
  mutate(DatSource = "source") %>%
  st_transform(., crs = 5070)
  
# Extract projected coordinates (in meters)
targetDat_xy <- st_geometry(st_centroid(targetDat_sf))
knownDat_xy <- st_geometry(st_centroid(knownDat_sf))

## Spatial distance matrix (Euclidean in meters)
dist_matrix <- st_distance(targetDat_xy, knownDat_xy)
dim(dist_matrix)
# [1] 7 757

# Remove the units (in metres)
dist_matrix.no.units <- drop_units(dist_matrix)

## Time distance matrix (in mins)
temp_matrix <- compute_time_diff_matrix(targetDat_sf$equilibrium_time.utc,
                                        knownDat_sf$equilibrium_time.utc,
                                        units = "secs")
dim(temp_matrix)
# [1] 7 757

#-------------------------------------------------------------------------------
# Combine with weights
#-------------------------------------------------------------------------------

# Create a spatial weight measure for metres
spatial_weight <- 1

# Create a temporal weight measure for seconds 
## Note this is like treating 1 second as 1 meter
time_weight <- 1

# Now combine
combined_score <- dist_matrix.no.units * spatial_weight + temp_matrix * time_weight

# Index of nearest matching (row in knownDat_sf)
nearest_index <- apply(combined_score, 1, function(x) which.min(x))
nearest_index

#-------------------------------------------------------------------------------
# Add keys and values
#-------------------------------------------------------------------------------

# Append matched values (change 'FID' and others to your actual column names)
targetDat_sf$nearest_key <- knownDat_sf$key[nearest_index]

# Append nearest Date/Time values
targetDat_sf$nearest_tim <-
  knownDat_sf$equilibrium_time.utc[nearest_index]

# Append nearest haul day
targetDat_sf$minTempDist_d <- temp_matrix[cbind(seq_len(nrow(temp_matrix)),
                                                nearest_index)]/(60*60*24)

# Append nearest haul coordinates
targetDat_sf <- targetDat_sf %>%
  mutate(nearest_haul_loc = st_geometry(st_centroid(knownDat_sf))[nearest_index] %>%
  # Convert back to WGS84
  st_transform(., crs = 4326)) %>%
  mutate(nearest_haul_loc = as.character(nearest_haul_loc)) %>%
  st_drop_geometry()

# Append nearest km distance between hauls
targetDat_sf$minSptlDist_km <- 
  dist_matrix.no.units[cbind(seq_len(nrow(dist_matrix.no.units)),
                             nearest_index)]/1000

# Append metre/second combined value
targetDat_sf$combined_score <- combined_score[nearest_index]
# View(targetDat_sf)

```

### Join lenth comps based on closest values
  
```{r, assign.closes.lenComps}

filldat <- Cleaned.data2.temp %>%
  filter(key %in% targetDat_sf$nearest_key) %>%
  mutate(nearest_key = key) %>%
  select(nearest_key, weight, standard_length) %>%
  st_drop_geometry()

targetDat_sf.f <- inner_join(targetDat_sf, filldat, relationship = "many-to-many")
#View(targetDat_sf.f)

```

### Save summary of corresponding values

```{r, summary}

targetDat_sf.f.summary <- targetDat_sf.f %>%
  select(cruise, ship, haul, collection, start_latitude, start_longitude,
         stop_latitude, stop_longitude, equilibrium_time.utc, nearest_key,
         nearest_tim, minTempDist_d, nearest_haul_loc, minSptlDist_km,
         combined_score) %>%
  distinct() %>%
  st_drop_geometry()
# View(targetDat_sf.f.summary)

fwrite(targetDat_sf.f.summary, 
       paste(here(dir), "/Data", survey.path,
             "ATM.Survey.estimated.specimen.weight-lenComps.csv", 
             sep = ""),
       sep = ",")

```

### Distribute weight among lengths
 
```{r, specimen.weight.partitioning}

targetDat_sf.f <- targetDat_sf.f %>%
  group_by(collection, ship, haul, equilibrium_time.utc, scientific_name) %>%
  mutate(prop.len.wgt = weight/sum(weight),
         sex = NA,
         mantle_length = NA,
         fork_length = NA,
         total_length = NA,
         is_random_sample = NA,
         specimen_number = NA) %>%
  ungroup() %>%
  group_by(collection,standard_length) %>%
  mutate(new.wgt = subsample_weight*prop.len.wgt) %>%
  ungroup() %>%
  distinct() %>%
  mutate(weight = new.wgt) %>%
  select(-c(new.wgt, nearest_key, nearest_tim, minTempDist_d, nearest_haul_loc,
            minSptlDist_km, combined_score, key, DatSource, prop.len.wgt)) %>%
  st_drop_geometry()
# View(targetDat_sf.f)

```

### Recode FID to account for length comps

Because we expanded the original row numbers (FID) values to account for length compositions, FID values need to be reassigned. 

```{r, new.FIDs}

targetDat_sf.f2 <- targetDat_sf.f %>%
  mutate(FID = seq(min(targetDat_sf.f$FID), 
              (min(targetDat_sf.f$FID) + nrow(targetDat_sf.f)-1), 1))
# View(targetDat_sf.f2)

```

# Join all data sets

```{r, final.merge}

allDat <- rbind(missing.coll.abs, Cleaned.data2, targetDat_sf.f2)

# Check that FID was applied correctly
unique(duplicated(allDat$FID))

fwrite(allDat, 
       paste(here(dir), "/Data", survey.path,
             "2003.2024.ATM.Survey.with.integrated.specimen.data.csv", 
             sep = ""),
       sep = ",")

```

