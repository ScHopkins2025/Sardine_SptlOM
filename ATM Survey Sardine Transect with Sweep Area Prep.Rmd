---
title: "ATM Survey Sardine Transect with Sweep Area Prep"
author: "Stephanie Hopkins"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    
---

```{r setup, include=FALSE}

# Install rnaturalearthhires if not installed already
# devtools::install_github("ropensci/rnaturalearthhires")

rm(list=ls())
gc()

load.lib <- c("tidyverse", "data.table", "cowplot", "sf", "rcompanion", "rnaturalearth", "rnaturalearthhires", "r4ss", "zoo", "suncalc", "here", "furrr")

install.lib <- load.lib[!load.lib %in% installed.packages()]

for(lib in install.lib) install.packages(lib,dependencies=TRUE)
sapply(load.lib,require,character=TRUE)

# Turn off s2 and use
# geos since you are computing
# distance and area
sf_use_s2(FALSE)

dir <- here(getwd())

survey.path <- "/Survey.Data/"

Figure.path <- "Figures/"

source(paste0(dir,"/functions.R"))

```

# Read data

Remember to convert length to cm to be able to use VBGF!

```{r, read.data}

full.series <- 
  fread(paste(here(dir), 
              "/Data", survey.path,
              "2003.2024.ATM.Survey.with.integrated.specimen.data.csv", 
              sep = ""), sep = ",") %>%
  # convert mm to cm to be able to use VBGF
  mutate(standard_length = standard_length/10) %>%
  select(-FID) %>%
  distinct()

length(unique(full.series$collection))
# [1] 2673

colnames(full.series)

```

# Mean of the haul geometry

Note that we are taking the mean of the starting point and ending point to denote the centroid.

```{r, mean.haul.geom}

#-------------------------------------------------------------------------------
# For Shooting location
#-------------------------------------------------------------------------------

range(full.series$start_latitude)
# [1] 28.6513 54.3997
Lat1 <- full.series$start_latitude

mean(full.series$start_latitude)
# [1] 39.06714

range(full.series$start_longitude)
# [1] -134.0793 -114.7928
Lon1 <- full.series$start_longitude

mean(full.series$start_longitude)
# [1] -122.4603

#-------------------------------------------------------------------------------
# For hauling location
#-------------------------------------------------------------------------------

range(full.series$stop_latitude)
# [1] 28.6548 54.4157
Lat2 <- full.series$stop_latitude

range(full.series$stop_longitude)
# [1] -134.0325 -114.8483
Lon2 <- full.series$stop_longitude

#-------------------------------------------------------------------------------
# Find the mean of Shooting and Hauling location
#-------------------------------------------------------------------------------

LAT <- data.table(Lat1, Lat2)
LON <- data.table(Lon1, Lon2)

LAT <- rowMeans(LAT)
LON <- rowMeans(LON)
 
plot(LON, LAT)

# Since the data falls within two UTM zones and there is not an ideal projection to use, 
# apply a custom equal area albers projection
aea_custom <- "+proj=aea +lat_1=30 +lat_2=50 +lat_0=40 +lon_0=-124.5 +datum=WGS84 +units=m +no_defs"

```

# Verify georefrence values

This will tell us whether or not to clip the data when we make plots.

```{r lat.lon.extent}

range(LAT)
# [1] 28.65305 54.40770

range(LON)
# [1] -134.0559 -114.8205

```

# Spatialize the transect data 

For each haul transect, group together the starting and stopping latitudinal longitudinal coordinates and create a line string spatial object.

```{r, spatial.transects.defs}

# Create a linestring for each transect
trip_rect <- make_transect_sf(full.series)

summary(trip_rect)
  
```

# Determine fishing duration

Note that for initial plots, CPUE was defined as kg/hr, but since we are estimating biomass abundance, we area is needed instead. For each haul transect, the same CPUE values are present for both computational methods.

```{r, convert.POSIXct.object}

full.series.sf <- trip_rect %>%
  group_by(cruise, ship, haul, equilibrium_time.utc, collection, geometry) %>%
  mutate(haulback_time.pacific = 
           as.POSIXct(haulback_time.utc, tz = "America/Los_Angeles"),
         equilibrium_time.pacific = 
           as.POSIXct(equilibrium_time.utc, tz = "America/Los_Angeles")) %>%
  ungroup() 

head(unique(full.series.sf$haulback_time.utc))
head(unique(full.series.sf$haulback_time.pacific))
head(unique(full.series.sf$equilibrium_time.utc))
head(unique(full.series.sf$equilibrium_time.pacific))
  
```

There are many different values used to assess length at age in the region and there are high levels of variability between individuals, years, seasons, and cohorts. For example K was historically nearly double that of the most recent publication (Wildermuth et al., 2024), where previous values were L(inf) = 31 cm, and K = 0.460 (Female) off the coast of California >> Phillips, J.B., 1948. Growth of the sardine *Sardinops caerulea* 1941-42 through 1945-47. Calif. Dept. Fish and Game. Calif. Fish. Bull. 71:33 p. Below, we investigate the potential of using the Wildermuth et al., 2024 values for L(inf), K, and t0 in the extended data set with all lengths considered.

```{r, extract.ss.growth.params}

# Where ss output files are
model_dir <- here(getwd(), "Data", "SSMSE.Base.Model.Data", "start2001",
                  "constGrowthMidSteepNewSelex_OM")

# Read ssouput file
OM_age.length <- SSgetoutput(dir = file.path(model_dir))

# Check structure
str(OM_age.length)

# How many age classes were fed into the model (model forecasts the rest)
OM_age.length$replist1$agebins
# [1] 0 1 2 3 4 5 6 7 8

#-------------------------------------------------------------------------------
# Below are the values needed for VBGF
#-------------------------------------------------------------------------------

# Estimated age
Linf <- OM_age.length$replist1$Growth_Parameters$Linf
Linf
# [1] 25.153

K <- OM_age.length$replist1$Growth_Parameters$K
K
# [1] 0.273111

# A1 is the starting length (true age 0 values)
A1 <- OM_age.length$replist1$Growth_Parameters$A1
A1 
# [1] 0.5

# A2 is either fixed at a theoretical maximum or inf (999)
A2 <- OM_age.length$replist1$Growth_Parameters$A2
A2 
# [1] 999

# Smallest observed length in the data set
L_a_A1 <- OM_age.length$replist1$Growth_Parameters$L_a_A1
L_a_A1
# [1] 13.1744

# You can also index them all at once
OM_age.length$replist1$Growth_Parameters

#-------------------------------------------------------------------------------
# To find t0, which is estimated in the ss model rather than explicit
#-------------------------------------------------------------------------------

# Equation used
t0 <- A1 + (log(1-L_a_A1/Linf))/K
t0
# [1] -2.216315

# Undefined parameter in the model handbook thought to be t0
A_a_L0 <- OM_age.length$replist1$Growth_Parameters$A_a_L0
A_a_L0
# [1] -2.2163

# Things to look up later for types of errors (!= fleets, != # of params)
OM_age.length$replist1$age_error_mean
OM_age.length$replist1$age_error_sd

# Returns the Maximum Likelihood Estimates (MLE)
OM_age.length$replist1$growthseries[1,5:15]

# Create a vector of mle values
mle.vals <- c(t(OM_age.length$replist1$growthseries[1,5:15]))

# Create a vector of ages
age.vals <- seq(0,10,1)

# Combine in a data frame ages and mle values
ss.age.len <- data.frame(mle.vals, age.vals)

# To view parameterizations and associated sd values
OM_age.length$replist1$parameters 

```

# Estimate age

Use the values extracted from the ss model for pacific sardine

```{r, apply.inverse.vbgg}

# Apply VBGF
Sardine <- full.series.sf %>% 
  filter(scientific_name == "Sardinops sagax") %>%
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry) %>%
  mutate(age = round((-1/K)*log(1-(standard_length/Linf)) + A_a_L0)) %>% 
  ungroup() %>%
  group_by(age) %>%
  mutate(mean.length.age = mean(standard_length)) %>%
  ungroup() %>%  
  distinct()

# Explore NaN values
sardine.nans.pt1 <- Sardine %>%
  filter(is.na(age))

# How many observations are lost
dim(sardine.nans.pt1)
# [1] 1118  41

# What lengths are not represented
sardine.nans.pt2 <- sardine.nans.pt1 %>%
  distinct(standard_length)

# Vector of lengths not represented
c(t(sardine.nans.pt2))
#  [1] 25.2 25.8 27.1 25.3 26.7 25.5 25.6 26.2 25.4 25.7 26.0 25.9 26.8 28.9 26.1
# [16] 26.4 26.3 28.4 27.2 27.3 27.4 28.1 26.5 26.9 27.8 26.6 27.5 27.0 27.6 28.0
# [31] 29.2 27.9 28.2 27.7 29.1

# Plot age length per haul, average age per haul, and ss mle values 
# (when excluding larger lengths from the data)...not a sensitivity analysis is 
# still needed
age.length.plot <-
  ggplot() + 
  geom_point(data = Sardine, aes(x=age, y=standard_length), col = "black") +
  geom_line(data = Sardine, aes(x=age, y=mean.length.age), col = "red") +
  geom_line(data=ss.age.len, aes(x=age.vals, y=mle.vals), col = "blue") + 
  scale_y_continuous(breaks = seq(0, 31, by = 2)) +
  scale_x_continuous(breaks = seq(0, 20, by = 1)) +
  ylab("Standard Length (cm)") +
  xlab("Estimate Age (yr)") +
  theme_classic()

age.length.plot

ggsave2(filename=paste("age.length.keys.jpeg",sep=''),
        plot=age.length.plot, device="jpeg", 
        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
        unit="cm", limitsize = FALSE)

```

# Define length bin definitions for Sardine only

Because aging is uncertain, propose length bins where the first 100mm are grouped (considered age 0), and where every 2.25 cm is added.

```{r, length.bin.defs}

len.bins <- Sardine

head(len.bins)

range(len.bins$standard_length)
# [1]  2.5 29.2

#-------------------------------------------------------------------------------
# Check lowest intervals
#-------------------------------------------------------------------------------

len100 <- len.bins %>%
  filter(standard_length <= 10) %>%
  mutate(len.bin = "100")

sort(unique(len100$age))
# [1] -2 -1 0

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len125 <- len.bins %>%
  filter(standard_length > 10 & standard_length < 12.6) %>%
  mutate(len.bin = "125")

sort(unique(len125$age))
# [1] 0

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len150 <- len.bins %>%
  filter(standard_length > 12.5 & standard_length < 15.1) %>%
  mutate(len.bin = "150")

sort(unique(len150$age))
# [1] 0 1

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len175 <- len.bins %>%
  filter(standard_length > 15 & standard_length < 17.6) %>%
  mutate(len.bin = "175")

sort(unique(len175$age))
# [1] 1 2

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len200 <- len.bins %>%
  filter(standard_length > 17.5 & standard_length < 20.1) %>%
  mutate(len.bin = "200")

sort(unique(len200$age))
# [1] 2 3 4

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len225 <- len.bins %>%
  filter(standard_length > 20 & standard_length < 22.6) %>%
  mutate(len.bin = "225")

sort(unique(len225$age))
# [1] 4 5 6

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm
#-------------------------------------------------------------------------------

len250 <- len.bins %>%
  filter(standard_length > 22.5 & standard_length < 25.1) %>%
  mutate(len.bin = "250")

sort(unique(len250$age))
# [1] 6 7 8 9 10 11 12 13 15 16

#-------------------------------------------------------------------------------
# Shift bin 2.5 cm and group values
#-------------------------------------------------------------------------------

len250plus <- len.bins %>%
  filter(standard_length > 25.5) %>%
  mutate(len.bin = "250+")

#-------------------------------------------------------------------------------
# Re-code negative age to 0
#-------------------------------------------------------------------------------

len100.negs <- len100 %>%
  filter(age < 0) %>%
  mutate(age = 0)

len100.0s <- len100 %>%
  filter(age == 0)

len100.new <- rbind(len100.negs,  len100.0s)

#-------------------------------------------------------------------------------
# Join data sets 
#-------------------------------------------------------------------------------

len.bins2 <- rbind(len100.new, len125, len150, len175, len200, len225, len250, len250plus)

#-------------------------------------------------------------------------------
# Re-code Sardines NaNs and to 13+ ages to 10+
#-------------------------------------------------------------------------------

len.bins2.ok <- len.bins2 %>%
  filter(!is.na(age) & age < 10)

len.bins2.no <- len.bins2 %>%
  filter(age > 9) %>%
  mutate(age = 10)

len250plus <- len250plus %>%
  filter(is.na(age)) %>%
  mutate(age = 10)

len.bins2 <- rbind(len.bins2.ok, len.bins2.no, len250plus)

```

# Calculate the total catch weight for each length

Note that here we are standardizing and correcting the data to remove noise and better match the outputs in the commercial data sets.

```{r, calculate.median.weight.per.lenbin}

len.bins3 <- len.bins2 %>%
  # Compute theoretical weight per individual
  mutate(TheoreticalWgt_i = 7.52e-06 * standard_length^3.23,
         ) %>%
  
  # Step 1: Aggregate per bin, haul, age
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry, 
           age, len.bin) %>%
  mutate(
    WGHT_bin = sum(as.numeric(weight)),
    NBRE_bin = n(),
    TheoreticalWgt_median = median(TheoreticalWgt_i),
    # haul-level total count (assumed constant within haul)
    NBRE_samp = first(subsample_count),
    # haul-level total weight (assumed constant within haul)
    Total_W = first(Species_weight)
  ) %>%
  ungroup() %>%
  
  # Step 2: Calculate proportion per bin
  mutate(
    PropLen_bin = NBRE_bin / NBRE_samp
  ) %>%
  
  # Step 3: Calculate EstimatedWeight and Correction.factor per haul group
  group_by(cruise, ship, haul, equilibrium_time.pacific, collection, geometry) %>%
  mutate(
    EstimatedWeight = sum(PropLen_bin * TheoreticalWgt_median) * NBRE_samp,
    Correction.factor = Total_W / EstimatedWeight
  ) %>%
  ungroup() %>%
  
  # Step 4: Calculate corrected catch weight per bin
  mutate(
    CatchWeightLen_bin = NBRE_bin * TheoreticalWgt_median * Correction.factor
  )

summary(len.bins3)

```

## Check outputs

```{r, check.length.distributions}

dim(len.bins3)
# [1] 17163  49

dim(distinct(len.bins3))
# [1] 17163  49

len.bins3$geometry

```

# Expand spatial temporal elements for hauls with Sardine

```{r, sardine.presence}

Sardine.present <- len.bins3 %>%
  group_by(cruise, ship, haul, collection, equilibrium_time.pacific, geometry) %>%
  mutate(Year = as.numeric(substr(equilibrium_time.pacific,1,4)),
         Month = as.numeric(substr(equilibrium_time.pacific,6,7))) %>%
  mutate(time.step = paste(Year," - ", Month, sep=""),
         keys = paste(cruise, ship, haul, collection, equilibrium_time.pacific,
                      geometry, sep = "-")) %>%
  ungroup() %>%
  select(-c(longitude, latitude, quality.indic, sex, is_random_sample, fork_length, 
            total_length, mantle_length, presence_only, ship_spd_through_water, 
            surface_temp_method, surface_temp, TheoreticalWgt_i,
            TheoreticalWgt_median, PropLen_bin, EstimatedWeight, Correction.factor,
            mean.length.age)) %>%
  distinct()
# View(Sardine.present)

```

# Expand spatial temporal elements for hauls with Sardine

```{r, Absent.bin}

Others <- full.series.sf %>% 
  filter(scientific_name != "Sardinops sagax") %>%
  group_by(cruise, ship, haul, collection, equilibrium_time.pacific, geometry) %>%
  mutate(Year = as.numeric(substr(equilibrium_time.pacific,1,4)),
         Month = as.numeric(substr(equilibrium_time.pacific,6,7))) %>%
  mutate(time.step = paste(Year," - ", Month, sep=""),
         keys = paste(cruise, ship, haul, collection, equilibrium_time.pacific,
                      geometry, sep = "-")) %>%
  ungroup() %>%
  select(-c(longitude, latitude, quality.indic, sex, is_random_sample, fork_length, 
            total_length, mantle_length, presence_only, ship_spd_through_water, 
            surface_temp_method, surface_temp)) %>%
  distinct() %>%
  mutate(remaining_weight = 0, 
         len.bin = NA, 
         age = NA, 
         NBRE_bin = 0, 
         NBRE_samp = 0, 
         CatchWeightLen_bin = 0,
         ) %>%
  distinct()

```

# Sum tables and simplify

Check for unique keys where no sardines were caught. These will be treated as true absent values.

```{r, view.absent.present.data.structure}

# Check that no weird stuff is happening (there shouldn't be)
!colnames(Sardine.present) %in% colnames(Others)
names(Sardine.present)[27]
names(Sardine.present)[30]

# Add unmatched columns to Others
Others <- Others %>%
  mutate(WGHT_bin = 0,
         Total_W = 0)

# Double check column names from other direction
!colnames(Others) %in% colnames(Sardine.present)

# Filter out hauls that had Sardine catch. What is left
# will be considered "true" absence.
Absent <- Others %>%
  filter(!keys %in% Sardine.present$keys)
# View(Absent)

#-------------------------------------------------------------------------------
# Check structure before join
#-------------------------------------------------------------------------------

str(len.bins3)
str(Absent)

# Check time zones
head(Sardine.present$equilibrium_time.pacific)
head(Absent$equilibrium_time.pacific)

```

# Complete table joins

From the structure it is necessary to change the len.bin3 equilibrium_time.pacifc to POSIXct.

```{r, join.tables}

# Formats match, join the two data frames.
PreAbs <- rbind(Sardine.present, Absent) %>%
  # Create a data object
  mutate(date = as.Date(equilibrium_time.pacific))
# View(PreAbs)

unique(is.na(PreAbs))

fwrite(PreAbs, paste(here(dir), "/Data", survey.path,
                           "CPS.ATM.Survey.final.dat.csv", sep=""), sep = ";")

# plot locations
ggplot() +
  geom_sf(data = PreAbs, col = "black")

# Project data for intersection analyses
PreAbs.aea_custom <- PreAbs %>%
  st_transform(., crs = aea_custom)

# plot projected data locations
ggplot() +
  geom_sf(data = PreAbs.aea_custom, col = "black")

```

# Find min and max geographical extents

Note this is only for graph labels and is chosen based on the mean lat and lon values of the haul transect locations.

```{r, Create.plot.label.breaks}

st_bbox(PreAbs) 
#      xmin      ymin      xmax      ymax 
# -134.0793   28.6513 -114.7928   54.4157 

lat_breaks <- seq(27, 55, 4)  # Latitude breaks

lat_labels <- c(paste(27, "\u00b0", "N", sep=""),
                paste(31, "\u00b0", "N", sep=""),
                paste(35, "\u00b0", "N", sep=""),
                paste(39, "\u00b0", "N", sep=""),
                paste(43, "\u00b0", "N", sep=""),
                paste(47, "\u00b0", "N", sep=""),
                paste(51, "\u00b0", "N", sep=""),
                paste(55, "\u00b0", "N", sep=""))

```

# Create base map

Note that cropping should be done before projecting the data and projecting the data should be done before any intersection analysis.

```{r, basemap}

#-------------------------------------------------------------------------------
# Make coastline backdrop
#-------------------------------------------------------------------------------

PacificCoast <- ne_countries(scale = 10, returnclass = "sf") 
PacificCoast2 <- st_crop(PacificCoast, xmin = -133, xmax = -110, ymin = 25, 
                         ymax = 55)
class(PacificCoast2)

#-------------------------------------------------------------------------------
# Plot output
#-------------------------------------------------------------------------------

ggplot() +
    geom_sf(data = PacificCoast2, fill = "#D2B48C", col = "#927143") +
    xlab("Longitude") + ylab("Latitude") +
    ggtitle("West Coast map") +
    theme_classic()

```

## Apply min and max geographical extents

```{r, adjust.map.dimensions}

#-------------------------------------------------------------------------------
# Convert to sf object
#-------------------------------------------------------------------------------

PacificCoast3.wgs84 <- PacificCoast2 %>%
  st_transform(., crs = 4326)

plot(PacificCoast3.wgs84)

#-------------------------------------------------------------------------------
# Find spatial extents
#-------------------------------------------------------------------------------

xmin <- st_bbox(PacificCoast3.wgs84)["xmin"]
xmin
# [1] -133

xmax <- st_bbox(PacificCoast3.wgs84)["xmax"]
xmax
# [1] -110

#-------------------------------------------------------------------------------
# Crop land mass
#-------------------------------------------------------------------------------

PacificCoast4.wgs84 <- PacificCoast3.wgs84 %>% 
  # Clip the shapefile to match the latitude range
  st_crop(PacificCoast3.wgs84, 
          xmin = -133, 
          xmax = -111,
          ymin = 27, 
          ymax = 55)
  
#-------------------------------------------------------------------------------
# Plot
#-------------------------------------------------------------------------------

PacificCoast4.wgs84.map <-
  ggplot() +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  theme_void() +
  scale_y_continuous(limits = c(27,55), 
                     breaks = lat_breaks, 
                     labels = lat_labels, 
                     expand = c(0.05, 0.05)) +
  labs(y = "Latitude")

PacificCoast4.wgs84.map

st_bbox(PacificCoast4.wgs84)
#     xmin ymin xmax ymax
# [1] -133   27 -111   55

```

## Creat hline boundaries

Because the current model looks at only the Northern sub population in US waters, one of the boundary extents should include the national boarders. The second should look at the coast of California where the most mixing is thought to occur for Pacific Sardine sub populations (under the current survey hypothesis). From the data we are looking at data from UTM zone 10 and UTM zone 11 use the USA Contiguous Albers Equal Area Conic project (ESRI:102003). The corresponding ESPG code for ESRI:102003 is aea_custom.

```{r, find.original.model.break.dimensions}

# First transform the data to be able to do intersection analysis
PacificCoast4.aea_custom <- st_transform(PacificCoast4.wgs84, crs = aea_custom)

#-------------------------------------------------------------------------------
# In order to find the state boundary extents, use ne_states function in 
# rnaturalearth
#-------------------------------------------------------------------------------

# Find state boundaries
PCoastStates.wgs84 <- ne_states(returnclass = "sf")

PCoastStates.aea_custom <-
  # transform to NAD83 / Conus Albers
  st_transform(PCoastStates.wgs84, crs = aea_custom) 

PCoastStates <- st_intersection(PCoastStates.aea_custom, PacificCoast4.aea_custom) %>%
  st_transform(., crs = 4326)

#-------------------------------------------------------------------------------
# For EEZ definitions 
#-------------------------------------------------------------------------------

# https://nauticalcharts.noaa.gov/data/us-maritime-limits-and-boundaries.html

eez.dir <- here(getwd(), "Data", "EEZ.Boundaries")
eez <- read_sf(paste(eez.dir, "/USMaritimeLimitsNBoundaries.shp", sep = ""))

#-------------------------------------------------------------------------------
# Putting it all together
#-------------------------------------------------------------------------------

hlines <- eez %>%
  filter(REGION == "Pacific Coast") %>%
  mutate(
    # Create southern US EEZ boundary
    ymin = st_bbox(.)["ymin"],
    # Create northern US EEZ boundary
    ymax = st_bbox(.)["ymax"],
    # Create northern stock boundary (Westernmost Point, CA)
    nstock = 40.4385,
    # Create southern stock boundary (Point Conception, CA)
    point.concept = 34.4486)

```

## Create city markers

To make the map easier to interpret, add the names and locations of most populated cites

```{r, add.map.details}

#-------------------------------------------------------------------------------
# populated areas (cities and city names)
#-------------------------------------------------------------------------------

Populated10.wgs84 <- ne_download(scale = 110, type = "populated_places_simple", 
                           category = "cultural", 
                           returnclass = "sf") 

Populated10.aea_custom <- Populated10.wgs84 %>%
  st_transform(., crs = aea_custom)

Populated10 <- st_intersection(Populated10.aea_custom, PacificCoast4.aea_custom) %>%
  st_transform(., crs = 4326)

#-------------------------------------------------------------------------------
# Plot definitions
#-------------------------------------------------------------------------------

Defs.with.cps.atm.survey.dat <- 
ggplot() +
  geom_hline(data = hlines, aes(yintercept = nstock), col = "blue", 
             linewidth = 0.7, linetype = "dashed") +
  geom_hline(data = hlines, aes(yintercept = point.concept), col = "red", 
             linewidth = 0.7, linetype = "longdash") +
  geom_hline(data = hlines, aes(yintercept = ymin), col = "black", 
             linewidth = 0.7) +
  geom_hline(data = hlines, aes(yintercept = ymax), col = "black", 
             linewidth = 0.7) +
  geom_sf(data = PacificCoast4.wgs84,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = PCoastStates,
          color = "#ae8f60", 
          size = 0.7, 
          fill = "#E1C699") +
  geom_sf(data = Populated10, col = "black") +
  geom_text(data = Populated10,
            aes(x = longitude, y = latitude, label = name),
            col = "black", 
            position = position_dodge(width = 1),
            vjust = 1) +
  scale_y_continuous(limits = c(27,55), 
                     breaks = lat_breaks, 
                     labels = lat_labels, 
                     expand = c(0.05, 0.05)) +
  labs(y = "Latitude") +
  geom_sf(data = PreAbs, col = "black")

Defs.with.cps.atm.survey.dat

ggsave2(filename=paste("Defs.with.cps.atm.survey.dat.jpeg",sep=''),
        plot=Defs.with.cps.atm.survey.dat, device="jpeg", 
        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
        unit="cm", limitsize = FALSE)

```

# Cluster hauls by date and sun event

## First find the corresponding sunrise and suntime times

This should be done by data, lat, and lon.

```{r, sunrise.table}

# Find the center of the polygon layers to use for joins
PreAbs2 <- PreAbs %>%
  mutate(
    lat = st_coordinates(st_centroid(.))[,2],
    lon = st_coordinates(st_centroid(.))[,1]
  )

# Use unique combinations from your data
suntimes <- PreAbs2 %>%
  transmute(
    lat = lat,
    lon = lon,
    date = date
  ) %>%
  distinct() %>%
  st_drop_geometry()

dim(suntimes)
# [1] 2672  3

# Plan parallel execution (adjust workers if needed)
plan(multisession)

# Apply get_sun_times function in parallel.
## Note that get_sun_times must in wgs84 
sun_times <- future_pmap_dfr(suntimes, get_sun_times)

# Check that tz is correct
head(sun_times$sunrise_pacific)
head(sun_times$sunset_pacific)

# ------------------------------------------------------------------------------
# Save (optional) : Note that the tz value is not stored and needs specified 
# manually each time you load data with a POSIXct structure.
# ------------------------------------------------------------------------------

# Where to save output:
## sun_times.dir <- here(getwd(), "Data", "Sun_times")

# Save as CSV:
## fwrite(sun_times, file = paste(sun_times.dir, "/sunrise_sunset_times.csv",
##                               sep=""), sep = ",")

```

## Join set set table with data for clusters

Note that you the code above returns an IDate column for date and needs converted to standard date.

```{r, join.sunrise.set.and.dat}

# Check reference system
st_crs(PreAbs2)

dim(PreAbs2)
# [1] 25488  38

# Check output
head(sun_times$sunrise_pacific)
head(sun_times$sunset_pacific)

dim(sun_times)
# [1] 2672    5

# Add sun_times to original data set
area.transects <- inner_join(PreAbs2, sun_times, 
                             join_by("date", "lat", "lon")) %>%
  st_transform(., crs = aea_custom)
# View(area.transects)  

dim(area.transects)
# [1] 25488  40

# Double check values
unique(is.na(area.transects))

```

## Create id for potential future work 

Determine whether the haul was taken during the day or night. 

```{r, join.sunrise.set.and.dat}

# Filter hauls that are taken before 
# sunset time that are after sunrise time
area.transects.d <- area.transects %>%
  group_by(date) %>%
  filter(equilibrium_time.pacific >= sunrise_pacific) %>%
  mutate(cluster.grp = "day Trawl") %>%
  ungroup()
  
# Filter hauls that are taken before 
# sunrise time that are after sunset time
area.transects.n <- area.transects %>%
  group_by(date) %>%
  filter(equilibrium_time.pacific < sunrise_pacific |
           equilibrium_time.pacific >= sunset_pacific) %>%
  mutate(cluster.grp = "night Trawl") %>%
  ungroup()
  
# Join spatial objects and convert DateTime values to character 
# string (ESIRI does not work with DateTime)
area.transects.final <- rbind(area.transects.d, area.transects.n) %>%
  mutate(haulback_time.utc = as.character(haulback_time.utc),
         haulback_time.pacific = as.character(haulback_time.pacific),
         equilibrium_time.utc = as.character(equilibrium_time.utc),
         equilibrium_time.pacific = as.character(equilibrium_time.pacific),
         sunrise_pacific = as.character(sunrise_pacific),
         sunset_pacific = as.character(sunset_pacific))

dim(area.transects.final)
# [1] 25488  41

#-------------------------------------------------------------------------------
# Remove unnecessary columns
#-------------------------------------------------------------------------------

area.transects.final <- area.transects.final %>%
  select(-c(beginTransect, endTransect, subsample_count, subsample_weight,
            itis_tsn, remaining_weight, Species_weight))

#-------------------------------------------------------------------------------
# Adjust names for clarity
#-------------------------------------------------------------------------------

names(area.transects.final) <- c("cruise", "ship", "haul", "collection",
                                "init_lat", "init_lon", "end_lat", "end_lon",
                                "sciname", "hbdattimUTC", "datetimeUTC", "weight", 
                                "specimen_i", "length_cm", "geometry", 
                                "hbdattimPC", "datetimePC", "age", "len_bin",
                                "WGHT_bin", "NBRE_bin", "NBRE_samp", "Total_W",
                                "WghtLenBin","Year","Month", "tim_stp", "keys",
                                "date", "latCntr", "lonCntr", "sunrisePC",
                                "sunsetPC", "clstGRP")      


write_sf(area.transects.final, 
         file.path(paste(dir, "/Data/Shapefiles/area.transects.pre-step.shp", 
                         sep = "")),
         overwrite = TRUE)

# View(area.transects.final)

```

# Read transect data

Note here we do not group hauls or investigate age or length bin (len_bin), so we can just work directly with the total weight argument. It is necessary however to remove the geometry feature aspects and apply the transect function to the data. This way we do not consider any spatial buffer extent.

```{r, read.dat}

no.clust.dat <- read_sf(
  file.path(paste(dir,
                  "/Data/Shapefiles/area.transects.pre-step.shp", 
                         sep = ""))) %>%
  st_drop_geometry()
# View(no.clust.dat)

# Rename columns to match function 
no.clust.dat <- no.clust.dat %>%
  mutate(start_latitude = init_lt,
         start_longitude = init_ln,
         stop_latitude = end_lat,
         stop_longitude = end_lon, 
         FID = row_number(.)) %>%
  select(-c(init_lt, init_lt, end_lat, end_lon))

# Since the data falls within two UTM zones and there is not an ideal projection to use, apply a custom equal area albers projection
aea_custom <- "+proj=aea +lat_1=30 +lat_2=50 +lat_0=40 +lon_0=-124.5 +datum=WGS84 +units=m +no_defs"

# Note data will be in wgs84
no.clust.sf <- make_transect_sf(no.clust.dat) %>%
  st_transform(., crs = aea_custom) 

# Define Season
sort(unique(no.clust.sf$Month))
# [1]  3  4  5  6  7  8  9 10 11

no.clust.sf <- no.clust.sf %>%
  mutate(Season = case_when(
    month(date) %in% c(3:5) ~ "Spring",
    month(date) %in% c(6:9) ~ "Summer",
    month(date) %in% c(10:11) ~ "Fall")
  )

```

# Create a summary table

This has to be done in multiple steps.

## Simplify temporal aspects

```{r, summary.table.time.step}

no.clust.sf.summary <- no.clust.sf  %>%
  # For the summary table, remove Year and Month initially
  select(-c(Year, Month)) %>%
  # separate out date elements (note the other way would be to rename a 
  # vectored list) 
  separate(date, into = c("Year", "Month", "Day")) %>%
  # Create a numeric index of Month Day combination to sort on
  mutate(MonthDay.num = as.numeric(paste(Month, Day, sep = ""))) %>%
  # Create abbreviated month value
  mutate(Month = month.abb[as.numeric(Month)],
         Year = as.numeric(Year)) %>%
  # Create Month day label
  mutate(MonthDay = paste(Month, ". ", Day, sep = "")) %>%
  distinct()

dim(no.clust.sf.summary) 
# [1] 19076  39
# View(no.clust.sf.summary)

#-------------------------------------------------------------------------------
# Summarize the corresponding months and days sampled per Year and Season
#-------------------------------------------------------------------------------

Hauls.N <- no.clust.sf.summary %>%
  # Group Year, Season and night or day trawls
  group_by(Year, Season) %>% 
  arrange(MonthDay.num, .by_group = TRUE) %>%
  mutate(MonthDay = paste(first(MonthDay), "-", last(MonthDay))) %>%
  ungroup() %>%
  # Remove unique identifiers
  select(-c(MonthDay.num, Month, Day, FID, hbdtUTC, hbdttPC, dttmUTC,
            dattmPC, clstGRP, sunrsPC, sunstPC)) %>%
  distinct()
# View(Hauls.N)

dim(Hauls.N) 
# [1] 19076  28

```

## Simplify spatial aspects

Summarize the corresponding latitudinal range per season and night or day sampling.

```{r, summary.table.sptl.dims}

Hauls.N2 <- Hauls.N %>%
  # For each haul, find min and max lat (this is not the center point)
  group_by(Year, Season, cruise, ship, haul, collctn) %>% 
  # For summary only, transform back to wgs84
  st_transform(., 4326) %>%
  mutate(
    #geometry = st_union(geometry),  # union to combine geometries per group
    # Extract the lowest latitude from the transect key
    minLAT = min(st_coordinates(geometry)[, "Y"]),
    # Extract the greatest latitude from the transect key
    maxLAT = max(st_coordinates(geometry)[, "Y"])
    ) %>%
  ungroup() %>%
  st_drop_geometry() %>%
  # For each Year Season, create summary statistics
  group_by(Year, Season) %>%
    mutate(
      Hauls = n_distinct(keys),
      Absent_hauls = sum(is.na(len_bin)),
      Latitudes = paste(format(round(min(minLAT), 2), nsmall = 2), # 2 digits after
                        " - ",
                        format(round(max(maxLAT), 2), nsmall = 2), sep = "")
      ) %>%
  ungroup() %>%
  # Remove individual measure observations
  select(-c(minLAT, maxLAT, keys, lngth_c, lonCntr, latCntr, sciname, start_latitude,
            start_longitude, stop_latitude, stop_longitude, init_ln)) %>%
  distinct()

dim(Hauls.N2)  
# [1] 19076  20
# View(Hauls.N2)

```

## summarize observations

If there are presence observations, find length range. Note that if more than one time.step value exists, it means more than one observation is present!

### Sardine presence data

```{r, summary.table.sum.obs.pt1}

#-------------------------------------------------------------------------------
# If Sardine are present and only one observation:
#-------------------------------------------------------------------------------

Hauls.N.p1 <- Hauls.N2 %>% 
  group_by(Year, Season) %>%
  filter(n() == 1 & !is.na(len_bin)) 

if (nrow(Hauls.N.p1) != 0){
  Hauls.N.p1 <- Hauls.N.p1 %>% 
    mutate(Lengths = paste(len_bin, " cm", sep = ""),
           n_bin = sum(NBRE_bn), 
           n_samp = sum(NBRE_sm),
           Total_W = sum(Total_W),
           Ages = paste(age, " yrs", sep = "")) %>%
    ungroup() %>%
    distinct()
}else{
  rm(Hauls.N.p1)
}

#-------------------------------------------------------------------------------
# If Sardine are present and more than one observation:  
#-------------------------------------------------------------------------------

Hauls.N.p2 <- Hauls.N2 %>% 
  group_by(Year, Season) %>%
  filter(n() > 1 & !is.na(len_bin)) %>%
      mutate(Lengths = paste(min(na.omit(len_bin)), " - ",
                         max(na.omit(len_bin)), 
                         " cm", sep = ""),
         Ages = if (n_distinct(age) > 1) {
           paste(min(na.omit(age)), " - ",
                 max(na.omit(age)), " yrs")
           }else{
             paste(age, " yrs")
             },
         n_bin = sum(NBRE_bn), 
         n_samp = sum(NBRE_sm),
         Total_W = sum(Total_W)) %>%
  ungroup() %>%
  distinct()

# Join presence data sets:  
Hauls.N.p <- 
  if (exists("Hauls.N.p1")){
    Hauls.N.p = rbind(Hauls.N.p1, Hauls.N.p2) %>% 
        # Remove individual measures
        select(-c(age, len_bin, WGHT_bn, WghtLnB, NBRE_bn, NBRE_sm, weight, specmn_)) %>%
        distinct()
    }else{
      Hauls.N.p = Hauls.N.p2 %>%
      # Remove individual measures
      select(-c(age, len_bin, WGHT_bn, WghtLnB, NBRE_bn, NBRE_sm, weight, specmn_)) %>%
      distinct()
      }
# View(Hauls.N.p)

```

### Sardine absence data

If there are no presence observations, write absent and put NA or 0.

```{r, summary.table.sum.obs.pt2}

Hauls.N.a <- Hauls.N2 %>%
  filter(is.na(len_bin) & 
           !Year %in% Hauls.N.p$Year & 
           !Season %in% Hauls.N.p$Season) %>%
  mutate(Lengths = NA,
         Ages = NA,
         n_bin = 0, 
         n_samp = 0,
         Total_W = 0) %>%
  # Remove individual measures
  select(-c(age, len_bin, WGHT_bn, WghtLnB, NBRE_bn, NBRE_sm, weight, specmn_)) %>%
  distinct()

```

### Join presence absence summary data

```{r, summary.table.sum.obs.pt3}

#-------------------------------------------------------------------------------  
# Merge presence absence data tables
#-------------------------------------------------------------------------------  

Hauls.N3 <- rbind(Hauls.N.p, Hauls.N.a) %>%
  distinct() %>%
  arrange(., collctn)

#-------------------------------------------------------------------------------  
# Create an index
#-------------------------------------------------------------------------------  

Hauls.N3 <- Hauls.N3 %>%
  select(-c(cruise, ship, haul, collctn, tim_stp)) %>%
  distinct()
# View(Hauls.N3)

Hauls.N3 <- Hauls.N3 %>%
  mutate(SCOD = case_when(
    Season == "Spring" ~ 1,
    Season == "Summer" ~ 2,
    Season == "Fall" ~ 3
  )) %>%
  arrange(., paste(Year,SCOD,sep="")) %>%
  select(-SCOD)

# Reorder table and simplify names
survey.dat <- Hauls.N3[,c("Year", "Season", "MonthDay", "Hauls",
                          "Absent_hauls", "Total_W", "Lengths", "Ages",
                          "n_samp","Latitudes")]


names(survey.dat) <- c("Year", "Season", "MonthDay", "Total Hauls", 
                       "Nb. Absent Hauls", "Total Catch (kg)", "Length Bins", 
                       "Est. Ages", "n sampled", "Latitudes") 
# View(survey.dat)

#-------------------------------------------------------------------------------  
# Save output
#-------------------------------------------------------------------------------  

fwrite(survey.dat, paste(dir,"/Results/summaryTable.csv", sep=""), sep=";")

```

# Clean data

```{r, clean.dat}

no.clust.sf.cln <- no.clust.sf %>%
  select(-c(weight, specmn_, lngth_c, age, len_bin, WGHT_bn, NBRE_bn, WghtLnB,
            latCntr, lonCntr, clstGRP, FID))
  
# Sum over individuals counted
no.clust.sf.cln <- no.clust.sf.cln %>%
  group_by(cruise, ship, haul, collctn, sciname, dattmPC, Total_W, Year, Month,
           tim_stp, keys, date, sunrsPC, sunstPC, start_latitude, start_longitude,
           stop_latitude, stop_longitude, geometry) %>%
  summarise(NBRE_sm = sum(NBRE_sm))

dim(no.clust.sf.cln)
# [1] 2672  20

dim(distinct(no.clust.sf.cln))
# [1] 2672  20

```

# Calculate distance between start and stop of haul

```{r, start.stop.dist}

no.clust.sf.cln2 <- no.clust.sf.cln %>%
# Define coordinate pairs
  group_by(keys) %>%
  mutate(
    start.coords = st_sfc(
      st_point(c(start_longitude, start_latitude)), 
      crs = 4326) %>%
      # Transform to custom CRS
      st_transform(crs = aea_custom),  
    end.coords = st_sfc(
      st_point(c(stop_longitude, stop_latitude)),
      crs = 4326) %>%
       # Transform to custom CRS
      st_transform(crs = aea_custom)  
  ) %>%
  ungroup()  

no.clust.sf.cln2 <- no.clust.sf.cln2 %>%
  group_by(keys) %>%
  # Calculate distance travelled
  mutate(dist.trvl = st_distance(start.coords, end.coords)) %>%
  ungroup() 

head(no.clust.sf.cln2$dist.trvl)

```

# Check for entries where start and stop are the same

```{r,start.stop.equal}

no.clust.sf.cln3 <- no.clust.sf.cln2 %>% 
  mutate(dist.trvl.m = as.numeric(dist.trvl)) %>%
  # Remove April 2007
  filter(dist.trvl.m != 0)  %>%
  select(-dist.trvl)

# Check if there is a consistent haul distance -1 year
test.2006 <- no.clust.sf.cln3 %>%
  filter(Year == 2006 & 
           substr(cruise, nchar(cruise)-1, nchar(cruise)) == "04")
range(test.2006$dist.trvl.m)
# [1] 1904.875 4330.351

# Check if there is a consistent haul distance +1 year
test.2008 <- no.clust.sf.cln3 %>%
  filter(Year == 2008 & 
           substr(cruise, nchar(cruise)-1, nchar(cruise)) == "04")
range(test.2008$dist.trvl.m)
# [1] 2323.137 8645.318

```

# Calculate sweep area

## Check ships present to help with literature search

```{r, ships.present}

# Find unique ships
unique(no.clust.sf.cln3$ship)
# [1] "FR" "JD" "OD" "LB" "MF" "SH" "OS" "RL"

# OD = Oscar Dyson
# JD = David Starr Jordan
# RL = Ruben Lasker
# SH = Bell M. Shimada
# FR = Frosti 
# MF = Miller Freeman
# OS = Ocean Star
# LB = contracted commercial vessel

```

## For 2003-2005

```{r, sweep.area.2004-2005}

no.clust.sf.2003.2005 <- no.clust.sf.cln3 %>%
  filter(Year %in% c(2003:2005)) %>%
  # Nordic264 rope, 30 min, 4 knots ~360m2 (20m * 18m)
  mutate(area.op.m2 = 20*18) %>%
  mutate(sweep.m3 = area.op.m2*dist.trvl.m) %>%
  distinct() %>%
  mutate(Density.kg.m3 = Total_W/sweep.m3,
         Density.g.m3 = Density.kg.m3*1000,
         Density.n.m3 = NBRE_sm/sweep.m3) %>%
  distinct()

# View(no.clust.sf.2003.2005)

write_sf(no.clust.sf.2003.2005, 
         file.path(paste(dir, "/Data/Shapefiles/no.clust.sf.2003.2005.shp", 
                         sep = "")),
         overwrite = TRUE)

View(no.clust.sf.2003.2005)

test.range <- no.clust.sf.2003.2005 %>%
  filter(Year != 2003 &
           !Month %in% c(10:11))

range(test.range$start_latitude)
# [1] 32.8480 48.0108

range(test.range$stop_latitude)
# [1] 32.8163 48.0670

```

## For 2006-2010

```{r, sweep.area.2006-2010}

no.clust.sf.2006.2010 <- no.clust.sf.cln3 %>%
  filter(Year %in% c(2006:2010)) %>%
  # Nordic264 rope, 30 min, 4 knots ~360m2 (20m * 18m)
  mutate(area.op.m2 = 24*30) %>%
  mutate(sweep.m3 = area.op.m2*dist.trvl.m) %>%
  distinct() %>%
  mutate(Density.kg.m3 = Total_W/sweep.m3,
         Density.g.m3 = Density.kg.m3*1000,
         Density.n.m3 = NBRE_sm/sweep.m3) %>%
  distinct()

# View(no.clust.sf.2006.2010)

write_sf(no.clust.sf.2006.2010, 
         file.path(paste(dir, "/Data/Shapefiles/no.clust.sf.2006.2010.shp", 
                         sep = "")),
         overwrite = TRUE)

test.range2 <- no.clust.sf.2006.2010 %>%
  filter(Year != 2007 &
           !Month %in% c(10:11))

range(test.range2$start_latitude)
# [1] 30.7001 50.9680

range(test.range2$stop_latitude)
# [1] 30.6663 50.9803

```

## For 2011-2014

```{r, sweep.area.2011-2014}

no.clust.sf.2011.2014 <- no.clust.sf.cln3 %>%
  filter(Year %in% c(2011:2014)) %>%
  # Nordic264 rope, 30min, 4 knots ~600m2 (not specified)
  mutate(area.op.m2 = 600) %>%
  mutate(sweep.m3 = area.op.m2*dist.trvl.m) %>%
  distinct() %>%
  mutate(Density.kg.m3 = Total_W/sweep.m3,
         Density.g.m3 = Density.kg.m3*1000,
         Density.n.m3 = NBRE_sm/sweep.m3) %>%
  distinct()

# View(no.clust.sf.2011.2014)

write_sf(no.clust.sf.2011.2014, 
         file.path(paste(dir, "/Data/Shapefiles/no.clust.sf.2011.2014.shp", 
                         sep = "")),
         overwrite = TRUE)

test.range3 <- no.clust.sf.2011.2014 %>%
  filter(!Month %in% c(10:11))

range(test.range3$start_latitude)
# [1] 31.3373 51.1141

range(test.range3$stop_latitude)
# [1] 31.3385 51.0923

```

## For 2015-2020

```{r, sweep.area.2015-2020}

no.clust.sf.2015.2020 <- no.clust.sf.cln3 %>%
  filter(Year %in% c(2015:2020)) %>%
  # Nordic264 rope, 30-45min, 3.5-5 knots ~300m2 (15m * 20m) 
  mutate(area.op.m2 = 15*20) %>%
  mutate(sweep.m3 = area.op.m2*dist.trvl.m) %>%
  distinct() %>%
  mutate(Density.kg.m3 = Total_W/sweep.m3,
         Density.g.m3 = Density.kg.m3*1000,
         Density.n.m3 = NBRE_sm/sweep.m3) %>%
  distinct()

# View(no.clust.sf.2015.2020)

write_sf(no.clust.sf.2015.2020, 
         file.path(paste(dir, "/Data/Shapefiles/no.clust.sf.2015.2020.shp", 
                         sep = "")),
         overwrite = TRUE)

test.range4 <- no.clust.sf.2015.2020 %>%
  filter(!Month %in% c(10:11))

range(test.range4$start_latitude)
# [1] 32.1748 54.3997

range(test.range4$stop_latitude)
# [1] 32.1355 54.4157

```

## For 2021-2024

```{r, sweep.area.2021-2024}

no.clust.sf.2021.2024 <- no.clust.sf.cln3 %>%
  filter(Year %in% c(2021:2024)) %>%
  # Nordic264 rope, 45min, 3.5-4.5 knots ~300m2 (15m * 20m)
  mutate(area.op.m2 = 15*20) %>%
  mutate(sweep.m3 = area.op.m2*dist.trvl.m) %>%
  distinct() %>%
  mutate(Density.kg.m3 = Total_W/sweep.m3,
         Density.g.m3 = Density.kg.m3*1000,
         Density.n.m3 = NBRE_sm/sweep.m3) %>%
  distinct()

# View(no.clust.sf.2021.2024)

write_sf(no.clust.sf.2021.2024, 
         file.path(paste(dir, "/Data/Shapefiles/no.clust.sf.2021.2024.shp", 
                         sep = "")),
         overwrite = TRUE)

test.range5 <- no.clust.sf.2021.2024 %>%
  filter(!Month %in% c(10:11))

range(test.range5$start_latitude)
# [1] 28.6513 49.7967

range(test.range5$stop_latitude)
# [1] 28.6548 49.8043

```
