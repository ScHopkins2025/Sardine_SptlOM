---
title: "BuildSdmTMB"
author: "Stephanie Hopkins"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    
---

```{r setup, include=FALSE}

# Run first if not already installed
#remotes::install_github("pbs-assess/sdmTMBextra", dependencies = TRUE)

rm(list=ls())
gc()

load.lib <- c("tidyverse", "data.table", "cowplot", "sf", "rcompanion",
              "pROC", "here", "sdmTMB", "sdmTMBextra", "visreg",
              "ggforce", # enables plot_anisotropy
              "spatstat",# Get summary table for nearest neighbor
              "rsample", "timetk", "zoo") 

install.lib <- load.lib[!load.lib %in% installed.packages()]

for(lib in install.lib) install.packages(lib,dependencies=TRUE)
sapply(load.lib,require,character=TRUE)

sf_use_s2(FALSE)

dir <- here(getwd())

```

# Load previous sdm data

Build new adult sardine SDM using environmental predictors from the 3km WC15 ROMS.
Vars are SST, surface chl-a, upper 50m zooplankton biomass, SSB. 
Chl-a didn't improve AUC, dropped for now.

```{r, load.previous.dat}

# Where previous data is stored
sdmDir <- here(dir, "Data", "Previous.sdmTMB.Data")

# Load data
dat <- readRDS(paste(sdmDir, "/sardineTrainingData.rds", sep = ""))

# Project data
dat2 <- dat %>%
  filter(yr > 2002 & survey == 'cps') %>% 
  distinct() %>%
  group_by(yr) %>%
  mutate(hauls = n_distinct(haul)) %>%
  ungroup() %>% 
  select(yr, cruise, haul, lat, hauls) %>% 
  distinct()
# View(dat2)

test2003 <- dat %>% filter(yr == 2003 & survey == 'cps')

length(unique(test2003$haul))
# [1] 41
# View(test2003)

```

# Load new sdm data

There will likely be some hauls not previously accounted, but the number of hauls should be similar. Use 2003 as a test run.

```{r, load.new.dat}

areaTranBuffsum.polygons <- 
   read_sf(here(dir, "Data", "Shapefiles", "cluster.with.area.defs.shp"))
#View(areaTranBuff.polygons)

unique(areaTranBuffsum.polygons$Area_id)
# [1] 2 3 4 5 1

sort(unique(areaTranBuffsum.polygons$Area_id))
# [1] 1 2 3 4 5

areaTranBuff.polygons.2003 <- areaTranBuffsum.polygons %>%
  filter(Year == 2003)

colnames(areaTranBuffsum.polygons)

```

# Create density estimates to be used for biomass abundance indices

Note that here we want to include the absence data and are not concerned by lengths

```{r, biomass.density.area2}

area2poly.dens <- areaTranBuffsum.polygons %>%
  mutate(Cruise = substr(keys,1,6)) %>%
  group_by(Cruise, Year, Month, Day, tim_stp, clstGRP, Hauls, NbClust, Area_id,
           Area_km2, geometry, age, len_bin) %>%
  # Note that some clusters will have both presence and absence data
  # but this will give you the proportion to apply in stock synthesis for length
  # and age
  summarise(WghtLenBin = sum(WghtLenBin), .groups = "drop") %>%
  mutate(
    # Calculate density
    Density = WghtLenBin / Area_km2,

    # Create proper date object
    Date = as.Date(paste(Year, Month, Day, sep = "-"), format = "%Y-%m-%d"),
    
    # Convert to numeric
    Year = as.numeric(Year),
    Month = as.numeric(Month),
    Day = as.numeric(Day),

    # Create YearQuarter
    YearQuarter = as.yearqtr(Date),

    # Extract numeric quarter
    Quarter = case_when(
      str_detect(YearQuarter, "Q1") ~ 1,
      str_detect(YearQuarter, "Q2") ~ 2,
      str_detect(YearQuarter, "Q3") ~ 3,
      str_detect(YearQuarter, "Q4") ~ 4
    ),

    # Create numeric time step index from baseline year (e.g., 2003)
    NumericQuarter = (Year - min(Year)) * 4 + Quarter,

  ) %>%
  select(-tim_stp)

# View(area2poly.dens)

plotNormalHistogram(area2poly.dens$Density)
range(area2poly.dens$Density)
# [1] 0.000 912149.7

```

# Extract polygon centers

```{r, area2.mesh.prestep}

# Extract polygon centers
area2poly.dens2 <- area2poly.dens %>%
  mutate(X = st_coordinates(st_centroid(.))[,1]/1000,
         Y = st_coordinates(st_centroid(.))[,2]/1000) %>%
  st_drop_geometry()

```

# Restructure data to include P/A

First find mixed clusters where both presence and absence values exist. Filter out clusters that have only absence values.

```{r, presence.absence.col.pt1}

# Find steps where there are absence values within a cluster
Absent.dat <- area2poly.dens2 %>%
  filter(is.na(age)) %>%
  select(-c(age,len_bin)) %>%
  # Add key to identify clusters
  mutate(vals = paste0(Cruise, Date, clstGRP, Area_id, Area_km2, X, Y))

dim(Absent.dat)
# [1] 1282   18

# Find steps where there are presence values within a cluster
Present.dat <- area2poly.dens2 %>%
  filter(!is.na(age)) %>%
  # Add presence binary value
  mutate(PA = 1) %>%
  # Add key to identify clusters
  mutate(vals = paste0(Cruise, Date, clstGRP, Area_id, Area_km2, X, Y))

# Clusters with both presence and absence should be given a 1 indicating presence, 
# and are but are accounted for in the density measure. 
Absent.dat.mxd <- Absent.dat %>%
  filter(vals %in% Present.dat$vals) %>%
  # Add presence binary value
  mutate(PA = 1)

# Find steps where the entire clusters has no Sardines
# These should be given a 0 binary value indicating total absence.
Absent.dat.all <- Absent.dat %>%
  filter(!vals %in% Present.dat$vals) %>%
  # Add absence binary value
  mutate(PA = 0)

```

## For each bin in the presence data, split by age length bin

Combine survey clusters with total absence and bin data so that for each cruise, there total absence clusters are appended.

```{r, presence.absence.col.pt2}

# Check that all Cruise values in Absent.dat.all are present in Present.dat
unique(Absent.dat.all$Cruise) %in% unique(Present.dat$Cruise)
#  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
# [14]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
# [27]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE

#-------------------------------------------------------------------------------
# Store cruises where there are no shared Cruises
#-------------------------------------------------------------------------------

Absent.matched.cruise <- Absent.dat.all %>%
  filter(Cruise %in% Present.dat$Cruise)

dim(Absent.matched.cruise)
# [1] 1020   19   

## Check if there is at least a shared Date
Absent.matched.time <- Absent.matched.cruise %>%
  filter(Date %in% Present.dat$Date)

dim(Absent.matched.time)
# [1] 181  19

## Check if there is at least a shared Year and Month
Absent.matched.yrmon <- Absent.matched.cruise %>%
  filter(!Date %in% Present.dat$Date &
           Year %in% Present.dat$Year &
           Month %in% Present.dat$Month)

dim(Absent.matched.yrmon)
# [1] 839  19

#-------------------------------------------------------------------------------
# Store cruises where there are no shared Cruises
#-------------------------------------------------------------------------------

Absent.ToTunique <- Absent.dat.all %>%
  filter(!Cruise %in% Present.dat$Cruise)

dim(Absent.ToTunique)
# [1] 44 19

## Check if there is at least a shared Date
Absent.ToTunique.date <- Absent.ToTunique %>%
  filter(Date %in% Present.dat$Date)

dim(Absent.ToTunique.date)
# [1] 24 19

## Check if there is at least a shared Year and Month
Absent.ToTunique.yrmon <- Absent.ToTunique %>%
  filter(!Date %in% Present.dat$Date &
           Year %in% Present.dat$Year &
           Month %in% Present.dat$Month)

dim(Absent.ToTunique.yrmon)
# [1] 20 19

```

## For shared Cruise and Date values, bind rows

```{r, presence.absence.col.pt3.1}

#-------------------------------------------------------------------------------
# For when a shared date exists
#-------------------------------------------------------------------------------

# Define keys
join_keys <- c("Cruise", "Date")

cruise.date.dat <- Present.dat %>%
  group_by(age, len_bin) %>%
  group_split() %>%
  map_df(~ {
    present_grp <- .x

    # Get rows from Absent.dat.all that match on Cruise
    absent_matches <- Absent.matched.time %>%
      filter(Cruise %in% Present.dat$Cruise & 
               Date %in% Present.dat$Date) %>%
      semi_join(present_grp, by = join_keys) %>%
      mutate(age = unique(present_grp$age),
         len_bin = unique(present_grp$len_bin))

    # Append matched Absent rows to this group
    bind_rows(present_grp, absent_matches)
  })

```

## For shared Cruise and Year Month values, bind rows

```{r, presence.absence.col.pt3.2}

# Define keys
join_keys <- c("Cruise", "Year", "Month")

cruise.yrmon.dat <- Present.dat %>%
  group_by(age, len_bin) %>%
  group_split() %>%
  map_df(~ {
    present_grp <- .x

    # Get rows from Absent.dat.all that match on Cruise
    absent_matches <- Absent.matched.yrmon %>%
      filter(Cruise %in% Present.dat$Cruise &
               !Date %in% Present.dat$Date &
               Year %in% Present.dat$Year &
               Month %in% Present.dat$Month) %>%
      semi_join(present_grp, by = join_keys) %>%
      mutate(age = unique(present_grp$age),
         len_bin = unique(present_grp$len_bin))

    # Append matched Absent rows to this group
    bind_rows(present_grp, absent_matches)
  })

```

## For non-shared Cruise data and Date only, bind rows

```{r, presence.absence.col.pt3.3}

# Define keys
join_keys <- c("Date")

date.dat.only <- Present.dat %>%
  group_by(age, len_bin) %>%
  group_split() %>%
  map_df(~ {
    present_grp <- .x

    # Get rows from Absent.dat.all that match on Cruise
    absent_matches <- Absent.ToTunique.date %>%
      filter(!Cruise %in% Present.dat$Cruise &
               Date %in% Present.dat$Date) %>%
      semi_join(present_grp, by = join_keys) %>%
      mutate(age = unique(present_grp$age),
         len_bin = unique(present_grp$len_bin))

    # Append matched Absent rows to this group
    bind_rows(present_grp, absent_matches)
  })

```

## For non-shared Cruise data and Year Month only, bind rows

```{r, presence.absence.col.pt3.4}

# Define keys
join_keys <- c("Year", "Month")

yrmon.dat.only <- Present.dat %>%
  group_by(age, len_bin) %>%
  group_split() %>%
  map_df(~ {
    present_grp <- .x

    # Get rows from Absent.dat.all that match on Cruise
    absent_matches <- Absent.ToTunique.yrmon %>%
      filter(!Cruise %in% Present.dat$Cruise &
               !Date %in% Present.dat$Date &
               Year %in% Present.dat$Year &
               Month %in% Present.dat$Month) %>%
      semi_join(present_grp, by = join_keys) %>%
      mutate(age = unique(present_grp$age),
         len_bin = unique(present_grp$len_bin))

    # Append matched Absent rows to this group
    bind_rows(present_grp, absent_matches)
  })

```

## Join back everything

```{r, join.full.data.presense.absence}

Full.dat <- rbind(cruise.date.dat, cruise.yrmon.dat, date.dat.only, yrmon.dat.only) %>%
  distinct() %>%
  # Reorder chronologically
  arrange(NumericQuarter)  %>%
  mutate(YearQuarter.dup = as.numeric(NumericQuarter))

summary(Full.dat)

unique(Full.dat$PA)

```

## Find missing time steps

```{r, missing.quarters}

# Generate complete sequence
full_seq <- data.frame(
  YearQuarter = seq(min(Full.dat$YearQuarter), 
                    max(Full.dat$YearQuarter), 
                    by = 0.25)
  )

missing_qtrs <- full_seq %>%
  anti_join(Full.dat, by = "YearQuarter") %>%
  mutate(Year = as.numeric(substr(YearQuarter, 1, 4)),
         Quarter = 
           case_when(
             str_detect(YearQuarter, "Q1") ~ 1,
             str_detect(YearQuarter, "Q2") ~ 2,
             str_detect(YearQuarter, "Q3") ~ 3,
             str_detect(YearQuarter, "Q4") ~ 4
             )
    ) %>%
  mutate(NumericQuarter = (Year - min(Year)) * 4 + Quarter) %>%
  select(NumericQuarter)

```

# Test/train split

Split 28 quarters (16 years) for training and 10 quarters (5 years) for out-of-model testing. For the time_series_split function, you need to have a datetime object. 

```{r, split.data}

# Define all levels for Year Quarter, combining training data and extra_time (numeric)
all_levels <- sort(unique(c(Full.dat$YearQuarter.dup, missing_qtrs$NumericQuarter)))

tims <- data.frame("YearQuarter" = sort(unique(Full.dat$YearQuarter))) %>%
  mutate(
    YearQuarter = as.yearqtr(YearQuarter, format = "%Y Q%q")
  ) %>%
  arrange(YearQuarter)  # ensures sorted order

# Single split: 75% train, last 3 time points as test
split <- timetk::time_series_split(
  tims,
  date_var = YearQuarter,
  initial = floor(0.75 * nrow(tims)),
  assess = ceiling(0.25 * nrow(tims))
)

# Create training set
train_dates <- rsample::training(split)
train <- subset(Full.dat, YearQuarter %in% train_dates$YearQuarter)
tail(train$YearQuarter)

fwrite(train, here(dir, "Data", "ExampleRun", "train"))

# Apply all levels to the data
# train <- train %>%
#  mutate(YearQuarter.dup = factor(YearQuarter.dup, levels = all_levels))

# Create test set
test_dates  <- rsample::testing(split)
test <- subset(Full.dat, YearQuarter %in% test_dates$YearQuarter) 
head(test$YearQuarter)

fwrite(test, here(dir, "Data", "ExampleRun", "test"))
# Apply all levels to the data
# test <- test %>%
#  mutate(YearQuarter.dup = factor(YearQuarter.dup, levels = all_levels))

# Define extra time steps to estimate
est.missing.time <- unique(c(missing_qtrs$NumericQuarter, test$NumericQuarter)) %>%
  sort

save(est.missing.time, file = here(dir, "Data", "ExampleRun",
                                    "est.missing.time.rda"))

```

# Next construct the mesh: 

see https://pbs-assess.github.io/sdmTMB/articles/basic-intro.html: cutoff is in the units of X and Y (km here), represents minimum distance between knots before a new mesh vertex is added). Muhling et al., 2025 tried a bunch of cutoff values, and chose one that 1) had good out-of-sample predictability, and 2) wasn't too "blocky". They found Values between ~ 50 and 200 were fairly reasonable. 

Here we look at the distance between clusters to assign knots using the summary function.

Important: cannot use anisotropy with barrier mesh (see warning when run sdmTMB).

#```{r, construct.mesh}
# Read full resolution - Continental land masses and ocean islands, except Antarctica.
land.barrier <- read_sf(here(dir, "Data", "Shapefiles", "gshhg-shp-2.3.7",
                             "GSHHS_shp", "f", "GSHHS_f_L1.shp")) %>%
  # Project to same coordinate system
  st_transform(crs = st_crs(area2poly.dens)) %>%
  # Trim to area extent
  st_crop(st_bbox(area2poly.dens))

plot(land.barrier)

# Filter geometry type
land.barrier <- land.barrier[st_geometry_type(land.barrier) %in%
                               c("POLYGON", "MULTIPOLYGON"), ] %>%
  st_union()

# Wrap back into sf object
land.barrier <- st_sf(geometry = land.barrier)

# Check that union worked
st_is_valid(land.barrier)
# [1] TRUE

# Drop levels
land.barrier <- land.barrier %>% mutate(across(where(is.factor), droplevels))

st_write(land.barrier, here(dir, "Data", "Shapefiles", "land.barrier.shp"))

#```


```{r, construct.mesh}

land.barrier <- read_sf(here(dir, "Data", "Shapefiles", "land.barrier.shp"))

# Get summary table for nearest neighbour
#coords <- cbind(train$X, train$Y)
#nn_dist <- nndist(coords)
#summary(nn_dist)
#   Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# 0.4631   4.8171   8.6841  11.2030  13.9212 163.6689 

# Set cut-toff to 1 + 2/3 distance (mean or median - choose median if strong outliers)
## This is informed by the SPDE theory about mesh resolution relative to
## spatial range (check Lindgren et al. 2011 and related papers)
#cutoff <- round(median(nn_dist) + median(nn_dist)*(2/3))
#cutoff

# Cutoff values control the minimum distance between knots.
## It is generally better to start with a coarser mesh (larger cutoff)
## However there is a tradeoff on spatial predictability (more knots) and
## over fitting the time to process. If the day is irregularly distributed
## you can also try residual-based knot placement
meshTrain <- make_mesh(train, xy_cols = c("X", "Y"), cutoff = 50) 

# Check number of mesh nodes 
## If greater than >1,000–2,000 nodes, 
## you’re likely in trouble unless you have a lot of RAM.
length(meshTrain$mesh$loc[,1])
# [1] 229

# proj_scaling should match units of (since we are working in m, but density
# is in km, divide by 1000)
barrier <- add_barrier_mesh(meshTrain, land.barrier, proj_scaling = 1000)

## If greater than >1,000–2,000 nodes, 
## you’re likely in trouble unless you have a lot of RAM.
length(barrier$mesh$loc[,1])
# [1] 229

# Example plot code from ?add_barrier_mesh
mesh_df_water <- barrier$mesh_sf[barrier$normal_triangles, ]
mesh_df_land <- barrier$mesh_sf[barrier$barrier_triangles, ]

Mesh.with.barrier <-
ggplot() +
  geom_sf(data = land.barrier) +
  geom_sf(data = mesh_df_water, size = 1, colour = "blue") +
  geom_sf(data = mesh_df_land, size = 1, colour = "green")

Mesh.with.barrier
# [1] 231

#ggsave2(filename=paste("Mesh.with.barrier.jpeg",sep=''),
#        plot=Mesh.with.barrier, device="jpeg", 
#        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
#        unit="cm", limitsize = FALSE)

plot(barrier)
points(train$X, train$Y, col = "red", pch = 19, 
       cex = 0.2)

```

# Regroup Quarts as 2 Seasons

There is moderate to strong temporal autocorrelation in the model as the modelling mean absolute error is significantly reduced by inclusion of temporal effects. However the spatial effects are not be well captured.

```{r, 2season.structure}

TwoSeas.dat <- Full.dat %>%
  mutate(
    Season = case_when(
      month(Date) %in% 1:6 ~ "Season 1 (Jan-Jun)",
      month(Date) %in% 7:12 ~ "Season 2 (July-Dec)"
      ),
    # Create combined time step variable using year and season
    YearSeas = paste(year(Date), Season, sep = "_"),
     Season.nm =  
      case_when(
      grepl("Season 1", Season) ~ 1,
      grepl("Season 2", Season) ~ 2
      ),
    # Combine year and numeric season
    YearSeas.nm = (Year + (Season.nm - 1) / 10),
    YearSeas.dup = as.numeric(YearSeas.nm))
# View(TwoSeas.dat)  

unique(TwoSeas.dat$YearSeas)
unique(TwoSeas.dat$YearSeas.nm)

```

# Find missing seasons

```{r, missing.seasons}

# Generate complete sequence
# Define the range of years
years <- 2003:2024

# Create the sequence of year-season combinations
year_season_sequence <- data.frame(
  YearSeas.nm = unlist(lapply(years, function(year) {
  # For each year, create two entries: season 1 (0.0) and season 2 (0.1)
  ## year + 0/10 gives 2003.0,while year + 1/10 gives 2003.1
  c(year + 0 / 10, year + 1 / 10)  
}))
)

# Print the resulting sequence
year_season_sequence

missing_seas <- year_season_sequence %>%
  anti_join(TwoSeas.dat, by = "YearSeas.nm") %>%
  select(YearSeas.nm)

missing_seas

```

# Test/train split with Season

Split 28 quarters (16 years) for training and 10 quarters (5 years) for out-of-model testing. For the time_series_split function, you need to have a datetime object. 

```{r, split.data2}

# Define all levels for Year Quarter, combining training data and extra_time (numeric)
all_levels2 <- sort(unique(c(TwoSeas.dat$YearSeas.dup, missing_seas$YearSeas.nm)))

tims2 <- data.frame("YearSeas" = sort(unique(TwoSeas.dat$YearSeas))) %>%
  arrange(YearSeas)  # ensures sorted order

# Single split: 75% train, last 3 time points as test
split <- timetk::time_series_split(
  tims2,
  date_var = YearSeas,
  initial = floor(0.75 * nrow(tims2)),
  assess = ceiling(0.25 * nrow(tims2))
)

# Create training set
train_dates2 <- rsample::training(split)
train2 <- subset(TwoSeas.dat, YearSeas %in% train_dates2$YearSeas)
tail(train2$YearSeas)

fwrite(train2, here(dir, "Data", "ExampleRun", "train2"))

# Apply all levels to the data
# train2 <- train2 %>%
#  mutate(YearSeas.dup = factor(YearSeas.dup, levels = all_levels2))

# Create test set
test_dates2  <- rsample::testing(split)
test2 <- subset(TwoSeas.dat, YearSeas %in% test_dates2$YearSeas) 
head(test2$YearSeas)

fwrite(test2, here(dir, "Data", "ExampleRun", "test2"))

# Apply all levels to the data
# test2 <- test2 %>%
#  mutate(YearSeas.dup = factor(YearSeas.dup, levels = all_levels2))

# Define extra time steps to estimate
est.missing.time2 <- unique(c(missing_seas$YearSeas.nm, test2$YearSeas.nm)) %>%
  sort

save(est.missing.time2, file = here(dir, "Data", "ExampleRun",
                                    "est.missing.time2.rda"))

```

# Next construct the seasonal mesh: 

This is the same procedure as above.

```{r, construct.seasonal.mesh}

# Get summary table for nearest neighbor
#coords2 <- cbind(train2$X, train2$Y)
#nn_dist2 <- nndist(coords2)
#summary(nn_dist2)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.0000  0.0000  0.0000  0.1143  0.0000 44.3049 

# Set cut-toff to 1 + 2/3 distance (mean or median - choose median if strong outliers)
## This is informed by the SPDE theory about mesh resolution relative to
## spatial range (check Lindgren et al. 2011 and related papers)
#cutoff2 <- round(median(nn_dist2) + median(nn_dist2)*(2/3))
#cutoff2

# Cutoff values control the minimum distance between knots.
## It is generally better to start with a coarser mesh (larger cutoff)
## However there is a tradeoff on spatial predictability (more knots) and
## over fitting the time to process. If the day is irregularly distributed
## you can also try residual-based knot placement
meshTrain2 <- make_mesh(train2, xy_cols = c("X", "Y"), cutoff = 50) 

# Check number of mesh nodes 
## If greater than >1,000–2,000 nodes, 
## you’re likely in trouble unless you have a lot of RAM.
length(meshTrain2$mesh$loc[,1])
# [1] 212

# proj_scaling should match units of (since we are working in m, but density
# is in km, divide by 1000)
barrier2 <- add_barrier_mesh(meshTrain2, land.barrier, proj_scaling = 1000)

## If greater than >1,000–2,000 nodes, 
## you’re likely in trouble unless you have a lot of RAM.
length(barrier2$mesh$loc[,1])
# [1] 212

# Example plot code from ?add_barrier_mesh
mesh_df_water2 <- barrier2$mesh_sf[barrier2$normal_triangles, ]
mesh_df_land2 <- barrier2$mesh_sf[barrier2$barrier_triangles, ]

Mesh.with.barrier2 <-
ggplot() +
  geom_sf(data = land.barrier) +
  geom_sf(data = mesh_df_water2, size = 1, colour = "blue") +
  geom_sf(data = mesh_df_land2, size = 1, colour = "green")

Mesh.with.barrier2
# [1] 231

#ggsave2(filename=paste("Seasonal.Mesh.with.barrier.jpeg",sep=''),
#        plot=Mesh.with.barrier2, device="jpeg", 
#        path=paste(dir, "/Figures",sep=""), dpi=1200, width = 29, height=21, 
#        unit="cm", limitsize = FALSE)

plot(barrier2)
points(train2$X, train2$Y, col = "red", pch = 19, 
       cex = 0.2)

# Define extra time steps to estimate
est.missing.time2 <- unique(c(missing_seas$YearSeas.nm, test2$YearSeas.nm)) %>%
  sort



```

## For a "Non-spatial" model without covars

This is a test run to see if we can explain the distribution by discrete time and OM spatial area 

```{r, make.base.nonsptl.model.w.fix.tim}

set.seed(1) 

## Add age and length fixed effects without temporal or spatial autocorrelation
## Note that this is testing presence absence and then density predicted by age
## and length bin
fit8 <- sdmTMB(Density ~ as.factor(age) + as.factor(len_bin) +
                 factor(YearQuarter.dup),
               data = train, 
               mesh = barrier,
               # Fit model by year and month
               time = "NumericQuarter",
               # Use "ar1" or "rw" if you expect temporal autocorrelation
               # Otherwise, use default "iid" (independent per time step)
               spatiotemporal = "off",
               spatial = "off", 
               extra_time = as.factor(est.missing.time),
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE,
               silent = FALSE)

# Check model skill, fit, and convergence
sanity(fit8)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
# ✔ No sigma parameters are > 100

summary(fit8)
#### Dispersion Parameter: 0.27 (Good, typical range: 0.5-1.5)
#### ML Criterion: 7987.858 (Best used for model comparison, not absolute assessment)

# Predict (gives a whole new df, unlike GAM)
pTrain8 <- predict(fit8, newdata = train, type = "response")

pTest8 <- predict(fit8, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain8))$r.squared # 0.005012378
summary(lm(Density ~ est, data = pTest8))$r.squared # 1.401812e-05ex

# MAE
mean(abs(pTrain8$Density - pTrain8$est)) # 413.7007
mean(abs(pTest8$Density - pTest8$est)) # 396.4788

mean(train$Density)
# [1] 209.332
sd(train$Density)
# [1] 11090.42

mean(test$Density)
# [1] 1.094451                                                                  
sd(test$Density)
# [1] 28.57991

```

## Try with temporal autocorrelation

```{r, make.nonsptl.model.w.fix.tim}

set.seed(1) 

## Add age and le ngth fixed effects with temporal autocorrelation
## Note that this is testing presence absence and then density predicted by age
## and length bin
fit9 <- sdmTMB(Density ~ as.factor(age) + as.factor(len_bin) +
                 as.factor(YeaSeas.dup),
               data = train, 
               mesh = barrier,
               # Fit model by year and month
               ## Note if time is specified, spatiotemporal correlation is tested
               ## by default
               time = "NumericQuarter",
               # Use "ar1" or "rw" if you expect temporal autocorrelation
               # Otherwise, use default "iid" (independent by time step)
               spatiotemporal = "ar1",
               spatial = "off", 
               extra_time = est.missing.time,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE,
               # Show progress
               silent = FALSE)

# Check model skill, fit, and convergence
sanity(fit9)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
##------------------------------------------------------------------------------
## ✖ `ln_phi` gradient > 0.001
## ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
##
#### `ln_phi` (often represented as the dispersion parameter) controls the variance of #### the error term in many models, especially in models like Generalized Linear Models #### (GLMs) or spatiotemporal models (e.g., sdmTMB).
##
#### ln_phi` gradient > 0.001 indicates that the log-likelihood with respect to the 
#### ln_phi parameter is still changing significantly at the point where the model 
#### converged.
##------------------------------------------------------------------------------
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
##------------------------------------------------------------------------------
## ✖ `sigma_E` is larger than 100
## ℹ Consider simplifying the model or adding priors
##
#### Large values of sigma_E indicate that there is a high level of spatial 
#### variation that your model has not explained. 
##------------------------------------------------------------------------------
# ✔ Range parameters don't look unreasonably large

summary(fit9)
## M1:
#### Spatiotemporal AR1 correlation (rho): 0.10
#### Matérn range: 75.12
#### Spatiotemporal marginal AR1 SD: 615.42
## M2:
#### Dispersion Parameter: 0.45 (Good, typical range: 0.5-1.5)
#### Spatiotemporal AR1 Correlation (rho): 0.67 (Moderate to strong, 
## typical range: 0.4 - 0.8)
#### Matérn Range: 122.28 (Good, typical range: 50-200 depending on scale)
#### Spatiotemporal Marginal AR1 SD: 48.57 (Good, typical range: 10-50)
#### ML Criterion: 4509.807 (Best used for model comparison, not absolute assessment)

# Predict (gives a whole new df, unlike GAM)
pTrain9 <- predict(fit9, newdata = train, type = "response")

pTest9 <- predict(fit9, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain9))$r.squared # 0.1437345
summary(lm(Density ~ est, data = pTest9))$r.squared # 8.934803e-05

# MAE
mean(abs(pTrain9$Density - pTrain9$est)) # 280.5764
mean(abs(pTest9$Density - pTest9$est)) # 1.094452

mean(train$Density)
# [1] 209.332
sd(train$Density)
# [1] 11090.42

mean(test$Density)
# [1] 1.094451                                                                  
sd(test$Density)
# [1] 28.57991

```

## Try with spatial effects

```{r, make.sptl.model.w.fix.tim}

set.seed(1) 

## Add age and length fixed effects with temporal and spatial autocorrelation
## Note that this is testing presence absence and then density predicted by age
## and length bin so we want the intercept
fit10 <- sdmTMB(Density ~ as.factor(age) + as.factor(len_bin) +
                  as.factor(YearSeas.dup),
               data = train, 
               mesh = barrier,
               # Fit model by year and month
               ## Note if time is specified, spatiotemporal correlation is tested
               ## by default
               time = "NumericQuarter",
               # Use "ar1" or "rw" if you expect temporal autocorrelation
               # Otherwise, use default "iid" (independent by time step)
               spatiotemporal = "ar1",
               spatial = "on", 
               extra_time = est.missing.time,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE,
               # Show progress
               silent = FALSE)

# Check model skill, fit, and convergence
sanity(fit10)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
##------------------------------------------------------------------------------
## ✖ `ln_tau_O` standard error may be large
## ℹ `ln_tau_O` is an internal parameter affecting `sigma_O`
## ℹ `sigma_O` is the spatial standard deviation
## ℹ Try simplifying the model, adjusting the mesh, or adding priors
##
#### tau_O: This parameter often represents the precision (the inverse of the variance) #### of a random effect component in the model. In spatial and spatiotemporal models, #### it typically refers to the precision of the spatial or temporal random effects.
##------------------------------------------------------------------------------
## ✖ `sigma_O` is smaller than 0.01
## ℹ Consider omitting this part of the model
##
#### In many spatial models, sigma_O could represent the spatial variance (or spatial #### range depending on the model), which determines the magnitude of spatial random #### effects or the degree of spatial correlation between observations.
##
#### could be:
#### 1) Overfitting
#### 2) Insufficient Data
#### 3) Poor Model Specification:
#### 4) Convergence Issues
##------------------------------------------------------------------------------
## ✖ `sigma_E` is larger than 100
## ℹ Consider simplifying the model or adding priors
##
#### Large values of sigma_E indicate that there is a high level of spatial 
#### variation that your model has not explained. 
##------------------------------------------------------------------------------
# ✔ Range parameters don't look unreasonably large

summary(fit10)
## M1:
#### Spatiotemporal AR1 correlation (rho): 0.10
#### Matérn range: 75.12
#### Spatial SD: 0.00
#### Spatiotemporal marginal AR1 SD: 615.41
## M2:
#### Dispersion Parameter: 0.45 (Good, typical range: 0.5-1.5)
#### Spatiotemporal AR1 Correlation (rho): 0.67 (Moderate to strong, 
## typical range: 0.4 - 0.8)
#### Matérn Range: 122.28 (Good, typical range: 50-200 depending on scale)
#### Spatial SD: 0.01
#### Spatiotemporal Marginal AR1 SD: 48.57 (Good, typical range: 10-50)
#### ML Criterion: 4509.807 (Best used for model comparison, not absolute assessment)

# Predict (gives a whole new df, unlike GAM)
pTrain10 <- predict(fit10, newdata = train, type = "response")

pTest10 <- predict(fit10, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain10))$r.squared # 0.1437345
summary(lm(Density ~ est, data = pTest10))$r.squared # 8.934804e-05

# MAE
mean(abs(pTrain10$Density - pTrain10$est)) # 280.5764
mean(abs(pTest10$Density - pTest10$est)) # 1.094452

mean(train$Density)
# [1] 209.332
sd(train$Density)
# [1] 11090.42

mean(test$Density)
# [1] 1.094451                                                                  
sd(test$Density)
# [1] 28.57991

```

## Try with spatio and no temporal effects 

```{r, make.base.sptl.model.w.fix.tim}

set.seed(1)

fit11 <- sdmTMB(Density ~ as.factor(age) + as.factor(len_bin) +
                  as.factor(YearSeas.dup), 
               data = train, 
               mesh = barrier,
               # Fit model by year and quarter
               time = "NumericQuarter",
               # Use "ar1" if you expect temporal autocorrelation
               # Otherwise, use default "rw"
               spatiotemporal = "off",
               spatial = "on", 
               extra_time = est.missing.time,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE)

# Check model skill, fit, and convergence
sanity(fit11)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
##------------------------------------------------------------------------------
## ✖ `sigma_O` is larger than 100
## ℹ Consider simplifying the model or adding priors
##
#### the variance (or precision component) associated with the spatial or temporal 
#### random effects in your model is being estimated as greater than 100
##
#### This could be an indication of overfitting, where the model is assigning large 
#### variability to the random effects because it’s fitting noise in the data rather #### than capturing meaningful signal or that there is a high degree of variability in #### the spatial (or temporal) random effects. 
##------------------------------------------------------------------------------
## ✔ Range parameters don't look unreasonably large

summary(fit11)
## M1:
#### Matérn range: 518.14
#### Spatial SD: 550.87
## M2:
#### Dispersion parameter: 0.32
#### Matérn range: 119.87
#### Spatial SD: 53.88
#### ML criterion at convergence: 7448.513

# Predict (gives a whole new df, unlike GAM)
pTrain11 <- predict(fit11, newdata = train, type = "response")
pTest11 <- predict(fit11, newdata = test, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain11))$r.squared # 0.1406107
summary(lm(Density ~ est, data = pTest11))$r.squared # 5.224278e-06

# MAE
mean(abs(pTrain3$Density - pTrain3$est)) # 250.8098
mean(abs(pTest3$Density - pTest3$est)) # 25.09882

mean(train$Density)
# [1] 209.332
sd(train$Density)
# [1] 11090.42

mean(test$Density)
# [1] 1.094451                                                                  
sd(test$Density)
# [1] 28.57991

```

# Now build the Seasonal models with factor time step:

## For a "Non-spatial" model without covars

This is a test run to see if we can explain the distribution by discrete time and OM spatial area 

```{r, make.base.seasonal.nonsptl.model.w.fix.tim}

set.seed(2) 

## Add age and length fixed effects without temporal or spatial autocorrelation
## Note that this is testing presence absence and then density predicted by age
## and length bin
fit12 <- sdmTMB(Density ~ as.factor(age) + as.factor(len_bin) + 
                 as.factor(YearSeas.dup),
               data = train2, 
               mesh = barrier2,
               # Fit model by year and month
               time = "YearSeas.nm",
               # Use "ar1" or "rw" if you expect temporal autocorrelation
               # Otherwise, use default "iid" (independent per time step)
               spatiotemporal = "off",
               spatial = "off", 
               extra_time = est.missing.time2,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE,
               silent = FALSE)

# Check model skill, fit, and convergence
sanity(fit12)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
# ✔ No sigma parameters are > 100

summary(fit12)
#### Dispersion Parameter: 0.17 (Good, typical range: 0.5-1.5)
#### ML Criterion: 8662.229 (Best used for model comparison, not absolute assessment)

# Predict (gives a whole new df, unlike GAM)
pTrain12 <- predict(fit12, newdata = train2, type = "response")

pTest12 <- predict(fit12, newdata = test2, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain12))$r.squared # 0.005737915
summary(lm(Density ~ est, data = pTest12))$r.squared # 1.419252e-05

# MAE
mean(abs(pTrain12$Density - pTrain12$est)) # 457.6253
mean(abs(pTest12$Density - pTest12$est)) # 415.5279

mean(train2$Density)
# [1] 231.8522
sd(train2$Density)
# [1] 11672.67

mean(test2$Density)
# [1] 0.9113857                                                               
sd(test2$Density)
# [1] 24.58594

```

## Try with temporal autocorrelation

```{r, make.seasonal.nonsptl.model.w.fix.tim}

set.seed(2) 

## Add age and length fixed effects with temporal autocorrelation
## Note that this is testing presence absence and then density predicted by age
## and length bin
fit13 <- sdmTMB(Density ~ as.factor(age) + as.factor(len_bin) +
                 as.factor(YearSeas.dup),
               data = train2, 
               mesh = barrier2,
               # Fit model by year and month
               ## Note if time is specified, spatiotemporal correlation is tested
               ## by default
               time = "YearSeas.nm",
               # Use "ar1" or "rw" if you expect temporal autocorrelation
               # Otherwise, use default "iid" (independent by time step)
               spatiotemporal = "ar1",
               spatial = "off", 
               extra_time = est.missing.time2,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE,
               # Show progress
               silent = FALSE)

# Check model skill, fit, and convergence
sanity(fit13)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
##------------------------------------------------------------------------------
## `ln_tau_E` gradient > 0.001
## ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
##
#### Signals that the optimization algorithm has not fully converged for the precision #### of the random effects parameter (ln_tau_E). This could be due to optimization 
#### difficulties, model specification issues, or data quality concerns. To address #### this, you might need to adjust your model, check your data, or tweak the 
#### optimization process (e.g., by increasing iterations or improving starting 
#### values).
##------------------------------------------------------------------------------
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
##------------------------------------------------------------------------------
## ✖ `sigma_E` is larger than 100
## ℹ Consider simplifying the model or adding priors
##
#### Large values of sigma_E indicate that there is a high level of spatial 
#### variation that your model has not explained. 
##------------------------------------------------------------------------------
# ✔ Range parameters don't look unreasonably large

summary(fit13)
## M1:
#### Spatiotemporal AR1 correlation (rho): 0.10
#### Matérn range: 77.17
#### Spatiotemporal marginal AR1 SD: 594.85
## M2:
#### Dispersion Parameter: 0.45 (Good, typical range: 0.5-1.5)
#### Spatiotemporal AR1 Correlation (rho): 0.48 (Moderate to strong, 
## typical range: 0.4 - 0.8)
#### Matérn Range: 121.11 (Good, typical range: 50-200 depending on scale)
#### Spatiotemporal Marginal AR1 SD: 49.19 (Good, typical range: 10-50)
#### ML Criterion: 4290.250 (Best used for model comparison, not absolute assessment)

# Predict (gives a whole new df, unlike GAM)
pTrain13 <- predict(fit13, newdata = train2, type = "response")

pTest13 <- predict(fit13, newdata = test2, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain13))$r.squared # 0.1411138
summary(lm(Density ~ est, data = pTest13))$r.squared # 8.335531e-05

# MAE
mean(abs(pTrain5$Density - pTrain5$est)) # 314.9381
mean(abs(pTest5$Density - pTest5$est)) # 0.9113859

mean(train2$Density)
# [1] 231.8522
sd(train2$Density)
# [1] 11672.67

mean(test2$Density)
# [1] 0.9113857                                                               
sd(test2$Density)
# [1] 24.58594

```

## Try with spatial effects

```{r, make.seasonal.sptl.model.w.fix.tim}

set.seed(2) 

## Add age and length fixed effects with temporal and spatial autocorrelation
## Note that this is testing presence absence and then density predicted by age
## and length bin so we want the intercept
fit14 <- sdmTMB(Density ~ as.factor(age) + as.factor(len_bin) +
                 as.factor(YearSeas.dup),
               data = train2, 
               mesh = barrier2,
               # Fit model by year and month
               ## Note if time is specified, spatiotemporal correlation is tested
               ## by default
               time = "YearSeas.nm",
               # Use "ar1" or "rw" if you expect temporal autocorrelation
               # Otherwise, use default "iid" (independent by time step)
               spatiotemporal = "ar1",
               spatial = "on", 
               extra_time = est.missing.time2,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE,
               # Show progress
               silent = FALSE)

# Check model skill, fit, and convergence
sanity(fit14)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
##------------------------------------------------------------------------------
## ✖ `sigma_E` is larger than 100
## ℹ Consider simplifying the model or adding priors
##
#### Large values of sigma_E indicate that there is a high level of spatial 
#### variation that your model has not explained. 
##------------------------------------------------------------------------------
# ✔ Range parameters don't look unreasonably large

summary(fit14)
## M1:
#### Spatiotemporal AR1 correlation (rho): 0.08
#### Matérn range: 82.73
#### Spatial SD: 81.67
#### Spatiotemporal marginal AR1 SD:  625.07

## M2:
#### Dispersion Parameter: 0.45 (Good, typical range: 0.5-1.5)
#### Spatiotemporal AR1 Correlation (rho): 0.44 (Moderate to strong, 
## typical range: 0.4 - 0.8)
#### Matérn Range: 119.278 (Good, typical range: 50-200 depending on scale)
#### Spatial SD: 10.11
#### Spatiotemporal Marginal AR1 SD: 47.38 (Good, typical range: 10-50)
#### ML Criterion: 4289.604 (Best used for model comparison, not absolute assessment)

# Predict (gives a whole new df, unlike GAM)
pTrain14 <- predict(fit14, newdata = train14, type = "response")

pTest14 <- predict(fit14, newdata = test14, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain14))$r.squared # 0.140585
summary(lm(Density ~ est, data = pTest14))$r.squared # 2.217959e-06

# MAE
mean(abs(pTrain14$Density - pTrain14$est)) # 314.6031
mean(abs(pTest14$Density - pTest14$est)) # 0.9113865

mean(train2$Density)
# [1] 231.8522
sd(train2$Density)
# [1] 11672.67

mean(test2$Density)
# [1] 0.9113857                                                               
sd(test2$Density)
# [1] 24.58594

```

## Try with spatio and no temporal effects 

```{r, make.base.seasonal.sptl.model.w.fix.tim}

set.seed(2)

fit15 <- sdmTMB(Density ~ as.factor(age) + as.factor(len_bin) + 
                  as.factor(YearSeas.dup), 
               data = train2, 
               mesh = barrier2,
               # Fit model by year and quarter
               time = "YearSeas.nm",
               # Use "ar1" if you expect temporal autocorrelation
               # Otherwise, use default "rw"
               spatiotemporal = "off",
               spatial = "on", 
               extra_time = est.missing.time2,
               # Fit hurdle model (first link is 0 or 1, second link is > 0)
               family = delta_gamma(link1 = "logit", link2 = "log"),
               anisotropy = FALSE)

# Check model skill, fit, and convergence
sanity(fit15)
# ✔ Non-linear minimizer suggests successful convergence
# ✔ Hessian matrix is positive definite
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# ✔ No fixed-effect standard errors are NA
# ✔ No standard errors look unreasonably large
# ✔ No sigma parameters are < 0.01
##------------------------------------------------------------------------------
## ✖ `sigma_O` is larger than 100
## ℹ Consider simplifying the model or adding priors
##
#### the variance (or precision component) associated with the spatial or temporal 
#### random effects in your model is being estimated as greater than 100
##
#### This could be an indication of overfitting, where the model is assigning large 
#### variability to the random effects because it’s fitting noise in the data rather #### than capturing meaningful signal or that there is a high degree of variability in #### the spatial (or temporal) random effects. 
##------------------------------------------------------------------------------
## ✔ Range parameters don't look unreasonably large

summary(fit15)
## M1:
#### Matérn range: 353.56
#### Spatial SD: 296.02
## M2:
#### Dispersion parameter: 0.35
#### Matérn range: 130.35
#### Spatial SD: 55.28
#### ML criterion at convergence: 6351.908

# Predict (gives a whole new df, unlike GAM)
pTrain15 <- predict(fit15, newdata = train2, type = "response")
pTest15 <- predict(fit15, newdata = test2, type = "response")

# Overall R2 for test and train 
summary(lm(Density ~ est, data = pTrain15))$r.squared # 0.09900705
summary(lm(Density ~ est, data = pTest15))$r.squared # 3.592542e-06

# MAE
mean(abs(pTrain15$Density - pTrain15$est)) # 294.2342
mean(abs(pTest15$Density - pTest15$est)) # 24.7259

mean(train2$Density)
# [1] 231.8522
sd(train2$Density)
# [1] 11672.67

mean(test2$Density)
# [1] 0.9113857                                                               
sd(test2$Density)
# [1] 24.58594

```

 
 